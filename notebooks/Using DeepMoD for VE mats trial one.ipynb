{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viscolelastic Materials Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we Choose the model and other parameters for our data, and save this data for reference later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "import data.Generation.VE_DataGen_Functions as vedg\n",
    "from deepymod_torch.library_function import mech_library\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.neural_net import deepmod_init, train\n",
    "from deepymod_torch.sparsity import scaling, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Type = 'Strain'\n",
    "E = [1,1,1]\n",
    "Eta = [1.25,2.5]\n",
    "t = sp.symbols('t', real=True)\n",
    "Input_Function = sp.exp(-t) + sp.exp(-t/10) + sp.exp(-t/20)\n",
    "Input_Description = 'Three_e_Decays'\n",
    "Int_Type = 'Numerical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.0 + 3.0*exp(-0.8*t) + 3.0*exp(-0.4*t), <function Stress_Strain_Master_Int.<locals>.<lambda> at 0x152c591320>)\n"
     ]
    }
   ],
   "source": [
    "Tuple_of_Expressions = vedg.Stress_Strain_Master_Int(Input_Type, E, Eta, Input_Function, Int_Type, t)\n",
    "print(Tuple_of_Expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = np.linspace(0.001, 50, 5000)\n",
    "\n",
    "title = ('Input: ' + Input_Description + ', ' + Input_Type + \n",
    "         '.\\nModel: E Mods ' + str(E) + ', Viscs ' + str(Eta))\n",
    "\n",
    "Strain_Array, Stress_Array = vedg.Eval_Graph_Strain_Stress(title, time, Input_Function, Tuple_of_Expressions, Input_Type, Int_Type, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vedg.save_stress_strain(time, Strain_Array, Stress_Array, '../data/StressStrain', Input_Type, Input_Description, E, Eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mostly use DeepMoD using default settings as we would for any other problems. the configuratiosn specific to this application come from knowledge we already have, namely:\n",
    "\n",
    "- We are going to use a customised $\\Theta$ that only ever computes the derivatives of stress and strain, never the squares, as we are looking solve a system where these are the only terms present. We are looking at a hybrid problem here where we aren't completely blindly exploring the system. We assume the data is stress/strain data and we assume our models for these materials are accurate so as to produce an accurate format for the equations describing their dynamic behaviour.\n",
    "\n",
    "- For this, we need to use a library function of a new design.\n",
    "\n",
    "- In this library function, we will ask PyTorch to compute derivatives for the output variable up to a couple of orders of differentiation beyond expectations, just to demonstrate teh DeepmoD both, finds the correct coefficients, but also removes terms that should not be present.\n",
    "\n",
    "- the library function will also contain the assumption that all coefficients on the input function are negative. this way, teh coefficients that DeepMoD shoudl find will all be positive, as all viscosties and elastic moduli are postive, so all sums and products of them are also, and this si what makes up the coefficient.\n",
    "\n",
    "- This will allow us to further tinker and ask the random initial values of teh coefficients to all be positive, getting round an issue with DeepMod getting stuck in local minima\n",
    "\n",
    "- The derivatives of teh input function can be calculated without the neural network, as teh functional form of this is know. These results are still built into the loss though, as part of the equation-regression term.\n",
    "\n",
    "- Additionalyl, the lambda paramter will be made small. in essence, we are not looking for a very sparse vector, so L1 regullarisation actually hinders the convergence process.\n",
    "\n",
    "- Additionally, the LHS of the equation is fixed to the time derivative of the input with coeff 1\n",
    "\n",
    "> Is it the time derivative of the input, or always strain!!!!!!\n",
    "\n",
    "Once DeepMoD has its coefficnets, the job is not done as the coefficients need to be traced back into E_Mods and Viscs. i am not yet sure how to do this but\n",
    "\n",
    "- there will be two stages. the first is developing teh expressions for coeff 1 is this combination, coeff 2 is that one etc, and then teh second stage is using these simultaneous expressions to instead right each paramter in terms of teh coeffs.\n",
    "\n",
    "- The first part is not trivial as the parameters that go into forming the coeffs will be different depending on how many parameters are genuinely present in the model. The number of derivatives that deepmod finds can be used to determine which set of equations are going to be used.\n",
    "\n",
    "- Either these equations can be hard coded or\n",
    "\n",
    "- A pattern can be identified so that once the number of parameters is known, a loop can be run to generate the expressions (However, this method is only weakly scalable as, although identifying a pattern would allow easy identification of the full description of the coeffs for low order derivatives, the patterns would need to be worked out by hand for even higher derivatives leaving a) a lot of work by hand to allow the analysis to handle, let's say, 12th order derivatives and b) there will always be a ceiling determined arbitrarily by how high I have worked out the expressions for.) or\n",
    "\n",
    "- The root equation with sum could be looped and written in sympy, and the rearranged and sympy could be asked to find the expressions for coeffs of each term. the function `Differential` rather than `.diff` will be useful here to maintain the unsolved derivatives. Side note for coding: probably symbols can be placed into lists so that they do not have to be given unique names which could be tricky in a loop. -> ie `List_Syms[3] = sym.symbols('Tau'+Loop_Number, real=True)` etc.\n",
    "\n",
    "- The coeff on the LHS of the equation being fixed to 1 fixes all the other coeffs, so there is no ambiguity here.\n",
    "\n",
    "For the second part, using the coeffs to calculate each model parameter\n",
    "\n",
    "- I am hoping i can just use some function called solve or something in sympy to get the expressions for each aparamter in term so the coeffs, once the part 1 above is done. Indeed, this is possible on mathematica as demonstrated by the screenshot Remy sent by email.\n",
    "\n",
    "- Alternatively, they could be hard coded in if worked them out by hand......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra idea: The order of derivative should be teh same in both stress and strain. This is not a wild assumption. Perhaps teh loss can be further modified so as to penilise a situation where higher derivatives are being allowed in the 1st half of coeffs vs the other.\n",
    "\n",
    "Extra Idea: If the third derivative has been identified as being zero, all higher derivatives must also be zero. Remy suggested not harshly setting all to zero immediately, but removing the highest derivative and retraining, and then repeating, to see if lower derivatives are recovered.\n",
    "\n",
    "There is another consideration. There are two ways we can get the optimisation procedure to discover the right equation. The 'One Big Step' approach means modifying the loss function to be more and more aggressive. The other is a more iterative approach where the result can be critiqued, something changed, and the optimisation run again, such as the removal of the highest derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra note: need to think about the GKM vs GMM stuff. Essentially, teh creep and relaxation expressions used to generate the data have come from a GKM, not a GMM!. Presumably the results may differ slightly with a GMM, but at the same time, this leads to a direct final result, and the result should be the same if the two models are genuinely equivalent???\n",
    "\n",
    "In any case, the coefficents worked out will be different depending on whether we analyse in terms of a GKM or GMM. At leats at first glnce, would be an interesting test. certainly, the set of parameters making up each deduced coeff would be different, but perhaps, once the simultaneous equations are solved the resulting paramters are the same. ie, the coeff for the second deriv of stress will be described by different parameters sepending on the model, but a) the value for this coeff is model independant and b) the paremeters are model independant, its just that a different set of these parameters can be combined in a different way to get the same coeff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually getting on with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to reshape the data into a conveneint form. `vedg.save_stress_strain` does this also, so if importing the data this next step is unnecessary.\n",
    "\n",
    "For the purpose of running deepmod, we do not need the calculated dat for whatever the input is, as we will instead provide teh analytical expression. Understanding this is important as it explains why for around half of the terms in theta, we don't need to use the result of the NN to obtain the values, we can easily obtain them directly from the anayltical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_Array = time.reshape(-1, 1)\n",
    "Stress_Array = Stress_Array.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/StressStrain/Strain Three_e_Decays E[1, 1, 1] V[1.25, 2.5].csv', delimiter=',')\n",
    "time_Array = data[:,[0]] # submitting the column index as a list preserves the idea that time_Array is a column,\n",
    "                            #otherwise it would default to a 1D array.\n",
    "Stress_Array = data[:,[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This is now the point where we might add **noise** and perform **random sampling** of the data. For now, in this first version of the notebook, we will skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to fully prepare the data for injection into PyTorch, we need to convert the arrays into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_Tensor = torch.tensor(time_Array, dtype=torch.float32, requires_grad=True)\n",
    "Stress_Tensor = torch.tensor(Stress_Array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by setting up DeepMoD as normal, as per Burgers example and then I will interogate for changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optim_config` is contains only a minor change. I have reduced the lambda variable to deemphasise the need for a sparse coeff_vector. I may do further tweaks later.\n",
    "\n",
    "`network config` contains the important change that there is only 1 input dimension in our problem, ie, only $t$. I have also reduced the size of the NN rather arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'max_iterations': 20000}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 15, 'layers': 3, 'output_dim': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes to `lib_config` is the business end. Poly order is a redundant variable as the single 'polynomial' term which would be stress or strain to the power of one, can be accounted for as the zeroth order derivative. There is no constant. An entirely new library 'type' function is created for our problem.\n",
    "\n",
    "Additionally, three new configuration paramters for our library dictionary are created. The first is the additional entry that will enforce the idea that all the initial guesses at the value of the coeffs (ksi vector) must be positive. The latter two allow the library to put the coefficients in the same order regardless of whether strain or stress was described by an analytical expression of our design, the analytical expression also being given as the final item in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.library_function import mech_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': Input_Type, 'input_expr': Input_Function}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these configurations, in theory we should be able to run deepmod to interrogate the values of the coefficients of the derivatives in the general expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except, to break it down a little for this first run, let's actually pull out the code from DeepMoD and run bits step by step to better check for errors.\n",
    "\n",
    "First, we initialise the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.neural_net import deepmod_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network, coeff_vector_list, sparsity_mask_list = deepmod_init(network_config, lib_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to some changes made in the neural_net.py file, we notice the following has managed to take on board the need for these starting values to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4994],\n",
       "         [1.5211],\n",
       "         [1.5500],\n",
       "         [0.6857],\n",
       "         [0.1835],\n",
       "         [0.3029],\n",
       "         [1.6142]], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3, 4, 5, 6])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our network and starting coeffs, let's try and train the network. This is the part one trainign which still includes the L1 term and contains all terms. As I specified a diff order of 5 earlier, this may be a little long to process...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.neural_net import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 1.4E+00 1.3E+00 6.7E-02 5.8E-06\n",
      "tensor([[0.4974],\n",
      "        [1.5191],\n",
      "        [1.5520],\n",
      "        [0.6877],\n",
      "        [0.1855],\n",
      "        [0.3009],\n",
      "        [1.6162]], requires_grad=True)\n",
      "500 1.1E-02 1.0E-02 1.2E-03 1.6E-05\n",
      "tensor([[0.2972],\n",
      "        [1.9223],\n",
      "        [1.1311],\n",
      "        [0.3688],\n",
      "        [1.1013],\n",
      "        [1.0335],\n",
      "        [0.4551]], requires_grad=True)\n",
      "1000 1.1E-03 8.8E-04 1.5E-04 2.0E-05\n",
      "tensor([[0.2577],\n",
      "        [1.9467],\n",
      "        [1.1045],\n",
      "        [0.3646],\n",
      "        [1.1979],\n",
      "        [0.9729],\n",
      "        [0.2536]], requires_grad=True)\n",
      "1500 3.9E-04 3.0E-04 7.1E-05 2.0E-05\n",
      "tensor([[0.2563],\n",
      "        [1.9489],\n",
      "        [1.1013],\n",
      "        [0.3610],\n",
      "        [1.1937],\n",
      "        [0.9511],\n",
      "        [0.1865]], requires_grad=True)\n",
      "2000 3.4E-03 3.0E-03 3.7E-04 2.0E-05\n",
      "tensor([[0.2560],\n",
      "        [1.9513],\n",
      "        [1.0979],\n",
      "        [0.3561],\n",
      "        [1.1835],\n",
      "        [0.9344],\n",
      "        [0.1446]], requires_grad=True)\n",
      "2500 1.3E-04 8.4E-05 3.1E-05 2.0E-05\n",
      "tensor([[0.2535],\n",
      "        [1.9533],\n",
      "        [1.0946],\n",
      "        [0.3520],\n",
      "        [1.1721],\n",
      "        [0.9158],\n",
      "        [0.1232]], requires_grad=True)\n",
      "3000 2.1E-03 1.8E-03 2.8E-04 1.9E-05\n",
      "tensor([[0.2509],\n",
      "        [1.9543],\n",
      "        [1.0921],\n",
      "        [0.3439],\n",
      "        [1.1577],\n",
      "        [0.9031],\n",
      "        [0.1067]], requires_grad=True)\n",
      "3500 8.7E-05 4.3E-05 2.6E-05 1.9E-05\n",
      "tensor([[0.2442],\n",
      "        [1.9543],\n",
      "        [1.0902],\n",
      "        [0.3361],\n",
      "        [1.1403],\n",
      "        [0.8885],\n",
      "        [0.0986]], requires_grad=True)\n",
      "4000 7.2E-05 3.1E-05 2.2E-05 1.9E-05\n",
      "tensor([[0.2415],\n",
      "        [1.9543],\n",
      "        [1.0880],\n",
      "        [0.3310],\n",
      "        [1.1244],\n",
      "        [0.8722],\n",
      "        [0.0895]], requires_grad=True)\n",
      "4500 7.1E-05 2.9E-05 2.3E-05 1.8E-05\n",
      "tensor([[0.2324],\n",
      "        [1.9522],\n",
      "        [1.0871],\n",
      "        [0.3170],\n",
      "        [1.1019],\n",
      "        [0.8622],\n",
      "        [0.0855]], requires_grad=True)\n",
      "5000 5.5E-05 2.1E-05 1.6E-05 1.8E-05\n",
      "tensor([[0.2295],\n",
      "        [1.9516],\n",
      "        [1.0845],\n",
      "        [0.3116],\n",
      "        [1.0844],\n",
      "        [0.8457],\n",
      "        [0.0797]], requires_grad=True)\n",
      "5500 8.7E-05 4.6E-05 2.3E-05 1.7E-05\n",
      "tensor([[0.2196],\n",
      "        [1.9486],\n",
      "        [1.0831],\n",
      "        [0.2966],\n",
      "        [1.0606],\n",
      "        [0.8365],\n",
      "        [0.0774]], requires_grad=True)\n",
      "6000 4.5E-05 1.6E-05 1.2E-05 1.7E-05\n",
      "tensor([[0.2155],\n",
      "        [1.9475],\n",
      "        [1.0793],\n",
      "        [0.2897],\n",
      "        [1.0411],\n",
      "        [0.8211],\n",
      "        [0.0741]], requires_grad=True)\n",
      "6500 3.9E-05 1.2E-05 9.6E-06 1.7E-05\n",
      "tensor([[0.2134],\n",
      "        [1.9481],\n",
      "        [1.0730],\n",
      "        [0.2855],\n",
      "        [1.0236],\n",
      "        [0.8042],\n",
      "        [0.0690]], requires_grad=True)\n",
      "7000 3.9E-05 1.4E-05 9.0E-06 1.7E-05\n",
      "tensor([[0.2059],\n",
      "        [1.9457],\n",
      "        [1.0686],\n",
      "        [0.2738],\n",
      "        [1.0003],\n",
      "        [0.7919],\n",
      "        [0.0675]], requires_grad=True)\n",
      "7500 3.3E-05 9.1E-06 7.9E-06 1.6E-05\n",
      "tensor([[0.2048],\n",
      "        [1.9465],\n",
      "        [1.0606],\n",
      "        [0.2708],\n",
      "        [0.9818],\n",
      "        [0.7736],\n",
      "        [0.0633]], requires_grad=True)\n",
      "8000 3.4E-05 9.0E-06 8.8E-06 1.6E-05\n",
      "tensor([[0.1986],\n",
      "        [1.9438],\n",
      "        [1.0552],\n",
      "        [0.2608],\n",
      "        [0.9580],\n",
      "        [0.7588],\n",
      "        [0.0621]], requires_grad=True)\n",
      "8500 2.9E-05 7.0E-06 6.9E-06 1.6E-05\n",
      "tensor([[0.1986],\n",
      "        [1.9443],\n",
      "        [1.0462],\n",
      "        [0.2592],\n",
      "        [0.9387],\n",
      "        [0.7383],\n",
      "        [0.0580]], requires_grad=True)\n",
      "9000 3.0E-05 7.1E-06 7.6E-06 1.5E-05\n",
      "tensor([[0.1931],\n",
      "        [1.9404],\n",
      "        [1.0411],\n",
      "        [0.2499],\n",
      "        [0.9137],\n",
      "        [0.7216],\n",
      "        [0.0572]], requires_grad=True)\n",
      "9500 2.7E-05 5.6E-06 6.0E-06 1.5E-05\n",
      "tensor([[0.1937],\n",
      "        [1.9400],\n",
      "        [1.0322],\n",
      "        [0.2491],\n",
      "        [0.8938],\n",
      "        [0.6993],\n",
      "        [0.0532]], requires_grad=True)\n",
      "10000 2.6E-05 5.6E-06 6.0E-06 1.5E-05\n",
      "tensor([[0.1898],\n",
      "        [1.9356],\n",
      "        [1.0271],\n",
      "        [0.2421],\n",
      "        [0.8694],\n",
      "        [0.6802],\n",
      "        [0.0519]], requires_grad=True)\n",
      "10500 2.8E-04 2.4E-04 3.1E-05 1.4E-05\n",
      "tensor([[0.1866],\n",
      "        [1.9311],\n",
      "        [1.0216],\n",
      "        [0.2358],\n",
      "        [0.8456],\n",
      "        [0.6601],\n",
      "        [0.0498]], requires_grad=True)\n",
      "11000 2.3E-05 4.4E-06 4.8E-06 1.4E-05\n",
      "tensor([[0.1864],\n",
      "        [1.9273],\n",
      "        [1.0156],\n",
      "        [0.2339],\n",
      "        [0.8240],\n",
      "        [0.6373],\n",
      "        [0.0471]], requires_grad=True)\n",
      "11500 2.4E-05 5.5E-06 4.5E-06 1.4E-05\n",
      "tensor([[0.1822],\n",
      "        [1.9203],\n",
      "        [1.0125],\n",
      "        [0.2265],\n",
      "        [0.7993],\n",
      "        [0.6177],\n",
      "        [0.0459]], requires_grad=True)\n",
      "12000 2.1E-05 3.5E-06 3.7E-06 1.3E-05\n",
      "tensor([[0.1826],\n",
      "        [1.9159],\n",
      "        [1.0066],\n",
      "        [0.2253],\n",
      "        [0.7788],\n",
      "        [0.5946],\n",
      "        [0.0423]], requires_grad=True)\n",
      "12500 2.0E-05 3.5E-06 3.5E-06 1.3E-05\n",
      "tensor([[0.1786],\n",
      "        [1.9075],\n",
      "        [1.0046],\n",
      "        [0.2180],\n",
      "        [0.7546],\n",
      "        [0.5751],\n",
      "        [0.0410]], requires_grad=True)\n",
      "13000 2.3E-05 6.7E-06 4.0E-06 1.3E-05\n",
      "tensor([[0.1742],\n",
      "        [1.8981],\n",
      "        [1.0034],\n",
      "        [0.2101],\n",
      "        [0.7305],\n",
      "        [0.5565],\n",
      "        [0.0397]], requires_grad=True)\n",
      "13500 1.8E-05 3.1E-06 2.7E-06 1.2E-05\n",
      "tensor([[0.1728],\n",
      "        [1.8906],\n",
      "        [1.0003],\n",
      "        [0.2068],\n",
      "        [0.7094],\n",
      "        [0.5346],\n",
      "        [0.0371]], requires_grad=True)\n",
      "14000 1.8E-05 3.4E-06 2.7E-06 1.2E-05\n",
      "tensor([[0.1680],\n",
      "        [1.8800],\n",
      "        [1.0001],\n",
      "        [0.1985],\n",
      "        [0.6858],\n",
      "        [0.5164],\n",
      "        [0.0360]], requires_grad=True)\n",
      "14500 1.6E-05 2.5E-06 2.0E-06 1.2E-05\n",
      "tensor([[0.1673],\n",
      "        [1.8719],\n",
      "        [0.9973],\n",
      "        [0.1960],\n",
      "        [0.6662],\n",
      "        [0.4954],\n",
      "        [0.0331]], requires_grad=True)\n",
      "15000 1.6E-05 2.7E-06 2.0E-06 1.1E-05\n",
      "tensor([[0.1616],\n",
      "        [1.8598],\n",
      "        [0.9983],\n",
      "        [0.1865],\n",
      "        [0.6426],\n",
      "        [0.4783],\n",
      "        [0.0322]], requires_grad=True)\n",
      "15500 1.5E-05 2.3E-06 1.6E-06 1.1E-05\n",
      "tensor([[0.1602],\n",
      "        [1.8505],\n",
      "        [0.9965],\n",
      "        [0.1831],\n",
      "        [0.6232],\n",
      "        [0.4586],\n",
      "        [0.0297]], requires_grad=True)\n",
      "16000 1.5E-05 2.3E-06 1.4E-06 1.1E-05\n",
      "tensor([[0.1553],\n",
      "        [1.8381],\n",
      "        [0.9974],\n",
      "        [0.1747],\n",
      "        [0.6008],\n",
      "        [0.4414],\n",
      "        [0.0285]], requires_grad=True)\n",
      "16500 3.7E-05 2.5E-05 1.7E-06 1.0E-05\n",
      "tensor([[0.1535],\n",
      "        [1.8275],\n",
      "        [0.9963],\n",
      "        [0.1708],\n",
      "        [0.5817],\n",
      "        [0.4224],\n",
      "        [0.0263]], requires_grad=True)\n",
      "17000 1.3E-05 1.9E-06 1.1E-06 1.0E-05\n",
      "tensor([[0.1486],\n",
      "        [1.8143],\n",
      "        [0.9977],\n",
      "        [0.1625],\n",
      "        [0.5596],\n",
      "        [0.4052],\n",
      "        [0.0251]], requires_grad=True)\n",
      "17500 5.9E-05 4.7E-05 2.0E-06 9.9E-06\n",
      "tensor([[0.1458],\n",
      "        [1.8026],\n",
      "        [0.9975],\n",
      "        [0.1573],\n",
      "        [0.5404],\n",
      "        [0.3877],\n",
      "        [0.0233]], requires_grad=True)\n",
      "18000 1.2E-05 1.8E-06 8.6E-07 9.5E-06\n",
      "tensor([[0.1402],\n",
      "        [1.7887],\n",
      "        [0.9993],\n",
      "        [0.1480],\n",
      "        [0.5182],\n",
      "        [0.3716],\n",
      "        [0.0224]], requires_grad=True)\n",
      "18500 1.6E-04 1.5E-04 3.6E-06 9.2E-06\n",
      "tensor([[0.1360],\n",
      "        [1.7750],\n",
      "        [1.0004],\n",
      "        [0.1414],\n",
      "        [0.4978],\n",
      "        [0.3556],\n",
      "        [0.0211]], requires_grad=True)\n",
      "19000 1.1E-05 1.5E-06 6.7E-07 8.9E-06\n",
      "tensor([[0.1335],\n",
      "        [1.7622],\n",
      "        [1.0004],\n",
      "        [0.1359],\n",
      "        [0.4786],\n",
      "        [0.3368],\n",
      "        [0.0192]], requires_grad=True)\n",
      "19500 1.1E-05 1.5E-06 6.3E-07 8.6E-06\n",
      "tensor([[0.1288],\n",
      "        [1.7468],\n",
      "        [1.0027],\n",
      "        [0.1280],\n",
      "        [0.4571],\n",
      "        [0.3195],\n",
      "        [0.0181]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "time_deriv_list, theta, coeff_vector_list = train(time_Tensor, Stress_Tensor, network, coeff_vector_list, \n",
    "                                                  sparsity_mask_list, lib_config, optim_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the deepmod procedure is to apply scaling to the coefficients before thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.sparsity import scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_coeff_vector_list = [scaling(coeff_vector, theta, time_deriv) for coeff_vector, time_deriv in zip(coeff_vector_list, time_deriv_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.7359],\n",
       "         [1.3081],\n",
       "         [0.7443],\n",
       "         [1.0574],\n",
       "         [2.3255],\n",
       "         [1.3626],\n",
       "         [0.7127]], grad_fn=<MulBackward0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_coeff_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then thresholding, which unlike scaling has been modified to take additional arguements so that some common sense rejection on deepmods results can be implemented based on\n",
    "\n",
    "-  if between pairs of like order derivatives of stress and strain, one of the pair is thresholded to zero but the other not\n",
    "\n",
    "- if derivatives of certain order have been thresholded to zero but not higher order derivatives\n",
    "\n",
    "then\n",
    "\n",
    "- deepmods sparse selection is rejected, highest order derivative pair is removed, and deepmod retrains with all other coeffs still allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.sparsity import threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coeff_vector_list, sparsity_mask_list, Overode_list = zip(*[threshold(scaled_coeff_vector, coeff_vector, sparsity_mask, lib_config) for scaled_coeff_vector, coeff_vector, sparsity_mask in zip(scaled_coeff_vector_list, coeff_vector_list, sparsity_mask_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1240],\n",
       "         [1.7304],\n",
       "         [1.0057],\n",
       "         [0.1199],\n",
       "         [0.4357],\n",
       "         [0.3018],\n",
       "         [0.0169]], requires_grad=True),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overode_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that if Overode_List contained a true, DeepMoD would not enter teh final phase and would befgin again. In the trial that i am running today, DeepMoD didn't threshold **any** of the terms, so the thresholding was not rejected (we happen to know this is wrong, but no common sense way to see that the model doesn't make sense). As a result, we enter directly final phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training is run again with the L1 loss term set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.2E-06 2.5E-06 6.7E-07 0.0E+00\n",
      "tensor([[0.1220],\n",
      "        [1.7284],\n",
      "        [1.0077],\n",
      "        [0.1219],\n",
      "        [0.4337],\n",
      "        [0.3038],\n",
      "        [0.0189]], requires_grad=True)\n",
      "500 2.4E-06 1.7E-06 6.7E-07 0.0E+00\n",
      "tensor([[0.1212],\n",
      "        [1.7275],\n",
      "        [1.0083],\n",
      "        [0.1157],\n",
      "        [0.4297],\n",
      "        [0.3001],\n",
      "        [0.0175]], requires_grad=True)\n",
      "1000 2.0E-06 1.4E-06 6.1E-07 0.0E+00\n",
      "tensor([[0.1228],\n",
      "        [1.7292],\n",
      "        [1.0065],\n",
      "        [0.1177],\n",
      "        [0.4285],\n",
      "        [0.2964],\n",
      "        [0.0165]], requires_grad=True)\n",
      "1500 1.7E-06 1.2E-06 5.4E-07 0.0E+00\n",
      "tensor([[0.1241],\n",
      "        [1.7298],\n",
      "        [1.0057],\n",
      "        [0.1193],\n",
      "        [0.4273],\n",
      "        [0.2928],\n",
      "        [0.0156]], requires_grad=True)\n",
      "2000 1.6E-06 1.1E-06 4.8E-07 0.0E+00\n",
      "tensor([[0.1247],\n",
      "        [1.7291],\n",
      "        [1.0065],\n",
      "        [0.1199],\n",
      "        [0.4261],\n",
      "        [0.2900],\n",
      "        [0.0148]], requires_grad=True)\n",
      "2500 1.5E-06 1.0E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1218],\n",
      "        [1.7253],\n",
      "        [1.0102],\n",
      "        [0.1156],\n",
      "        [0.4188],\n",
      "        [0.2861],\n",
      "        [0.0149]], requires_grad=True)\n",
      "3000 5.8E-06 5.2E-06 6.5E-07 0.0E+00\n",
      "tensor([[0.1191],\n",
      "        [1.7210],\n",
      "        [1.0145],\n",
      "        [0.1111],\n",
      "        [0.4099],\n",
      "        [0.2809],\n",
      "        [0.0149]], requires_grad=True)\n",
      "3500 1.6E-06 1.1E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1184],\n",
      "        [1.7186],\n",
      "        [1.0168],\n",
      "        [0.1098],\n",
      "        [0.4026],\n",
      "        [0.2730],\n",
      "        [0.0143]], requires_grad=True)\n",
      "4000 1.4E-06 9.4E-07 4.7E-07 0.0E+00\n",
      "tensor([[0.1183],\n",
      "        [1.7159],\n",
      "        [1.0198],\n",
      "        [0.1092],\n",
      "        [0.3966],\n",
      "        [0.2659],\n",
      "        [0.0135]], requires_grad=True)\n",
      "4500 1.5E-06 1.0E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1159],\n",
      "        [1.7101],\n",
      "        [1.0260],\n",
      "        [0.1051],\n",
      "        [0.3867],\n",
      "        [0.2580],\n",
      "        [0.0133]], requires_grad=True)\n",
      "5000 1.4E-06 9.3E-07 4.7E-07 0.0E+00\n",
      "tensor([[0.1158],\n",
      "        [1.7062],\n",
      "        [1.0307],\n",
      "        [0.1045],\n",
      "        [0.3807],\n",
      "        [0.2505],\n",
      "        [0.0125]], requires_grad=True)\n",
      "5500 1.4E-06 9.2E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1134],\n",
      "        [1.6994],\n",
      "        [1.0383],\n",
      "        [0.1006],\n",
      "        [0.3709],\n",
      "        [0.2421],\n",
      "        [0.0122]], requires_grad=True)\n",
      "6000 1.3E-06 8.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1125],\n",
      "        [1.6937],\n",
      "        [1.0450],\n",
      "        [0.0988],\n",
      "        [0.3633],\n",
      "        [0.2338],\n",
      "        [0.0116]], requires_grad=True)\n",
      "6500 8.0E-05 7.6E-05 3.8E-06 0.0E+00\n",
      "tensor([[0.1106],\n",
      "        [1.6867],\n",
      "        [1.0532],\n",
      "        [0.0957],\n",
      "        [0.3556],\n",
      "        [0.2274],\n",
      "        [0.0114]], requires_grad=True)\n",
      "7000 1.3E-06 7.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1103],\n",
      "        [1.6812],\n",
      "        [1.0601],\n",
      "        [0.0947],\n",
      "        [0.3495],\n",
      "        [0.2189],\n",
      "        [0.0105]], requires_grad=True)\n",
      "7500 1.3E-06 7.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1085],\n",
      "        [1.6732],\n",
      "        [1.0696],\n",
      "        [0.0916],\n",
      "        [0.3410],\n",
      "        [0.2105],\n",
      "        [0.0101]], requires_grad=True)\n",
      "8000 1.2E-06 7.5E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1084],\n",
      "        [1.6668],\n",
      "        [1.0778],\n",
      "        [0.0911],\n",
      "        [0.3360],\n",
      "        [0.2030],\n",
      "        [0.0094]], requires_grad=True)\n",
      "8500 1.5E-06 9.8E-07 5.2E-07 0.0E+00\n",
      "tensor([[0.1059],\n",
      "        [1.6578],\n",
      "        [1.0885],\n",
      "        [0.0870],\n",
      "        [0.3266],\n",
      "        [0.1946],\n",
      "        [0.0093]], requires_grad=True)\n",
      "9000 1.1E-06 6.3E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1059],\n",
      "        [1.6515],\n",
      "        [1.0968],\n",
      "        [0.0867],\n",
      "        [0.3220],\n",
      "        [0.1875],\n",
      "        [0.0086]], requires_grad=True)\n",
      "9500 1.2E-06 6.7E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1042],\n",
      "        [1.6429],\n",
      "        [1.1074],\n",
      "        [0.0838],\n",
      "        [0.3143],\n",
      "        [0.1795],\n",
      "        [0.0082]], requires_grad=True)\n",
      "10000 1.4E-05 1.4E-05 5.8E-07 0.0E+00\n",
      "tensor([[0.1028],\n",
      "        [1.6344],\n",
      "        [1.1182],\n",
      "        [0.0814],\n",
      "        [0.3078],\n",
      "        [0.1725],\n",
      "        [0.0080]], requires_grad=True)\n",
      "10500 1.1E-06 6.1E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1024],\n",
      "        [1.6268],\n",
      "        [1.1281],\n",
      "        [0.0804],\n",
      "        [0.3019],\n",
      "        [0.1640],\n",
      "        [0.0073]], requires_grad=True)\n",
      "11000 3.1E-05 2.4E-05 6.2E-06 0.0E+00\n",
      "tensor([[0.1007],\n",
      "        [1.6183],\n",
      "        [1.1389],\n",
      "        [0.0775],\n",
      "        [0.2957],\n",
      "        [0.1589],\n",
      "        [0.0072]], requires_grad=True)\n",
      "11500 1.1E-06 5.9E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1002],\n",
      "        [1.6112],\n",
      "        [1.1484],\n",
      "        [0.0765],\n",
      "        [0.2905],\n",
      "        [0.1504],\n",
      "        [0.0066]], requires_grad=True)\n",
      "12000 1.2E-06 7.1E-07 5.3E-07 0.0E+00\n",
      "tensor([[0.0984],\n",
      "        [1.6021],\n",
      "        [1.1597],\n",
      "        [0.0735],\n",
      "        [0.2821],\n",
      "        [0.1417],\n",
      "        [0.0064]], requires_grad=True)\n",
      "12500 9.9E-07 4.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0994],\n",
      "        [1.5969],\n",
      "        [1.1672],\n",
      "        [0.0747],\n",
      "        [0.2815],\n",
      "        [0.1373],\n",
      "        [0.0057]], requires_grad=True)\n",
      "13000 1.2E-05 1.1E-05 9.1E-07 0.0E+00\n",
      "tensor([[0.0984],\n",
      "        [1.5899],\n",
      "        [1.1766],\n",
      "        [0.0732],\n",
      "        [0.2770],\n",
      "        [0.1317],\n",
      "        [0.0055]], requires_grad=True)\n",
      "13500 9.9E-07 4.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0974],\n",
      "        [1.5831],\n",
      "        [1.1859],\n",
      "        [0.0713],\n",
      "        [0.2722],\n",
      "        [0.1261],\n",
      "        [0.0052]], requires_grad=True)\n",
      "14000 3.9E-06 3.3E-06 6.1E-07 0.0E+00\n",
      "tensor([[0.0953],\n",
      "        [1.5753],\n",
      "        [1.1963],\n",
      "        [0.0681],\n",
      "        [0.2659],\n",
      "        [0.1202],\n",
      "        [0.0052]], requires_grad=True)\n",
      "14500 1.0E-06 5.0E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0954],\n",
      "        [1.5691],\n",
      "        [1.2051],\n",
      "        [0.0679],\n",
      "        [0.2625],\n",
      "        [0.1143],\n",
      "        [0.0047]], requires_grad=True)\n",
      "15000 1.8E-05 1.7E-05 1.1E-06 0.0E+00\n",
      "tensor([[0.0939],\n",
      "        [1.5620],\n",
      "        [1.2149],\n",
      "        [0.0658],\n",
      "        [0.2581],\n",
      "        [0.1095],\n",
      "        [0.0046]], requires_grad=True)\n",
      "15500 9.2E-07 4.3E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0944],\n",
      "        [1.5556],\n",
      "        [1.2241],\n",
      "        [0.0659],\n",
      "        [0.2548],\n",
      "        [0.1031],\n",
      "        [0.0040]], requires_grad=True)\n",
      "16000 9.5E-07 4.5E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0932],\n",
      "        [1.5490],\n",
      "        [1.2334],\n",
      "        [0.0639],\n",
      "        [0.2502],\n",
      "        [0.0979],\n",
      "        [0.0039]], requires_grad=True)\n",
      "16500 4.0E-06 3.1E-06 8.8E-07 0.0E+00\n",
      "tensor([[0.0918],\n",
      "        [1.5423],\n",
      "        [1.2429],\n",
      "        [0.0619],\n",
      "        [0.2459],\n",
      "        [0.0932],\n",
      "        [0.0038]], requires_grad=True)\n",
      "17000 8.8E-07 3.8E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0923],\n",
      "        [1.5361],\n",
      "        [1.2519],\n",
      "        [0.0622],\n",
      "        [0.2430],\n",
      "        [0.0872],\n",
      "        [0.0033]], requires_grad=True)\n",
      "17500 8.9E-07 4.0E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0913],\n",
      "        [1.5300],\n",
      "        [1.2608],\n",
      "        [0.0605],\n",
      "        [0.2389],\n",
      "        [0.0825],\n",
      "        [0.0031]], requires_grad=True)\n",
      "18000 8.3E-07 3.4E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0913],\n",
      "        [1.5236],\n",
      "        [1.2699],\n",
      "        [0.0602],\n",
      "        [0.2360],\n",
      "        [0.0772],\n",
      "        [0.0028]], requires_grad=True)\n",
      "18500 8.5E-07 3.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0902],\n",
      "        [1.5176],\n",
      "        [1.2786],\n",
      "        [0.0585],\n",
      "        [0.2321],\n",
      "        [0.0727],\n",
      "        [0.0027]], requires_grad=True)\n",
      "19000 3.2E-06 1.8E-06 1.4E-06 0.0E+00\n",
      "tensor([[0.0889],\n",
      "        [1.5114],\n",
      "        [1.2876],\n",
      "        [0.0564],\n",
      "        [0.2282],\n",
      "        [0.0688],\n",
      "        [0.0028]], requires_grad=True)\n",
      "19500 7.9E-07 3.1E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0894],\n",
      "        [1.5056],\n",
      "        [1.2962],\n",
      "        [0.0569],\n",
      "        [0.2259],\n",
      "        [0.0632],\n",
      "        [0.0022]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optim_config_internal = optim_config.copy()\n",
    "optim_config_internal['lambda'] = 0\n",
    "\n",
    "time_deriv_list, theta, coeff_vector_list = train(time_Tensor, Stress_Tensor, network, coeff_vector_list, sparsity_mask_list, lib_config, optim_config_internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this in theory is the whole process done. Only 2 particular lists of tensors are important for us, in this case both lists containing a single tensor, and so simply two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0884],\n",
       "         [1.4997],\n",
       "         [1.3048],\n",
       "         [0.0553],\n",
       "         [0.2222],\n",
       "         [0.0589],\n",
       "         [0.0021]], requires_grad=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into accounts all of these edits, the DeepMoD algorithm can be run in one shot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will restate the config dictionaries and reimport the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sym\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "from deepymod_torch.library_function import mech_library\n",
    "from deepymod_torch.DeepMod import DeepMoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Type = 'Strain'\n",
    "t = sym.symbols('t', real=True)\n",
    "Input_Function = sym.exp(-t) + sym.exp(-t/10) + sym.exp(-t/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/StressStrain/Strain Three_e_Decays E[1, 1, 1] V[1.25, 2.5].csv', delimiter=',')\n",
    "time_Array, Stress_Array = data[:,[0]], data[:,[2]]\n",
    "time_Tensor = torch.tensor(time_Array, dtype=torch.float32, requires_grad=True)\n",
    "Stress_Tensor = torch.tensor(Stress_Array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'max_iterations': 20000}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 15, 'layers': 3, 'output_dim': 1}\n",
    "lib_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': Input_Type, 'input_expr': Input_Function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.4E+00 1.9E+00 1.5E+00 1.1E-05\n",
      "tensor([[1.5623],\n",
      "        [0.7681],\n",
      "        [0.7769],\n",
      "        [0.3311],\n",
      "        [1.7713],\n",
      "        [0.0844],\n",
      "        [0.8914]], requires_grad=True)\n",
      "500 1.3E+00 7.9E-01 5.4E-01 1.4E-05\n",
      "tensor([[ 1.4745],\n",
      "        [ 0.6825],\n",
      "        [ 0.8623],\n",
      "        [ 0.4329],\n",
      "        [ 1.8682],\n",
      "        [-0.0456],\n",
      "        [ 0.9315]], requires_grad=True)\n",
      "1000 7.1E-01 4.3E-01 2.8E-01 1.6E-05\n",
      "tensor([[ 1.4055],\n",
      "        [ 0.6450],\n",
      "        [ 0.8970],\n",
      "        [ 0.5569],\n",
      "        [ 1.7367],\n",
      "        [-0.0801],\n",
      "        [ 1.0094]], requires_grad=True)\n",
      "1500 4.6E-01 2.7E-01 1.9E-01 1.7E-05\n",
      "tensor([[ 1.3390],\n",
      "        [ 0.6163],\n",
      "        [ 0.9218],\n",
      "        [ 0.6526],\n",
      "        [ 1.5977],\n",
      "        [-0.0531],\n",
      "        [ 1.0502]], requires_grad=True)\n",
      "2000 3.2E-01 1.9E-01 1.3E-01 1.8E-05\n",
      "tensor([[1.2775],\n",
      "        [0.5983],\n",
      "        [0.9347],\n",
      "        [0.7289],\n",
      "        [1.4907],\n",
      "        [0.0721],\n",
      "        [0.9882]], requires_grad=True)\n",
      "2500 2.3E-01 1.3E-01 9.9E-02 1.9E-05\n",
      "tensor([[1.2220],\n",
      "        [0.6067],\n",
      "        [0.9187],\n",
      "        [0.7882],\n",
      "        [1.4105],\n",
      "        [0.2328],\n",
      "        [0.8419]], requires_grad=True)\n",
      "3000 1.6E-01 9.0E-02 7.5E-02 2.1E-05\n",
      "tensor([[1.1715],\n",
      "        [0.6435],\n",
      "        [0.8723],\n",
      "        [0.8339],\n",
      "        [1.3583],\n",
      "        [0.3850],\n",
      "        [0.6620]], requires_grad=True)\n",
      "3500 1.1E-01 5.9E-02 5.1E-02 2.2E-05\n",
      "tensor([[1.1232],\n",
      "        [0.6931],\n",
      "        [0.8138],\n",
      "        [0.8713],\n",
      "        [1.3313],\n",
      "        [0.5182],\n",
      "        [0.4979]], requires_grad=True)\n",
      "4000 6.5E-02 3.7E-02 2.8E-02 2.3E-05\n",
      "tensor([[1.0750],\n",
      "        [0.7330],\n",
      "        [0.7665],\n",
      "        [0.9067],\n",
      "        [1.3243],\n",
      "        [0.6227],\n",
      "        [0.3547]], requires_grad=True)\n",
      "4500 3.5E-02 2.2E-02 1.3E-02 2.4E-05\n",
      "tensor([[1.0285],\n",
      "        [0.7510],\n",
      "        [0.7422],\n",
      "        [0.9430],\n",
      "        [1.3283],\n",
      "        [0.6838],\n",
      "        [0.2484]], requires_grad=True)\n",
      "5000 2.1E-02 1.4E-02 6.9E-03 2.5E-05\n",
      "tensor([[0.9874],\n",
      "        [0.7610],\n",
      "        [0.7267],\n",
      "        [0.9753],\n",
      "        [1.3503],\n",
      "        [0.6995],\n",
      "        [0.1828]], requires_grad=True)\n",
      "5500 1.4E-02 9.0E-03 4.6E-03 2.5E-05\n",
      "tensor([[0.9519],\n",
      "        [0.7771],\n",
      "        [0.7058],\n",
      "        [1.0009],\n",
      "        [1.3936],\n",
      "        [0.6886],\n",
      "        [0.1422]], requires_grad=True)\n",
      "6000 8.8E-03 5.9E-03 2.8E-03 2.6E-05\n",
      "tensor([[0.9202],\n",
      "        [0.7988],\n",
      "        [0.6796],\n",
      "        [1.0214],\n",
      "        [1.4481],\n",
      "        [0.6659],\n",
      "        [0.1124]], requires_grad=True)\n",
      "6500 5.4E-03 3.6E-03 1.7E-03 2.6E-05\n",
      "tensor([[0.8909],\n",
      "        [0.8227],\n",
      "        [0.6518],\n",
      "        [1.0386],\n",
      "        [1.5056],\n",
      "        [0.6344],\n",
      "        [0.0927]], requires_grad=True)\n",
      "7000 3.0E-03 1.9E-03 1.0E-03 2.7E-05\n",
      "tensor([[0.8640],\n",
      "        [0.8477],\n",
      "        [0.6233],\n",
      "        [1.0530],\n",
      "        [1.5619],\n",
      "        [0.5977],\n",
      "        [0.0844]], requires_grad=True)\n",
      "7500 1.6E-03 9.5E-04 5.9E-04 2.8E-05\n",
      "tensor([[0.8398],\n",
      "        [0.8712],\n",
      "        [0.5967],\n",
      "        [1.0654],\n",
      "        [1.6098],\n",
      "        [0.5631],\n",
      "        [0.0827]], requires_grad=True)\n",
      "8000 9.0E-04 4.4E-04 4.3E-04 2.8E-05\n",
      "tensor([[0.8196],\n",
      "        [0.8900],\n",
      "        [0.5754],\n",
      "        [1.0755],\n",
      "        [1.6451],\n",
      "        [0.5375],\n",
      "        [0.0825]], requires_grad=True)\n",
      "8500 5.4E-04 3.1E-04 2.0E-04 2.8E-05\n",
      "tensor([[0.8047],\n",
      "        [0.9038],\n",
      "        [0.5598],\n",
      "        [1.0823],\n",
      "        [1.6696],\n",
      "        [0.5225],\n",
      "        [0.0820]], requires_grad=True)\n",
      "9000 4.2E-04 2.4E-04 1.5E-04 2.9E-05\n",
      "tensor([[0.7950],\n",
      "        [0.9132],\n",
      "        [0.5493],\n",
      "        [1.0857],\n",
      "        [1.6860],\n",
      "        [0.5166],\n",
      "        [0.0810]], requires_grad=True)\n",
      "9500 3.7E-04 2.0E-04 1.4E-04 2.9E-05\n",
      "tensor([[0.7892],\n",
      "        [0.9192],\n",
      "        [0.5427],\n",
      "        [1.0866],\n",
      "        [1.6972],\n",
      "        [0.5166],\n",
      "        [0.0793]], requires_grad=True)\n",
      "10000 3.3E-04 1.8E-04 1.3E-04 2.9E-05\n",
      "tensor([[0.7854],\n",
      "        [0.9225],\n",
      "        [0.5389],\n",
      "        [1.0861],\n",
      "        [1.7055],\n",
      "        [0.5195],\n",
      "        [0.0774]], requires_grad=True)\n",
      "10500 3.1E-04 1.6E-04 1.2E-04 2.9E-05\n",
      "tensor([[0.7827],\n",
      "        [0.9236],\n",
      "        [0.5376],\n",
      "        [1.0847],\n",
      "        [1.7118],\n",
      "        [0.5244],\n",
      "        [0.0759]], requires_grad=True)\n",
      "11000 2.9E-04 1.5E-04 1.1E-04 2.9E-05\n",
      "tensor([[0.7804],\n",
      "        [0.9226],\n",
      "        [0.5384],\n",
      "        [1.0827],\n",
      "        [1.7172],\n",
      "        [0.5309],\n",
      "        [0.0745]], requires_grad=True)\n",
      "11500 2.7E-04 1.4E-04 1.0E-04 2.9E-05\n",
      "tensor([[0.7781],\n",
      "        [0.9196],\n",
      "        [0.5411],\n",
      "        [1.0802],\n",
      "        [1.7227],\n",
      "        [0.5386],\n",
      "        [0.0732]], requires_grad=True)\n",
      "12000 2.6E-04 1.3E-04 9.4E-05 2.9E-05\n",
      "tensor([[0.7754],\n",
      "        [0.9147],\n",
      "        [0.5458],\n",
      "        [1.0772],\n",
      "        [1.7287],\n",
      "        [0.5471],\n",
      "        [0.0720]], requires_grad=True)\n",
      "12500 2.4E-04 1.3E-04 8.9E-05 2.9E-05\n",
      "tensor([[0.7724],\n",
      "        [0.9075],\n",
      "        [0.5528],\n",
      "        [1.0736],\n",
      "        [1.7354],\n",
      "        [0.5562],\n",
      "        [0.0711]], requires_grad=True)\n",
      "13000 2.3E-04 1.2E-04 8.3E-05 2.9E-05\n",
      "tensor([[0.7688],\n",
      "        [0.8980],\n",
      "        [0.5622],\n",
      "        [1.0695],\n",
      "        [1.7425],\n",
      "        [0.5660],\n",
      "        [0.0707]], requires_grad=True)\n",
      "13500 2.2E-04 1.2E-04 8.0E-05 2.9E-05\n",
      "tensor([[0.7647],\n",
      "        [0.8864],\n",
      "        [0.5739],\n",
      "        [1.0646],\n",
      "        [1.7500],\n",
      "        [0.5767],\n",
      "        [0.0708]], requires_grad=True)\n",
      "14000 2.1E-04 1.1E-04 7.5E-05 2.9E-05\n",
      "tensor([[0.7600],\n",
      "        [0.8731],\n",
      "        [0.5874],\n",
      "        [1.0592],\n",
      "        [1.7579],\n",
      "        [0.5880],\n",
      "        [0.0713]], requires_grad=True)\n",
      "14500 2.1E-04 1.0E-04 7.2E-05 2.9E-05\n",
      "tensor([[0.7548],\n",
      "        [0.8585],\n",
      "        [0.6024],\n",
      "        [1.0531],\n",
      "        [1.7662],\n",
      "        [0.6002],\n",
      "        [0.0722]], requires_grad=True)\n",
      "15000 2.9E-04 1.5E-04 1.1E-04 3.0E-05\n",
      "tensor([[0.7493],\n",
      "        [0.8432],\n",
      "        [0.6183],\n",
      "        [1.0464],\n",
      "        [1.7747],\n",
      "        [0.6131],\n",
      "        [0.0733]], requires_grad=True)\n",
      "15500 1.9E-04 9.5E-05 6.4E-05 3.0E-05\n",
      "tensor([[0.7434],\n",
      "        [0.8278],\n",
      "        [0.6345],\n",
      "        [1.0394],\n",
      "        [1.7832],\n",
      "        [0.6265],\n",
      "        [0.0746]], requires_grad=True)\n",
      "16000 1.8E-04 9.1E-05 6.1E-05 3.0E-05\n",
      "tensor([[0.7374],\n",
      "        [0.8125],\n",
      "        [0.6506],\n",
      "        [1.0322],\n",
      "        [1.7916],\n",
      "        [0.6401],\n",
      "        [0.0759]], requires_grad=True)\n",
      "16500 1.7E-04 8.6E-05 5.8E-05 3.0E-05\n",
      "tensor([[0.7312],\n",
      "        [0.7973],\n",
      "        [0.6667],\n",
      "        [1.0247],\n",
      "        [1.8001],\n",
      "        [0.6541],\n",
      "        [0.0772]], requires_grad=True)\n",
      "17000 1.9E-04 9.3E-05 6.6E-05 3.0E-05\n",
      "tensor([[0.7250],\n",
      "        [0.7824],\n",
      "        [0.6823],\n",
      "        [1.0172],\n",
      "        [1.8085],\n",
      "        [0.6681],\n",
      "        [0.0784]], requires_grad=True)\n",
      "17500 1.6E-04 7.9E-05 5.2E-05 3.0E-05\n",
      "tensor([[0.7187],\n",
      "        [0.7679],\n",
      "        [0.6975],\n",
      "        [1.0097],\n",
      "        [1.8166],\n",
      "        [0.6820],\n",
      "        [0.0795]], requires_grad=True)\n",
      "18000 1.5E-04 7.4E-05 4.9E-05 3.0E-05\n",
      "tensor([[0.7126],\n",
      "        [0.7539],\n",
      "        [0.7122],\n",
      "        [1.0023],\n",
      "        [1.8247],\n",
      "        [0.6957],\n",
      "        [0.0805]], requires_grad=True)\n",
      "18500 1.6E-04 7.7E-05 5.4E-05 3.0E-05\n",
      "tensor([[0.7065],\n",
      "        [0.7403],\n",
      "        [0.7262],\n",
      "        [0.9948],\n",
      "        [1.8325],\n",
      "        [0.7093],\n",
      "        [0.0814]], requires_grad=True)\n",
      "19000 1.4E-04 6.7E-05 4.4E-05 3.0E-05\n",
      "tensor([[0.7004],\n",
      "        [0.7272],\n",
      "        [0.7397],\n",
      "        [0.9875],\n",
      "        [1.8401],\n",
      "        [0.7226],\n",
      "        [0.0821]], requires_grad=True)\n",
      "19500 1.4E-04 6.4E-05 4.2E-05 3.0E-05\n",
      "tensor([[0.6944],\n",
      "        [0.7145],\n",
      "        [0.7527],\n",
      "        [0.9802],\n",
      "        [1.8476],\n",
      "        [0.7357],\n",
      "        [0.0826]], requires_grad=True)\n",
      "[tensor([[0.6885],\n",
      "        [0.7023],\n",
      "        [0.7651],\n",
      "        [0.9730],\n",
      "        [1.8549],\n",
      "        [0.7485],\n",
      "        [0.0830]], requires_grad=True)] (tensor([[0.6885],\n",
      "        [0.9730],\n",
      "        [1.8549]], requires_grad=True),) (tensor([0, 3, 4]),)\n",
      "Now running final cycle.\n",
      "Epoch | Total loss | MSE | PI | L1 \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (7) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e7ef382dd9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_coeff_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepMoD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_Tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStress_Tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/DeePyMoD_torch_AlexVersion/src/deepymod_torch/DeepMod.py\u001b[0m in \u001b[0;36mDeepMoD\u001b[0;34m(data, target, network_config, library_config, optim_config)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Training of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config_internal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/DeePyMoD_torch_AlexVersion/src/deepymod_torch/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, target, network, coeff_vector_list, sparsity_mask_list, library_config, optim_config)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mcoeff_vector_scaled_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeff_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Calculating PI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/DeePyMoD_torch_AlexVersion/src/deepymod_torch/neural_net.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# Scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mcoeff_vector_scaled_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoeff_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Calculating PI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/DeePyMoD_torch_AlexVersion/src/deepymod_torch/sparsity.py\u001b[0m in \u001b[0;36mscaling\u001b[0;34m(weight_vector, library, time_deriv)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mscaling_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mscaling_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscaled_weight_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_vector\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscaling_theta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mscaling_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_weight_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (7) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "sparse_coeff_vector, sparsity_mask, network = DeepMoD(time_Tensor, Stress_Tensor, network_config, lib_config, optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coeff_vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
