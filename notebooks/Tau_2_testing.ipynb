{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the second characteristic decay time in a viscoelastic model based on a Kelvin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../src')\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.library_function import stress_input_library\n",
    "import deepymod_torch.VE_datagen as VE_datagen\n",
    "import deepymod_torch.VE_params as VE_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set many parameters that define our problem. We define:\n",
    "- The shape, frequency and magnitude of the sinc input manipulation\n",
    "- The model and model type (which in the case that input_type is stress is a Kelvin type)\n",
    "- The sampling rate from the data that will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "omega = 1\n",
    "E = [1, 1, 1]\n",
    "eta = [2.5, 1]\n",
    "input_expr = lambda t: np.sin(t)/(t)\n",
    "dsigma = lambda t: (1/t)*(np.cos(t) - np.sin(t)/(t))\n",
    "input_torch_lambda = lambda t: torch.sin(t)/(t)\n",
    "input_type = 'Stress'\n",
    "func_desc = 'Sinc'\n",
    "number_of_samples = 1000\n",
    "investigated_param = 'Decay Constant 2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data, we need to choose where and when to evaluate the target data fr a given manipulation. Below, we choose those time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = np.linspace(0.00001, 30, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure DeepMoD. We configure:\n",
    "- The initial L1 regularisation penalty\n",
    "- The number of epochs at each stage of training\n",
    "- The size and shape of the network\n",
    "- The library function for calculating potential terms relevant to VE problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'max_iterations': 100001, 'final_run_iterations': 10001}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 40, 'layers': 5, 'output_dim': 1}\n",
    "lib_config = {'type': stress_input_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': input_type, 'input_expr': input_torch_lambda}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the decay constant values we wish to test, in this case those between $10^{-2}$ and $10^3$, doubling each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_2_values = [0.01]\n",
    "while tau_2_values[-1] < 1:\n",
    "    tau_2_values += [tau_2_values[-1]*2]\n",
    "\n",
    "tau_2_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the loop which will generate the data on a tau_2 by tau_2 individual basis for each value in the above code block.\n",
    "\n",
    "With this data, in each iteration of the loop:\n",
    "\n",
    "- The data will be prepared for DeepMoD injection\n",
    "- DeepMoD will try its best\n",
    "- The results will be organised and saved in a named folder.\n",
    "- The progress will be available in Tensorboard files also which will need to be manually dragged across after the loop is done (or during!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for tau_2 value: 0.01\n",
      "Running DeepMoD\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.3E-01 2.6E-01 6.5E-02 1.9E-04\n",
      "tensor([[1.3179],\n",
      "        [0.2597],\n",
      "        [0.6223],\n",
      "        [0.2686],\n",
      "        [0.8649],\n",
      "        [1.1199],\n",
      "        [0.5159]], requires_grad=True)\n",
      "lrs are 0.001 0.001\n",
      "Time elapsed: 0.0 minutes 0.5257399082183838 seconds\n",
      "[tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4885],\n",
      "        [0.3851],\n",
      "        [0.7605],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)] [tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4885],\n",
      "        [0.3851],\n",
      "        [0.7605],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)] [tensor([0, 1, 2, 3, 4, 5, 6])]\n",
      "Now running final cycle.\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 1.3E-01 6.6E-02 6.2E-02 0.0E+00\n",
      "tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4885],\n",
      "        [0.3851],\n",
      "        [0.7605],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)\n",
      "lrs are 0.001 0.001\n",
      "Time elapsed: 0.0 minutes 0.48818016052246094 seconds\n",
      "Saving results\n",
      "Generating data for tau_2 value: 0.02\n",
      "Running DeepMoD\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.3E-01 2.6E-01 6.5E-02 1.9E-04\n",
      "tensor([[1.3179],\n",
      "        [0.2597],\n",
      "        [0.6223],\n",
      "        [0.2686],\n",
      "        [0.8649],\n",
      "        [1.1199],\n",
      "        [0.5159]], requires_grad=True)\n",
      "lrs are 0.001 0.001\n",
      "Time elapsed: 0.0 minutes 0.4162302017211914 seconds\n",
      "[tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4886],\n",
      "        [0.3851],\n",
      "        [0.7604],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)] [tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4886],\n",
      "        [0.3851],\n",
      "        [0.7604],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)] [tensor([0, 1, 2, 3, 4, 5, 6])]\n",
      "Now running final cycle.\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 1.3E-01 6.6E-02 6.3E-02 0.0E+00\n",
      "tensor([[1.2023],\n",
      "        [0.3730],\n",
      "        [0.4886],\n",
      "        [0.3851],\n",
      "        [0.7604],\n",
      "        [0.9977],\n",
      "        [0.6250]], requires_grad=True)\n",
      "lrs are 0.001 0.001\n",
      "Time elapsed: 0.0 minutes 0.5712676048278809 seconds\n",
      "Saving results\n",
      "Generating data for tau_2 value: 0.04\n",
      "Running DeepMoD\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.3E-01 2.6E-01 6.5E-02 1.9E-04\n",
      "tensor([[1.3179],\n",
      "        [0.2597],\n",
      "        [0.6223],\n",
      "        [0.2686],\n",
      "        [0.8649],\n",
      "        [1.1199],\n",
      "        [0.5159]], requires_grad=True)\n",
      "lrs are 0.001 0.001\n",
      "Time elapsed: 0.0 minutes 0.6281638145446777 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2559446570d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# run DeepMoD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running DeepMoD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msparse_coeff_vector_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_coeff_vector_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepMoD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrain_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/working/src/deepymod_torch/DeepMod.py\u001b[0m in \u001b[0;36mDeepMoD\u001b[0;34m(data, target, network_config, library_config, optim_config, NN, coeffs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Training of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config_internal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Scaling + Thresholding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/working/src/deepymod_torch/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, target, network, coeff_vector_list, sparsity_mask_list, library_config, optim_config)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# Optimizer step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for tau_2 in tau_2_values:\n",
    "    \n",
    "    # reset randomised initialisation\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    \n",
    "    # DATA GENERATION\n",
    "    # Include new tau_2 value for this iteration of the loop\n",
    "    eta[1] = tau_2*E[2]\n",
    "    \n",
    "    # generate data\n",
    "    print('Generating data for tau_2 value:', tau_2)\n",
    "    strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=dsigma)\n",
    "    \n",
    "    # reshape data into columns\n",
    "    time_array = time_array.reshape(-1, 1)\n",
    "    strain_array = strain_array.reshape(-1, 1)\n",
    "    \n",
    "    # randomly sample\n",
    "    reordered_row_indices = np.random.permutation(time_array.size)\n",
    "    reduced_time_array = time_array[reordered_row_indices, :][:number_of_samples]\n",
    "    reduced_strain_array = strain_array[reordered_row_indices, :][:number_of_samples]\n",
    "    \n",
    "    \n",
    "    # DEEPMOD PREPARATION\n",
    "    # convert to tensors\n",
    "    time_tensor = torch.tensor(reduced_time_array, dtype=torch.float32, requires_grad=True)\n",
    "    strain_tensor = torch.tensor(reduced_strain_array, dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    # DEEPMOD\n",
    "    # record start time for later transfer of tensorboard files to correct folders\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "    \n",
    "    # run DeepMoD\n",
    "    print('Running DeepMoD')\n",
    "    sparse_coeff_vector_list_list, scaled_coeff_vector_list_list, sparsity_mask_list_list, network = DeepMoD(time_tensor, strain_tensor, network_config, lib_config, optim_config)\n",
    "    print('Saving results')\n",
    "\n",
    "    \n",
    "    # ORGANISING RESULTS\n",
    "    # calculate expected coeffs. Depending on input type, a different model will have been used to intepret the provided model parameters during data generation.\n",
    "    # the choice of model must be taken into account to calculate the coeffs that are correct for the data generated.\n",
    "    if input_type == 'Stress':\n",
    "        expected_coeffs = VE_params.coeffs_from_model_params_kelvin(E, eta)\n",
    "    elif input_type == 'Strain':\n",
    "        expected_coeffs = VE_params.coeffs_from_model_params_maxwell(E, eta)\n",
    "    \n",
    "    # reshape, sample and convert to arrays all series data for homogeneity of form and saving\n",
    "    stress_array = stress_array.reshape(-1,1)\n",
    "    reduced_stress_array = stress_array[reordered_row_indices, :][:number_of_samples]\n",
    "    prediction_array = np.array(network(time_tensor).detach())\n",
    "    \n",
    "    # convert list of expected coeffs to array for saving\n",
    "    target_coeffs_array = np.array(expected_coeffs).reshape(-1,1)\n",
    "    \n",
    "    # convert pre-thresholding coeffs data to arrays for saving\n",
    "    pre_thresh_coeffs_array = np.array(sparse_coeff_vector_list_list[0][0].detach())\n",
    "    pre_thresh_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[0][0].detach())\n",
    "    \n",
    "    # convert final coeffs data to arrays for saving\n",
    "    final_coeffs_array = np.array(sparse_coeff_vector_list_list[-1][0].detach())\n",
    "    final_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[-1][0].detach())\n",
    "    sparsity_mask_array = np.array(sparsity_mask_list_list[-1][0]).reshape(-1,1)\n",
    "    \n",
    "    # group like data vectors together for saving\n",
    "    series_data = np.concatenate((reduced_time_array, reduced_strain_array, reduced_stress_array, prediction_array), axis=1)\n",
    "    pre_thresh_coeffs_data = np.concatenate((pre_thresh_coeffs_array, pre_thresh_scaled_coeffs_array), axis=1)\n",
    "    final_coeffs_data = np.concatenate((final_coeffs_array, final_scaled_coeffs_array, sparsity_mask_array), axis=1)\n",
    "    \n",
    "    # Gather miscellaneous information into lists for saving\n",
    "    DG_info_list = [str(omega), str(E), str(eta), input_type, func_desc]\n",
    "    misc_list = [dt_string, investigated_param, str(tau_2)]\n",
    "\n",
    "\n",
    "    # SAVING RESULTS\n",
    "    # collect parameters to name folder for saving\n",
    "    first_subfolder = investigated_param\n",
    "    second_subfolder = 'param_' + str(tau_2).replace('.', '-')\n",
    "    parent_folder = '../data/Results_tau2_testing'\n",
    "    foldername = parent_folder + '/' + first_subfolder + '/' + second_subfolder\n",
    "\n",
    "    # make folder\n",
    "    if not os.path.isdir(foldername):\n",
    "        os.makedirs(foldername)\n",
    "    \n",
    "    # save all array data\n",
    "    np.savetxt(foldername+'/series_data.csv', series_data, delimiter=',', header='Time, Target_Strain, Stress, Prediction_Strain')\n",
    "    np.savetxt(foldername+'/expected_coeffs.csv', target_coeffs_array, delimiter=',')\n",
    "    np.savetxt(foldername+'/pre_thresh_coeffs_data.csv', pre_thresh_coeffs_data, delimiter=',', header='Prediction_Coeffs, Scaled_Prediction_Coeffs')\n",
    "    np.savetxt(foldername+'/final_coeffs_data.csv', final_coeffs_data, delimiter=',', header='Prediction_Coeffs, Scaled_Prediction_Coeffs, Sparsity_Mask')\n",
    "    \n",
    "    # save all lists data\n",
    "    with open(foldername+'/misc_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in misc_list)\n",
    "\n",
    "    with open(foldername+'/DG_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in DG_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
