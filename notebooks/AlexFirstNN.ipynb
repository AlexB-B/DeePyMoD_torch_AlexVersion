{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex's exploration of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all necessary modules from Numpy and PyTorch (Code copied from neural_net.py).\n",
    "\n",
    "The last import is just so I can slow it down a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a set of input data. I believe this must be as a Torch tensor, but first I will create it as a list. It is completely possible to create it directly as a tensor or a NumPy array also, but I am just taking the long route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = [n for n in range(10)]\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn this into a Torch tensor, we first need to turn it into a NumPy array (Edit: this is not true. torch.tensor can accept lists as well. [3] could be commented out and it wouldn't affect anything)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.array(Input_Data)\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then turn this array into a tensor. Contrary to the guidelines in the 60 minute blitz tutorial say, you can turn a numpy array into a PyTorch tensor simply using the torch.tensor operation. This, crucially, allows you specify the data type as float. For reasons best known to the developers of PyTorch, the datatype of a float is not assumed, and then this creates errors later on.\n",
    "\n",
    "The requires_grad=True is not necessary at least for feedfoward use of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Data = torch.tensor(Input_Data, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to make sure the tensor is not given in terms of columns. Each row is taken later as a seperate instance for training, and all higher dimensions are data associated with that same instance. If we supply our simple tensor as a row vector, it will look like one single instance with 10 input parameters. This, we transpose it. Transposing is not made easy because when the array or tensor is just a vector, python modules like to strip it of its awareness of higher dimensions for reasons best known to the developers of NumPy and PyTorch. The -1 means default to whatever dimension (#rows) necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]], grad_fn=<ViewBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = Input_Data.view(-1, 1)\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the target data, which in this case is the result of performing $y = 2x + 20$ where y is the Target Data and x is the Input_Data. The same method will be used but in one step this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[20.],\n",
      "        [22.],\n",
      "        [24.],\n",
      "        [26.],\n",
      "        [28.],\n",
      "        [30.],\n",
      "        [32.],\n",
      "        [34.],\n",
      "        [36.],\n",
      "        [38.]], grad_fn=<ViewBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "def Poly(x):\n",
    "    y = x*2 + 20\n",
    "    return y\n",
    "\n",
    "Target_Data = [Poly(n) for n in range(10)]\n",
    "Target_Data = np.array(Target_Data)\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data = Target_Data.view(-1,1)\n",
    "print(Target_Data, type(Target_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Data and the corresponding target, which change subject slightly and set up a simple neural network. The design for this will be one that has a single input, 2 hidden layers, each with 3 nodes, and no activation functions. Edit:(the loop fails after at maximum teh first 10 epochs unless some kind of activation function is used, presumably because the results blow up. All results become Nan). There will be one output. Deciding on the shape and functional description of each node should be enough to make a forward pass on the network. To do this we will need to use PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will construct the netowrk the same way as in \"neural_net\" which seems to differ significantly from what I read in the tutorial. In the tutorial, fundamentally, a class was created with certain properties that were then linked together by a method of that class. In DeepMoD, what seems to be occurring is that a list is created of neural network layers, and then some function is used to transform that into a network of some class that i cannot guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of layers, we start with the first hidden layer. It has 1 as the first arguement to specify that each node should expect 1 input (each single element of out input data). It then has a 3 to specify the number of nodes in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = [nn.Linear(1, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the piecewise activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the second hidden layer, which we chose to be identical to the first. This time the 1st arguement is 3 as there were 3 nodes in teh previous layer. In between these two elements is where we would have applied the \"activation function\" if we were including one. append() is a method of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Linear(3, 3))\n",
    "Network.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the output. It is single valued, so the second arguement becomes 1. There is no activation function here as we just want to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply ths funny function that changes it into a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "Torch_Network = nn.Sequential(*Network)\n",
    "print(type(Torch_Network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a built network. We can feed a tensor to this network and get the feedforward output. Currently, the weights and biases have all been set randomly between -1 and 1, so the result will itself be fairly random. However, for a given input, it will be consistant, as no backprop occurs, and the network is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the feedforward output, we can create a tensor of single value that will therefore output a single value. We need to call the number 1 as \"1.\" so that it takes it as a torch.float class object, which otherwise is, for some reason, not assumed. We will output what the network currently tells us for 1 and 20, twice each, to show consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3264], grad_fn=<AddBackward0>)\n",
      "tensor([0.3264], grad_fn=<AddBackward0>)\n",
      "tensor([0.3230], grad_fn=<AddBackward0>)\n",
      "tensor([0.3230], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Torch_Network(torch.tensor([1.])))\n",
    "print(Torch_Network(torch.tensor([1.])))\n",
    "print(Torch_Network(torch.tensor([20.])))\n",
    "print(Torch_Network(torch.tensor([20.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also demonstrate feeding the entire Input_Data Tensor into the network to show how it provides a tensor in return of equal number of rows. the input tensor is always [rowxcolumn] (Number of instances x Number of features or variables for each instance) and the output tensor is always (Number of instances x Number of output results for each instance). As we have the same number of input features as output results per instance, both the input and output tensors have the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3243],\n",
      "        [0.3264],\n",
      "        [0.3271],\n",
      "        [0.3266],\n",
      "        [0.3258],\n",
      "        [0.3249],\n",
      "        [0.3242],\n",
      "        [0.3237],\n",
      "        [0.3234],\n",
      "        [0.3233]], grad_fn=<AddmmBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "Output_Data = Torch_Network(Input_Data)\n",
    "print(Output_Data, type(Output_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now exam the Network a little bit to understand how the parameters are stored. In theory, I have worked out that the number of parameters (catch all term for weights and biases) is equal to\n",
    "\n",
    "$Number of Parameters = \\sum\\limits_{Layer=1} n_{Layer} * (n_{Layer-1} + 1)$\n",
    "\n",
    "Where $n_{Layer}$ is the number of nodes in a given layer and $n_0$ is the number of input features. The $+ 1$ comes from the bias parameters in each layer, and the product of the nodes in 2 layers gives the weights, as each node in the previous layer connects to each node in the current layer.\n",
    "\n",
    "Considering we have 1 input feature and nodes that go 3, 3, 1, where the result of the final one is our output, we expect 22 parameters in our network.\n",
    "\n",
    "We examine parameters using the .parameters() method of tensors, but printing it directly is useless as it is a generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f09be3e0f10> <class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "print(Torch_Network.parameters(), type(Torch_Network.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can pull out the components of this using list. We get a list of length that is equal to twice the number of layers in our system (not including input). In this case, that is 6. Each element of this list is a tensor itself. The reason for the number of tensors is that, for each a layer, one tensor containing the weights, and one containing the biases is produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "Parameter_Tensor = list(Torch_Network.parameters())\n",
    "print(len(Parameter_Tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of tensors produced for each layer, the first is for the weights applied to the inputs from the previous layer. This is generally in the form of a matrix where each row corresponds to a different node in the current layer, and each column corresponds to input from a node in the previous layer. In the first hidden layer, there are 3 nodes and one input per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter # 1 has size torch.Size([3, 1])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([[-0.6825],\n",
      "        [ 0.7378],\n",
      "        [ 0.5528]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Parameter #', 1, 'has size', Parameter_Tensor[0].size())\n",
    "print('And looks like \\n', Parameter_Tensor[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd tensor is for the biases on the nodes in the first layer. As the shape of this tensor is independant of anything but the number of nodes, it is returned as a tensor in the form of a 1 dimensional vector, unspecified as to whether it is a column or row vector. There is an element for each node in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter # 2 has size torch.Size([3])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([-0.2242,  0.1473, -0.7378], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Parameter #', 2, 'has size', Parameter_Tensor[1].size())\n",
    "print('And looks like \\n', Parameter_Tensor[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now examining the 3rd and 4th elements of Parameter_Tensor, we examine the weights and biases of the second layer. The shape of the biases tensor is identical to the first hidden layer, but the weights tensor now is in th form of a 3x3 matrix, as each node in the second hidden layer (each row) receives input from each node in the first hidden layer (each column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter # 3 has size torch.Size([3, 3])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([[-0.2523,  0.2746, -0.0982],\n",
      "        [-0.1485, -0.4616, -0.2932],\n",
      "        [ 0.1923,  0.3346, -0.2605]], requires_grad=True)\n",
      "Parameter # 4 has size torch.Size([3])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([ 0.2470, -0.3116, -0.1488], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Parameter #', 3, 'has size', Parameter_Tensor[2].size())\n",
    "print('And looks like \\n', Parameter_Tensor[2])\n",
    "print('Parameter #', 4, 'has size', Parameter_Tensor[3].size())\n",
    "print('And looks like \\n', Parameter_Tensor[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the output layer, only has one node, and so only 1 bias. It receives input from all three nodes in the previous layer and so it's weights tensor is a row vector, but this time, unlike the bias vector, this row shape is explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter # 5 has size torch.Size([1, 3])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([[0.1714, 0.0276, 0.2445]], requires_grad=True)\n",
      "Parameter # 6 has size torch.Size([1])\n",
      "And looks like \n",
      " Parameter containing:\n",
      "tensor([0.0946], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Parameter #', 5, 'has size', Parameter_Tensor[4].size())\n",
    "print('And looks like \\n', Parameter_Tensor[4])\n",
    "print('Parameter #', 6, 'has size', Parameter_Tensor[5].size())\n",
    "print('And looks like \\n', Parameter_Tensor[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to start to train the neural network. For that, 2 additional things need to be decided upon; the loss function, and the optimisation function. For the former, we will simply use MSE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the optimisation function, we will simply use stochastic gradient descent with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(Torch_Network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowing the simple components of the loop from the Neural Networks part of the PyTorch Tutorial, we combine everything into a loop, so that we train the neural network. The function \"optimizer.zero_grad()\" apparently needs to be run.\n",
    "After that, we calculate the networks prediction on the data. We are feeding it a Tensor of size 10, so it will sequentially (I think) use the network to evaluate each element in turn, and output a tensor of equal size to give the results.\n",
    "The loss is then calculated, I believe this is a tensor as well\n",
    "Then we just sort of *do* the backprop to work out the gradients in loss with respect to each weight and bias\n",
    "Then we trigger the SGD to adjust each bias for each input data element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to end this\n"
     ]
    }
   ],
   "source": [
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Torch_Network(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data)\n",
    "        #time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "\n",
    "print('Ready to end this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as we never used the final iteration of our network to calculate results, we do this last bit one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0458, grad_fn=<MeanBackward0>)\n",
      "tensor([[20.1411],\n",
      "        [21.8972],\n",
      "        [24.0307],\n",
      "        [26.1969],\n",
      "        [28.1849],\n",
      "        [30.0871],\n",
      "        [32.1267],\n",
      "        [34.3484],\n",
      "        [36.4528],\n",
      "        [38.0558]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Output_Data = Torch_Network(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Currently, unless I put in something as simple as $y = 2x + 20$, the NN isn't good enough. Need to play around with\n",
    "\n",
    "- shape\n",
    "\n",
    "- activation functions\n",
    "\n",
    "- learning rate\n",
    "\n",
    "I don't really want to muck around with loss calculation or optimisation functions just yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....\n",
    "Some time Later\n",
    "....\n",
    "\n",
    "If we want to improve the network, we should increase the size of each layer and the number of layers. Let's create a new network, this time with 4 hidden layers, and 30 nodes per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network = nn.Sequential(*[nn.Linear(1, 30), nn.Sigmoid(), nn.Linear(30, 30), nn.Sigmoid(), nn.Linear(30, 30), nn.Sigmoid(),\n",
    "                                nn.Linear(30, 30), nn.Sigmoid(), nn.Linear(30, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of parameters in this network is much larger. It is $30*2 + 30*31 + 30*31 + 30*31 + 1*31 = $..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2881\n"
     ]
    }
   ],
   "source": [
    "print(60 + 3*30*31 + 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't change anything but the shape, so we can now reconfigure the optimiser and then run the training loop as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "tensor(0.0292, grad_fn=<MeanBackward0>)\n",
      "tensor([[19.9913],\n",
      "        [21.8427],\n",
      "        [23.9340],\n",
      "        [25.9418],\n",
      "        [27.8624],\n",
      "        [29.7803],\n",
      "        [31.7710],\n",
      "        [33.8421],\n",
      "        [35.8780],\n",
      "        [37.6839]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer_2 = torch.optim.SGD(Better_Network.parameters(), lr=0.01)\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer_2.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer_2.step()    # Does the update\n",
    "    '''\n",
    "    if n % 100 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data)\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a bit of thought, it is possible to note that any network without activation function will essentially just be simplifiable to a linear equation in each of the input parameters, ie  $y = \\alpha + ax_1 + bx_2 + cx_3$ ... Considering that our input equation is linear (essentially meaning can be expressed as a polynomial of order 1 or 0), we shouldn't need an activation function at all. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "tensor(1.0477e-10, grad_fn=<MeanBackward0>)\n",
      "tensor([[20.0000],\n",
      "        [22.0000],\n",
      "        [24.0000],\n",
      "        [26.0000],\n",
      "        [28.0000],\n",
      "        [30.0000],\n",
      "        [32.0000],\n",
      "        [34.0000],\n",
      "        [36.0000],\n",
      "        [38.0000]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Simple_Network = nn.Sequential(*[nn.Linear(1, 30), nn.Linear(30, 30), nn.Linear(30, 30), nn.Linear(30, 30), nn.Linear(30, 1)])\n",
    "\n",
    "optimizer_3 = torch.optim.SGD(Simple_Network.parameters(), lr=0.0001)\n",
    "Max_Iterations = 1000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer_3.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Simple_Network(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer_3.step()    # Does the update\n",
    "    '''\n",
    "    if n % 100 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data)\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Simple_Network(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is almost perfect, and certainly perfect as far as we could ever reasonably require. Notice however, the learning rate has been set to 0.0001. Setting it at the higher rate of 0.01 lead to Nans by Epoch 10 at the latest, and 0.001 lead to results that were highly variable with a loss that rapidly spiked and reduced - clearly the minimisation was unstable and widely oscillated around an optimal minimisation direction. It may eventually have convergeed, I did not check.\n",
    "\n",
    "With the very low learning rate however, the convergence is rapid and precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Trying a more complicated result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our Better_Network to try and deduce a higher order polynomial: This time, we shall use a larger training data set and aim to fit the Equation $y = x^2 + x + 50$. First of all, we create the data:\n",
    "\n",
    "This time, I have also chosen to use `np.arange` to create a data array in a style more similar to matlab. ie, i could specify `np.arange(start_value, stop_value, step_size)` where only `stop_value` is compulsory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  50   52   56   62   70   80   92  106  122  140  160  182  206  232\n",
      "   260  290  322  356  392  430  470  512  556  602  650  700  752  806\n",
      "   862  920  980 1042 1106 1172 1240 1310 1382 1456 1532 1610 1690 1772\n",
      "  1856 1942 2030 2120 2212 2306 2402 2500 2600 2702 2806 2912 3020 3130\n",
      "  3242 3356 3472 3590 3710 3832 3956 4082 4210 4340 4472 4606 4742 4880\n",
      "  5020 5162 5306 5452 5600 5750 5902 6056 6212 6370 6530 6692 6856 7022\n",
      "  7190 7360 7532 7706 7882 8060 8240 8422 8606 8792 8980 9170 9362 9556\n",
      "  9752 9950]]\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(100)\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "print(Target_Data.reshape(1, -1)) # reshaped just so it fits on the page nicely\n",
    "Input_Data = torch.tensor(Input_Data, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialise the Network:\n",
    "\n",
    "Here I have noted that the `*[]` construction inside `nn.sequential` is only necessary when creating the network over multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3304], grad_fn=<AddBackward0>)\n",
      "tensor([0.3304], grad_fn=<AddBackward0>)\n",
      "tensor([0.3288], grad_fn=<AddBackward0>)\n",
      "tensor([0.3288], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Better_Network = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 1))\n",
    "print(Better_Network(torch.tensor([1.])))\n",
    "print(Better_Network(torch.tensor([1.])))\n",
    "print(Better_Network(torch.tensor([20.])))\n",
    "print(Better_Network(torch.tensor([20.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine the optimizer and run the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "tensor(8887777., grad_fn=<MeanBackward0>)\n",
      "tensor([[3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998, 3382.9998,\n",
      "         3382.9998, 3382.9998, 3382.9998, 3382.9998]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "Loss_Function = nn.MSELoss() # this has been put here just in case one wants to skip beyond the earlier stuff, and so miss out this line.\n",
    "optimizer_4 = torch.optim.SGD(Better_Network.parameters(), lr=0.001)\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer_4.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer_4.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't work it turns out. I believe what is happening is due to 2 things.\n",
    "\n",
    "1. The optimisation is getting stuck on a saddle point or local minimum as the optimisation has no momentum\n",
    "\n",
    "\n",
    "2. The extensive use of Tanh is too much.\n",
    "\n",
    "Instead, let's use the Adam optimiser as in DeepMoD which seems to be a mix of gradient decent with momentum and batch normalisation (Jan says there is no batch normalisation...). We will also change the output of the 4th hidden layer to be activated by the ReLU function which essentially does nothing if the output is positive, and sets it to zero if the output is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "tensor(79.3329, grad_fn=<MeanBackward0>)\n",
      "tensor([[  52.2700,   50.9930,   55.0556,   63.3784,   69.7567,   79.3826,\n",
      "           91.5507,  105.4291,  121.9146,  140.4485,  160.9191,  182.8826,\n",
      "          206.5374,  232.0595,  259.5842,  289.2013,  320.9562,  354.8628,\n",
      "          390.9084,  429.0680,  469.3065,  511.5880,  555.8788,  602.1511,\n",
      "          650.3813,  700.5551,  752.6631,  806.7026,  862.6768,  920.5922,\n",
      "          980.4595, 1042.2909, 1106.1000, 1171.9000, 1239.7051, 1309.5266,\n",
      "         1381.3743, 1455.2565, 1531.1786, 1609.1436, 1689.1501, 1771.1963,\n",
      "         1855.2766, 1941.3848, 2029.5120, 2119.6489, 2211.7859, 2305.9131,\n",
      "         2402.0220, 2500.1050, 2600.1567, 2702.1743, 2806.1572, 2912.1094,\n",
      "         3020.0347, 3129.9417, 3241.8403, 3355.7400, 3471.6501, 3589.5786,\n",
      "         3709.5288, 3831.5022, 3955.4939, 4081.4971, 4209.5029, 4339.5020,\n",
      "         4471.4888, 4605.4619, 4741.4292, 4879.4058, 5019.4097, 5161.4541,\n",
      "         5305.5415, 5451.6450, 5599.7026, 5749.6094, 5901.6226, 6056.2725,\n",
      "         6212.4058, 6370.0288, 6529.3003, 6690.5195, 6854.0747, 7020.3755,\n",
      "         7189.7075, 7361.7173, 7534.5303, 7705.5137, 7876.0898, 8054.8408,\n",
      "         8244.9062, 8428.6602, 8596.7285, 8781.2734, 8993.7891, 9174.2988,\n",
      "         9333.5615, 9564.5156, 9782.5879, 9875.6533]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Better_Network_2.parameters()) # Adam has a default lr which I have chosen to use\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are finally much better. However, it is possible to make them better by scaling the data so that we are dealing with numbers between -1 and 1. I will scale the data by dividing by the maximum value in the array, prior to importing into a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00502513 0.00522613 0.00562814 0.00623116 0.00703518 0.0080402\n",
      "  0.00924623 0.01065327 0.01226131 0.01407035 0.0160804  0.01829146\n",
      "  0.02070352 0.02331658 0.02613065 0.02914573 0.03236181 0.03577889\n",
      "  0.03939698 0.04321608 0.04723618 0.05145729 0.0558794  0.06050251\n",
      "  0.06532663 0.07035176 0.07557789 0.08100503 0.08663317 0.09246231\n",
      "  0.09849246 0.10472362 0.11115578 0.11778894 0.12462312 0.13165829\n",
      "  0.13889447 0.14633166 0.15396985 0.16180905 0.16984925 0.17809045\n",
      "  0.18653266 0.19517588 0.2040201  0.21306533 0.22231156 0.23175879\n",
      "  0.24140704 0.25125628 0.26130653 0.27155779 0.28201005 0.29266332\n",
      "  0.30351759 0.31457286 0.32582915 0.33728643 0.34894472 0.36080402\n",
      "  0.37286432 0.38512563 0.39758794 0.41025126 0.42311558 0.4361809\n",
      "  0.44944724 0.46291457 0.47658291 0.49045226 0.50452261 0.51879397\n",
      "  0.53326633 0.5479397  0.56281407 0.57788945 0.59316583 0.60864322\n",
      "  0.62432161 0.64020101 0.65628141 0.67256281 0.68904523 0.70572864\n",
      "  0.72261307 0.73969849 0.75698492 0.77447236 0.7921608  0.81005025\n",
      "  0.8281407  0.84643216 0.86492462 0.88361809 0.90251256 0.92160804\n",
      "  0.94090452 0.96040201 0.9801005  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(100)\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "\n",
    "# These are the lines where scaling takes place\n",
    "Scaling_Factor = np.amax(Target_Data)\n",
    "Input_Data_Scaled = Input_Data/Scaling_Factor\n",
    "Target_Data_Scaled = Target_Data/Scaling_Factor\n",
    "\n",
    "print(Target_Data_Scaled.reshape(1, -1)) # reshaped just so it fits on the page nicely\n",
    "\n",
    "Input_Data = torch.tensor(Input_Data, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32) #Turn this into a tensor also, just to allow computation of loss\n",
    "Input_Data_Scaled = torch.tensor(Input_Data_Scaled, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data_Scaled = torch.tensor(Target_Data_Scaled, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinitialise the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first instance, we will try to train the network after only scaling the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "tensor(1.5672e-06, grad_fn=<MeanBackward0>)\n",
      "tensor([[0.0049, 0.0049, 0.0054, 0.0058, 0.0065, 0.0076, 0.0089, 0.0103, 0.0120,\n",
      "         0.0139, 0.0159, 0.0180, 0.0202, 0.0225, 0.0255, 0.0290, 0.0325, 0.0362,\n",
      "         0.0399, 0.0438, 0.0477, 0.0517, 0.0558, 0.0599, 0.0642, 0.0685, 0.0739,\n",
      "         0.0801, 0.0863, 0.0926, 0.0990, 0.1054, 0.1120, 0.1186, 0.1253, 0.1321,\n",
      "         0.1389, 0.1458, 0.1527, 0.1606, 0.1693, 0.1781, 0.1869, 0.1957, 0.2046,\n",
      "         0.2136, 0.2226, 0.2316, 0.2406, 0.2496, 0.2594, 0.2705, 0.2817, 0.2928,\n",
      "         0.3039, 0.3150, 0.3261, 0.3372, 0.3482, 0.3592, 0.3701, 0.3821, 0.3959,\n",
      "         0.4095, 0.4230, 0.4364, 0.4497, 0.4629, 0.4759, 0.4888, 0.5016, 0.5166,\n",
      "         0.5322, 0.5476, 0.5628, 0.5777, 0.5925, 0.6070, 0.6212, 0.6382, 0.6557,\n",
      "         0.6729, 0.6898, 0.7063, 0.7226, 0.7385, 0.7541, 0.7717, 0.7911, 0.8100,\n",
      "         0.8286, 0.8467, 0.8644, 0.8838, 0.9035, 0.9228, 0.9416, 0.9599, 0.9778,\n",
      "         0.9937]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Better_Network_2.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data) # Notice here that it is NOT Input_Data_Scaled\n",
    "    Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "print(Loss)\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we need to scale the results back up as well now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is: tensor(155.1595, grad_fn=<MseLossBackward>)\n",
      "tensor([[  48.8473,   48.9315,   53.2814,   57.4380,   64.6736,   75.2767,\n",
      "           88.1173,  102.8596,  119.5233,  137.9920,  157.9742,  179.0984,\n",
      "          201.0042,  223.3885,  253.5789,  288.3331,  323.6436,  360.0324,\n",
      "          397.3939,  435.6455,  474.7223,  514.5760,  555.1672,  596.4682,\n",
      "          638.4562,  681.1106,  735.6388,  796.5303,  858.3719,  921.1119,\n",
      "          984.7042, 1049.1069, 1114.2838, 1180.1959, 1246.8099, 1314.0876,\n",
      "         1381.9955, 1450.4958, 1519.5520, 1598.3336, 1684.8268, 1771.9187,\n",
      "         1859.5593, 1947.6923, 2036.2635, 2125.2153, 2214.4890, 2304.0227,\n",
      "         2393.7571, 2483.6277, 2580.7625, 2691.5605, 2802.4282, 2913.2837,\n",
      "         3024.0452, 3134.6299, 3244.9543, 3354.9373, 3464.4968, 3573.5520,\n",
      "         3682.0229, 3802.2026, 3938.8142, 4074.4907, 4209.1372, 4342.6641,\n",
      "         4474.9814, 4606.0083, 4735.6626, 4863.8687, 4990.5552, 5140.2793,\n",
      "         5295.5347, 5448.7031, 5599.7148, 5748.5024, 5895.0063, 6039.1719,\n",
      "         6180.9521, 6349.7314, 6523.9194, 6695.0498, 6863.0830, 7027.9854,\n",
      "         7189.7310, 7348.3018, 7503.6826, 7678.8057, 7871.2993, 8059.7354,\n",
      "         8244.1182, 8424.4609, 8600.9941, 8793.9072, 8990.0234, 9181.5947,\n",
      "         9368.6602, 9551.2598, 9728.6738, 9887.0889]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "Output_Data = Output_Data*Scaling_Factor\n",
    "\n",
    "print(\"Loss is:\", Loss_Function(Output_Data, Target_Data))\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't immediately seem to work much better, so now we will try with Input Data that was also scaled. First, we reinitialise the network, then we train and see our result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "Loss is: tensor(12228.5029, grad_fn=<MseLossBackward>)\n",
      "tensor([[-7.7099e+01, -5.1873e+01, -2.6623e+01, -1.3481e+00,  2.3947e+01,\n",
      "          4.9264e+01,  7.4604e+01,  9.9966e+01,  1.2535e+02,  1.5076e+02,\n",
      "          1.7619e+02,  2.0164e+02,  2.2711e+02,  2.5260e+02,  2.7812e+02,\n",
      "          3.0366e+02,  3.3170e+02,  3.6725e+02,  4.0282e+02,  4.3842e+02,\n",
      "          4.7403e+02,  5.0968e+02,  5.4534e+02,  5.8102e+02,  6.1673e+02,\n",
      "          6.5246e+02,  6.8820e+02,  7.2398e+02,  7.5977e+02,  7.9558e+02,\n",
      "          8.3141e+02,  8.6726e+02,  9.5209e+02,  1.0387e+03,  1.1253e+03,\n",
      "          1.2119e+03,  1.2985e+03,  1.3852e+03,  1.4719e+03,  1.5586e+03,\n",
      "          1.6453e+03,  1.7320e+03,  1.8188e+03,  1.9056e+03,  1.9923e+03,\n",
      "          2.0792e+03,  2.1660e+03,  2.2528e+03,  2.3397e+03,  2.4266e+03,\n",
      "          2.5134e+03,  2.6003e+03,  2.6927e+03,  2.8120e+03,  2.9313e+03,\n",
      "          3.0506e+03,  3.1700e+03,  3.2894e+03,  3.4088e+03,  3.5282e+03,\n",
      "          3.6476e+03,  3.7671e+03,  3.8865e+03,  4.0060e+03,  4.1254e+03,\n",
      "          4.2449e+03,  4.3644e+03,  4.4839e+03,  4.6034e+03,  4.7230e+03,\n",
      "          4.8425e+03,  4.9620e+03,  5.0816e+03,  5.2011e+03,  5.3674e+03,\n",
      "          5.5404e+03,  5.7133e+03,  5.8863e+03,  6.0592e+03,  6.2322e+03,\n",
      "          6.4052e+03,  6.5781e+03,  6.7511e+03,  6.9241e+03,  7.0971e+03,\n",
      "          7.2701e+03,  7.4430e+03,  7.6160e+03,  7.7890e+03,  7.9620e+03,\n",
      "          8.1350e+03,  8.3079e+03,  8.4809e+03,  8.6539e+03,  8.8268e+03,\n",
      "          8.9998e+03,  9.1728e+03,  9.3457e+03,  9.5186e+03,  9.6915e+03]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Better_Network_2.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data_Scaled) # Notice here that it is NOT Input_Data_Scaled\n",
    "    Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data_Scaled)\n",
    "Output_Data = Output_Data*Scaling_Factor\n",
    "print(\"Loss is:\", Loss_Function(Output_Data, Target_Data))\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this was horrible in comparison. The scale up in the end is an external process to the NN, just to allow us to see how good a job it did more intuitively. We know what the relationship between Target_Data and Target_Data_Scaled is, and if Output_Data approximates Target_Data_Scaled well, then making the same transformation should yeild a small loss between it and Target_Data.\n",
    "\n",
    "So the question is which scaling allows the best approximation of Target_Data_Scaled.\n",
    "\n",
    "- Unscaled Input -> Unscaled Target was alright\n",
    "\n",
    "- Unscaled Input -> Scaled Target was about the same. Running both multiple times shows that either one can perform better on any given attempt.\n",
    "\n",
    "- Scaled Input -> Scaled target produced a terrible approximation.\n",
    "\n",
    "Remember, the issue is not how well the network approximates $y = x^2 + x + 50$, but simply how well it approximates the given target data.\n",
    "\n",
    "The issue is potentially that the scaling factor used is the same for both Data sets. Let us try independantly normalising the two:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00502513 0.00522613 0.00562814 0.00623116 0.00703518 0.0080402\n",
      "  0.00924623 0.01065327 0.01226131 0.01407035 0.0160804  0.01829146\n",
      "  0.02070352 0.02331658 0.02613065 0.02914573 0.03236181 0.03577889\n",
      "  0.03939698 0.04321608 0.04723618 0.05145729 0.0558794  0.06050251\n",
      "  0.06532663 0.07035176 0.07557789 0.08100503 0.08663317 0.09246231\n",
      "  0.09849246 0.10472362 0.11115578 0.11778894 0.12462312 0.13165829\n",
      "  0.13889447 0.14633166 0.15396985 0.16180905 0.16984925 0.17809045\n",
      "  0.18653266 0.19517588 0.2040201  0.21306533 0.22231156 0.23175879\n",
      "  0.24140704 0.25125628 0.26130653 0.27155779 0.28201005 0.29266332\n",
      "  0.30351759 0.31457286 0.32582915 0.33728643 0.34894472 0.36080402\n",
      "  0.37286432 0.38512563 0.39758794 0.41025126 0.42311558 0.4361809\n",
      "  0.44944724 0.46291457 0.47658291 0.49045226 0.50452261 0.51879397\n",
      "  0.53326633 0.5479397  0.56281407 0.57788945 0.59316583 0.60864322\n",
      "  0.62432161 0.64020101 0.65628141 0.67256281 0.68904523 0.70572864\n",
      "  0.72261307 0.73969849 0.75698492 0.77447236 0.7921608  0.81005025\n",
      "  0.8281407  0.84643216 0.86492462 0.88361809 0.90251256 0.92160804\n",
      "  0.94090452 0.96040201 0.9801005  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(100)\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "\n",
    "# These are the lines where scaling takes place\n",
    "Scaling_Factor_Input = np.amax(Input_Data)\n",
    "Scaling_Factor_Target = np.amax(Target_Data)\n",
    "Input_Data_Scaled = Input_Data/Scaling_Factor_Input\n",
    "Target_Data_Scaled = Target_Data/Scaling_Factor_Target\n",
    "\n",
    "print(Target_Data_Scaled.reshape(1, -1)) # reshaped just so it fits on the page nicely\n",
    "\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32) #Turn this into a tensor also, just to allow computation of loss\n",
    "Input_Data_Scaled = torch.tensor(Input_Data_Scaled, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data_Scaled = torch.tensor(Target_Data_Scaled, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Result:\n",
      "Loss is: tensor(159.8869, grad_fn=<MseLossBackward>)\n",
      "tensor([[  48.5908,   53.7471,   58.8083,   63.7739,   71.6121,   80.1517,\n",
      "           88.6266,   97.0361,  117.2570,  137.6169,  157.9832,  178.3547,\n",
      "          198.7278,  225.6299,  258.8672,  292.1520,  325.4800,  358.8480,\n",
      "          392.2490,  425.6808,  459.1374,  502.9512,  553.3946,  603.9490,\n",
      "          654.6070,  705.3601,  756.1992,  807.1157,  858.1019,  909.1488,\n",
      "          960.2452, 1014.7471, 1088.3585, 1162.1637, 1236.1511, 1310.3107,\n",
      "         1384.6320, 1459.1012, 1533.7090, 1608.4413, 1683.2867, 1760.9408,\n",
      "         1850.1373, 1939.4685, 2028.9198, 2118.4749, 2208.1172, 2297.8303,\n",
      "         2387.5969, 2477.3989, 2580.6611, 2690.9771, 2801.3008, 2911.6064,\n",
      "         3021.8674, 3132.0598, 3242.1543, 3352.1267, 3461.9487, 3571.5935,\n",
      "         3681.0317, 3806.6938, 3940.4795, 4074.0957, 4207.5049, 4340.6704,\n",
      "         4473.5566, 4606.1284, 4738.3462, 4870.1743, 5001.5728, 5132.5059,\n",
      "         5280.3901, 5436.6855, 5592.4648, 5747.6816, 5902.2861, 6056.2295,\n",
      "         6209.4653, 6361.9453, 6513.6211, 6664.4463, 6834.7964, 7009.6899,\n",
      "         7183.5576, 7356.3403, 7527.9824, 7698.4229, 7867.6069, 8035.4824,\n",
      "         8201.9932, 8393.7021, 8590.4258, 8785.4473, 8978.7080, 9170.1543,\n",
      "         9359.7334, 9547.3975, 9729.6152, 9906.4229]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Better_Network_2.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data_Scaled) # Notice here that it is NOT Input_Data_Scaled\n",
    "    Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data_Scaled)\n",
    "Output_Data = Output_Data*Scaling_Factor_Target\n",
    "print(\"Loss is:\", Loss_Function(Output_Data, Target_Data))\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this hasn't really helped at all. The approximation is about as good, once again, at no scaling whatsoever. Perhaps the values are too small? Let's try a logaritmic scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember if $\\log_{10}(a) = b$ then $10^b = a$. This will be the reversing transformation at the end.\n",
    "\n",
    "Also, log scaling like this will introduce negative values for elements less than 1. Nothing in my dataset is less than 1 prior to scaling, but of course this is just luck. this ay be an issue simply because I haven't experimented with negative values yet and am not sure what teh affect on choice of activation functions will be (ie sigmoid has no negative output, unlike tanh)\n",
    "\n",
    "Let's just try both applying a logarithm to just the target data and to both the input and the target.\n",
    "\n",
    "First, just the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69897    1.71600334 1.74818803 1.79239169 1.84509804 1.90308999\n",
      "  1.96378783 2.02530587 2.08635983 2.14612804 2.20411998 2.26007139\n",
      "  2.31386722 2.36548798 2.41497335 2.462398   2.50785587 2.55145\n",
      "  2.59328607 2.63346846 2.67209786 2.70926996 2.74507479 2.77959649\n",
      "  2.81291336 2.84509804 2.87621784 2.90633504 2.93550727 2.96378783\n",
      "  2.99122608 3.01786772 3.04375513 3.06892761 3.09342169 3.1172713\n",
      "  3.14050804 3.16316137 3.18525877 3.20682588 3.2278867  3.24846372\n",
      "  3.26857797 3.28824923 3.30749604 3.32633586 3.34478512 3.3628593\n",
      "  3.380573   3.39794001 3.41497335 3.43168534 3.44808767 3.46419137\n",
      "  3.48000694 3.49554434 3.51081301 3.52582195 3.54057972 3.55509445\n",
      "  3.56937391 3.5834255  3.59725628 3.610873   3.6242821  3.63748973\n",
      "  3.65050179 3.66332393 3.67596155 3.68841982 3.70070372 3.712818\n",
      "  3.72476725 3.73655585 3.74818803 3.75966784 3.77099921 3.78218587\n",
      "  3.79323145 3.80413943 3.81491318 3.82555593 3.83607081 3.84646083\n",
      "  3.85672889 3.86687781 3.87691031 3.886829   3.89663643 3.90633504\n",
      "  3.91592721 3.92541524 3.93480134 3.94408768 3.95327634 3.96236934\n",
      "  3.97136864 3.98027614 3.98909369 3.99782308]]\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(100)\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "\n",
    "# These are the lines where scaling takes place\n",
    "Input_Data_Scaled = Input_Data # No change here, but name change to reuse code\n",
    "Target_Data_Scaled = np.log10(Target_Data)\n",
    "\n",
    "print(Target_Data_Scaled.reshape(1, -1)) # reshaped just so it fits on the page nicely\n",
    "\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32) #Turn this into a tensor also, just to allow computation of loss\n",
    "Input_Data_Scaled = torch.tensor(Input_Data_Scaled, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data_Scaled = torch.tensor(Target_Data_Scaled, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now starting training ...\n",
      "Final Result:\n",
      "Loss is: tensor(323.8916, grad_fn=<MseLossBackward>)\n",
      "tensor([[  50.0038,   52.0103,   55.9414,   62.1794,   69.8635,   79.5464,\n",
      "           92.2009,  106.4471,  122.4087,  140.2052,  159.9356,  181.6750,\n",
      "          205.4736,  231.3596,  259.3419,  289.4162,  321.5676,  355.7767,\n",
      "          392.0223,  430.2835,  470.5422,  512.7838,  556.9971,  603.1741,\n",
      "          651.3096,  701.4017,  753.4480,  807.4496,  863.4081,  921.3236,\n",
      "          981.1986, 1043.0355, 1106.8380, 1172.6067, 1240.3473, 1310.0623,\n",
      "         1381.7561, 1455.4355, 1531.1040, 1608.7714, 1688.4447, 1770.1328,\n",
      "         1853.8477, 1939.5975, 2027.3962, 2117.2554, 2209.1924, 2303.2163,\n",
      "         2399.3445, 2496.6638, 2595.8093, 2697.0361, 2800.3540, 2905.7788,\n",
      "         3013.3242, 3122.9995, 3234.8159, 3348.7874, 3464.9155, 3583.2092,\n",
      "         3703.6768, 3826.3203, 3951.1372, 4078.1294, 4207.2949, 4338.6235,\n",
      "         4472.1123, 4607.7446, 4745.5151, 4885.3955, 5027.3706, 5171.4189,\n",
      "         5317.5151, 5465.6289, 5615.7256, 5767.7681, 5921.7197, 6077.5410,\n",
      "         6235.1787, 6394.5918, 6555.7266, 6718.5234, 6882.9258, 7048.8779,\n",
      "         7216.3145, 7385.1704, 7555.3638, 7726.8359, 7899.5020, 8073.2954,\n",
      "         8248.1318, 8423.9473, 8600.6348, 8778.1260, 8956.3350, 9135.1621,\n",
      "         9314.5527, 9494.3896, 9674.6025, 9855.0977]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(Better_Network_2.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "print(\"Now starting training ...\")\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data_Scaled) # Notice here that it is NOT Input_Data_Scaled\n",
    "    Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data_Scaled)\n",
    "Output_Data = 10**Output_Data # Notice the reversal of the logarithm is here, and not a simple scale factor liek before\n",
    "print(\"Loss is:\", Loss_Function(Output_Data, Target_Data))\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, both log scaled:\n",
    "\n",
    "However, note that the first element of Input_Data is 0, so the logarithm of it will be $-\\infty$. This isn't brilliant, but I am just going to not have the 0 for this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.71600334 1.74818803 1.79239169 1.84509804 1.90308999 1.96378783\n",
      "  2.02530587 2.08635983 2.14612804 2.20411998 2.26007139 2.31386722\n",
      "  2.36548798 2.41497335 2.462398   2.50785587 2.55145    2.59328607\n",
      "  2.63346846 2.67209786 2.70926996 2.74507479 2.77959649 2.81291336\n",
      "  2.84509804 2.87621784 2.90633504 2.93550727 2.96378783 2.99122608\n",
      "  3.01786772 3.04375513 3.06892761 3.09342169 3.1172713  3.14050804\n",
      "  3.16316137 3.18525877 3.20682588 3.2278867  3.24846372 3.26857797\n",
      "  3.28824923 3.30749604 3.32633586 3.34478512 3.3628593  3.380573\n",
      "  3.39794001 3.41497335 3.43168534 3.44808767 3.46419137 3.48000694\n",
      "  3.49554434 3.51081301 3.52582195 3.54057972 3.55509445 3.56937391\n",
      "  3.5834255  3.59725628 3.610873   3.6242821  3.63748973 3.65050179\n",
      "  3.66332393 3.67596155 3.68841982 3.70070372 3.712818   3.72476725\n",
      "  3.73655585 3.74818803 3.75966784 3.77099921 3.78218587 3.79323145\n",
      "  3.80413943 3.81491318 3.82555593 3.83607081 3.84646083 3.85672889\n",
      "  3.86687781 3.87691031 3.886829   3.89663643 3.90633504 3.91592721\n",
      "  3.92541524 3.93480134 3.94408768 3.95327634 3.96236934 3.97136864\n",
      "  3.98027614 3.98909369 3.99782308]]\n",
      "Now starting training ...\n",
      "Final Result:\n",
      "Loss is: tensor(250.4840, grad_fn=<MseLossBackward>)\n",
      "tensor([[  52.0000,   55.9875,   61.9678,   70.0364,   80.0163,   91.8406,\n",
      "          106.2001,  121.4965,  140.4959,  160.1178,  181.2967,  206.7637,\n",
      "          233.5714,  261.7154,  291.1924,  321.9979,  354.1274,  387.5765,\n",
      "          426.0715,  467.1666,  510.1732,  555.0978,  601.9436,  650.7134,\n",
      "          701.4078,  754.0248,  808.5621,  865.0131,  923.3734,  983.6320,\n",
      "         1045.7808, 1109.8048, 1175.6923, 1243.4282, 1312.9969, 1384.3779,\n",
      "         1457.5536, 1532.5017, 1609.2024, 1687.6321, 1767.7672, 1849.5801,\n",
      "         1933.0515, 2018.1462, 2105.4917, 2200.4871, 2297.4338, 2396.3118,\n",
      "         2497.0847, 2599.7283, 2704.2117, 2810.5034, 2918.5750, 3028.3955,\n",
      "         3139.9312, 3253.1445, 3368.0132, 3484.4980, 3602.5625, 3722.1846,\n",
      "         3843.3179, 3965.9387, 4090.0100, 4215.4932, 4342.3574, 4470.5708,\n",
      "         4600.1016, 4730.9121, 4862.9697, 4996.2500, 5130.7095, 5267.6655,\n",
      "         5422.4160, 5578.7109, 5736.5059, 5895.7725, 6056.4746, 6218.5620,\n",
      "         6382.0122, 6546.7676, 6712.8091, 6880.0996, 7048.6030, 7218.2798,\n",
      "         7389.0879, 7561.0068, 7734.0034, 7908.0322, 8083.0742, 8259.0791,\n",
      "         8436.0264, 8613.8936, 8792.6436, 8972.2305, 9152.6406, 9333.8350,\n",
      "         9515.7949, 9698.4902, 9881.8818]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(1, 100) # Set a starting value of 1, rather than the default of 0. This results in 1 fewer eleent in the array than in previous examples.\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "\n",
    "# These are the lines where scaling takes place\n",
    "Input_Data_Scaled = np.log10(Input_Data)\n",
    "Target_Data_Scaled = np.log10(Target_Data)\n",
    "\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32) #Turn this into a tensor also, just to allow computation of loss\n",
    "Input_Data_Scaled = torch.tensor(Input_Data_Scaled, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data_Scaled = torch.tensor(Target_Data_Scaled, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "Better_Network_2 = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.Tanh(),\n",
    "                                nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(Better_Network_2.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "print(\"Now starting training ...\")\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Better_Network_2(Input_Data_Scaled) # Notice here that it is NOT Input_Data_Scaled\n",
    "    Loss = Loss_Function(Output_Data, Target_Data_Scaled)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data.view(1, -1)) # reshaped for readability\n",
    "        time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "print('Final Result:')\n",
    "Output_Data = Better_Network_2(Input_Data_Scaled)\n",
    "Output_Data = 10**Output_Data # Notice the reversal of the logarithm is here, and not a simple scale factor liek before\n",
    "print(\"Loss is:\", Loss_Function(Output_Data, Target_Data))\n",
    "print(Output_Data.view(1, -1)) # reshaped for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is nice to see that applying logs isn't breaking anything (as long as all the data is $\\geq 0$), there was also no real advantage. Let's try changing the network again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the `nn.ModuleList` object to test a number of networks of various types to see which manages to produce the lowest loss on the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vary\n",
    "\n",
    "- The amount of layers\n",
    "\n",
    "- The size of the layers\n",
    "\n",
    "- The use of ReLU, Tanh and sigmoid (R, T, S)\n",
    "\n",
    "However i will not change the optimiser now and I will use the optimiser's default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test all the variations, I have created a general purpose permutation function below. The idea is that you tell the function the amount of layers (Instances) and the possible values that each layer can be described by. In this case, I will feed it a list of `[1, 2, 3]` with 1 corresponding to a Sigmoid function, 2 corresponding to a Tanh, and 3 corresponding to a ReLU. However, this translation is not done in this block of code.\n",
    "\n",
    "The logic of the function is that I track 2 lists. I do the leg work on a list that iterates through indices, ie [0, 0, 1, 2] then [0, 0, 1, 3] and then each iteration use this list of indices to create a new list where the value at each point in the index list chooses the value in my new list. I then add this permutation to my grand list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Permutate(Instances, Option_List):\n",
    "    '''\n",
    "    Instances: Integer\n",
    "            Number of elements in list, each of which is expected to be able to take any value given in Option_List\n",
    "    Option_List: List\n",
    "            List of options that each element in the list can be\n",
    "    '''\n",
    "    \n",
    "    Element_Index = -1 # negative as we cycle back from the final index\n",
    "    Option_Index = 0\n",
    "    Option_Total = len(Option_List)\n",
    "    Permutation_Tracker = [0 for _ in range(Instances)] # List of indices used to do the legwork for easier cranking through permutations\n",
    "    Current_Permutation = [Option_List[0] for _ in range(Instances)] # assigns an option to each element based on the index in the current Permutation_Tracker\n",
    "\n",
    "    List_of_Permutations = [list(Current_Permutation)] # The default list of zeroth options can be immediately added\n",
    "  \n",
    "    while True:\n",
    "        if Permutation_Tracker[Element_Index] == Option_Total-1: #flags that the highest option has been reached for this point in the list, and so slides up\n",
    "            Element_Index -= 1\n",
    "            if -1*Element_Index > Instances: #final flag to say that to perform any more permutations, would need to fall off the begining of the list\n",
    "                break\n",
    "\n",
    "        else: # in case where sliding was not triggered, this means that the index at this point in the tracker can be notched up\n",
    "            Permutation_Tracker[Element_Index] += 1\n",
    "            for n in range(Element_Index+1, 0): # and all indices to the right can be zeroed\n",
    "                Permutation_Tracker[n] = 0\n",
    "\n",
    "            Element_Index = -1 # and we begin from the rightmost point again\n",
    "\n",
    "            for i in range(Instances): # uses the indices described by the tracker to create a list from the provided options\n",
    "                Current_Permutation[i] = Option_List[Permutation_Tracker[i]]\n",
    "\n",
    "            List_of_Permutations.append(list(Current_Permutation))\n",
    "\n",
    "    return List_of_Permutations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now use this permutation function to create a list of distinct neural networks, which are appended into a list using `nn.ModuleList`. The idea is that I\n",
    "\n",
    "1. Examine all each number of hidden layers (up to 4)\n",
    "\n",
    "2. For each number of hidden layers, examine the affect on the size of the layers (all size 10, to all size 30)\n",
    "\n",
    "3. For each size and number of hidden layer combination, examine each combination of activation functions at each hidden layer.\n",
    "\n",
    "This last one proved very hard to manage as the amount of permutations is dependant on the number of layers, hence the number of nested loops needed to be a variable itself!\n",
    "\n",
    "The solution was to use my Permutate function above, and create a reference, fully shaped NN, through which I could modify the activation functions present through indexing provided in Permutate().\n",
    "\n",
    "> Note: This probably means, that for each shape of NN, the random initialisation of the weights is the same across all permutations of activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrialNNs = nn.ModuleList([])\n",
    "All_Sizes = []\n",
    "All_Perms = []\n",
    "for layers in [1,2,3,4]:\n",
    "    for size in [10,20,30]:\n",
    "        # Initial shaping of Network\n",
    "        NewNetworkList = [nn.Linear(1, size), nn.Sigmoid()]\n",
    "        for layer in range(1, layers):\n",
    "            NewNetworkList.append(nn.Linear(size, size))\n",
    "            NewNetworkList.append(nn.Sigmoid)\n",
    "            \n",
    "        NewNetworkList.append(nn.Linear(size, 1))  \n",
    "        \n",
    "        \n",
    "        Available_Permutations = Permutate(layers, [1,2,3])\n",
    "        \n",
    "        for Curr_Perm in Available_Permutations: #Curr_Perm is itself a list, containing the indices for defining the activation functions in this permutation\n",
    "            All_Sizes += [size]\n",
    "            All_Perms.append(Curr_Perm)\n",
    "            for Act_layer, Act_Type in enumerate(Curr_Perm):\n",
    "                index = (Act_layer*2)+1 # The activation functions are present at point indexed to be 1, 3, 5 etc (seperated by the nn.Linear)\n",
    "                \n",
    "                if Act_Type == 1:\n",
    "                    NewNetworkList[index] = nn.Sigmoid()\n",
    "                elif Act_Type == 2:\n",
    "                    NewNetworkList[index] = nn.Tanh()\n",
    "                else:\n",
    "                    NewNetworkList[index] = nn.ReLU()\n",
    "                \n",
    "            NewNetwork = nn.Sequential(*NewNetworkList)\n",
    "            TrialNNs.append(NewNetwork)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the list of NNs is ready, we modify the standard loop from before to train each NN in the list in turn, and print the loss for each.\n",
    "\n",
    "I have added a couple of things to speed this process up and print the results in a nicely readable fashion. I have a break flag that aborts networks that aren't converging after 10% of their iterations and I have introduced printing at the end that prints a lot of information about a network that is the best so far (and didn't break at 10% of max iterations), or prints a bit of information if another network comes along that does alright, but isn't the best yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 814.1373901367188\n",
      "And was able to produce an Output of: tensor([[  76.2092,   67.1142,   59.2505,   61.5927,   69.4994,   77.6761,\n",
      "           85.1141,   90.6185,  101.2550,  116.6654,  140.7080,  165.1126,\n",
      "          190.0645,  215.0163,  239.9682,  270.5295,  309.5128,  348.4962,\n",
      "          387.4796,  426.4630,  471.2713,  518.4315,  566.4324,  614.4333,\n",
      "          662.4343,  712.3842,  764.4906,  817.3260,  870.8168,  925.3922,\n",
      "          981.7546, 1045.1870, 1108.6193, 1172.0515, 1235.6450, 1305.1399,\n",
      "         1374.6350, 1444.1301, 1521.9595, 1601.1409, 1683.2501, 1771.3335,\n",
      "         1859.4167, 1947.5001, 2035.5835, 2123.6665, 2211.7500, 2305.7961,\n",
      "         2402.1768, 2499.3613, 2601.5232, 2703.6855, 2805.8477, 2908.3391,\n",
      "         3017.7654, 3128.4053, 3243.4663, 3358.5276, 3473.5886, 3588.6494,\n",
      "         3707.5464, 3833.4949, 3959.4426, 4085.3909, 4211.3389, 4337.2871,\n",
      "         4467.1860, 4602.9795, 4741.6245, 4880.2690, 5021.9370, 5164.7002,\n",
      "         5307.4639, 5451.2622, 5598.9702, 5750.8784, 5903.4316, 6055.9849,\n",
      "         6221.6265, 6392.3638, 6563.1011, 6733.8384, 6904.5757, 7075.3130,\n",
      "         7246.0503, 7416.7876, 7587.5249, 7758.2627, 7928.9995, 8099.7378,\n",
      "         8270.4756, 8441.2119, 8611.9502, 8782.6865, 8953.4238, 9124.1611,\n",
      "         9294.8994, 9465.6357, 9636.3740, 9807.1104]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[ 2.6209e+01,  1.5114e+01,  3.2505e+00, -4.0733e-01, -5.0057e-01,\n",
      "         -2.3239e+00, -6.8859e+00, -1.5382e+01, -2.0745e+01, -2.3335e+01,\n",
      "         -1.9292e+01, -1.6887e+01, -1.5936e+01, -1.6984e+01, -2.0032e+01,\n",
      "         -1.9471e+01, -1.2487e+01, -7.5038e+00, -4.5204e+00, -3.5370e+00,\n",
      "          1.2713e+00,  6.4315e+00,  1.0432e+01,  1.2433e+01,  1.2434e+01,\n",
      "          1.2384e+01,  1.2491e+01,  1.1326e+01,  8.8168e+00,  5.3922e+00,\n",
      "          1.7546e+00,  3.1870e+00,  2.6193e+00,  5.1514e-02, -4.3550e+00,\n",
      "         -4.8601e+00, -7.3650e+00, -1.1870e+01, -1.0041e+01, -8.8591e+00,\n",
      "         -6.7499e+00, -6.6650e-01,  3.4167e+00,  5.5001e+00,  5.5835e+00,\n",
      "          3.6665e+00, -2.5000e-01, -2.0386e-01,  1.7676e-01, -6.3867e-01,\n",
      "          1.5232e+00,  1.6855e+00, -1.5234e-01, -3.6609e+00, -2.2346e+00,\n",
      "         -1.5947e+00,  1.4663e+00,  2.5276e+00,  1.5886e+00, -1.3506e+00,\n",
      "         -2.4536e+00,  1.4949e+00,  3.4426e+00,  3.3909e+00,  1.3389e+00,\n",
      "         -2.7129e+00, -4.8140e+00, -3.0205e+00, -3.7549e-01,  2.6904e-01,\n",
      "          1.9370e+00,  2.7002e+00,  1.4639e+00, -7.3779e-01, -1.0298e+00,\n",
      "          8.7842e-01,  1.4316e+00, -1.5137e-02,  9.6265e+00,  2.2364e+01,\n",
      "          3.3101e+01,  4.1838e+01,  4.8576e+01,  5.3313e+01,  5.6050e+01,\n",
      "          5.6788e+01,  5.5525e+01,  5.2263e+01,  4.7000e+01,  3.9738e+01,\n",
      "          3.0476e+01,  1.9212e+01,  5.9502e+00, -9.3135e+00, -2.6576e+01,\n",
      "         -4.5839e+01, -6.7101e+01, -9.0364e+01, -1.1563e+02, -1.4289e+02]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 160.40664672851562\n",
      "And was able to produce an Output of: tensor([[  79.2517,   79.2517,   79.2517,   79.2517,   79.2517,   79.2517,\n",
      "           79.2517,   79.2517,   79.2517,  123.1877,  160.9389,  188.1779,\n",
      "          211.7360,  233.9322,  260.4960,  291.1337,  321.5029,  351.6555,\n",
      "          390.0182,  431.3142,  472.3806,  513.2305,  553.8765,  594.3325,\n",
      "          641.2083,  696.7637,  752.1826,  807.4898,  862.7103,  917.8716,\n",
      "          976.8475, 1041.6238, 1106.4553, 1171.3798, 1236.4326, 1301.6516,\n",
      "         1373.5392, 1452.1796, 1531.2726, 1610.8676, 1691.0178, 1771.7740,\n",
      "         1853.1886, 1935.3159, 2018.2100, 2104.2935, 2201.4106, 2299.6360,\n",
      "         2399.0330, 2499.6633, 2601.5925, 2704.8809, 2809.5918, 2915.7874,\n",
      "         3023.5261, 3132.8708, 3243.8767, 3356.6021, 3471.1023, 3587.4309,\n",
      "         3705.6384, 3825.7754, 3947.8872, 4072.0193, 4198.2109, 4326.5020,\n",
      "         4456.9253, 4589.5117, 4724.2905, 4861.2822, 5000.5083, 5141.9805,\n",
      "         5285.7104, 5431.7031, 5579.9561, 5730.4668, 5883.2236, 6038.2119,\n",
      "         6195.4072, 6354.7866, 6516.3145, 6679.9531, 6845.6592, 7013.3828,\n",
      "         7183.0684, 7354.6548, 7528.0771, 7703.2637, 7880.1367, 8058.6143,\n",
      "         8238.6123, 8420.0391, 8602.7988, 8786.7930, 8971.9219, 9158.0762,\n",
      "         9345.1543, 9533.0400, 9721.6240, 9910.7920]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[ 29.2517,  27.2517,  23.2517,  17.2517,   9.2517,  -0.7483, -12.7483,\n",
      "         -26.7483, -42.7483, -16.8123,   0.9389,   6.1779,   5.7360,   1.9322,\n",
      "           0.4960,   1.1337,  -0.4971,  -4.3445,  -1.9818,   1.3142,   2.3806,\n",
      "           1.2305,  -2.1235,  -7.6675,  -8.7917,  -3.2363,   0.1826,   1.4898,\n",
      "           0.7103,  -2.1284,  -3.1525,  -0.3762,   0.4553,  -0.6202,  -3.5674,\n",
      "          -8.3484,  -8.4608,  -3.8204,  -0.7274,   0.8676,   1.0178,  -0.2260,\n",
      "          -2.8114,  -6.6841, -11.7900, -15.7065, -10.5894,  -6.3640,  -2.9670,\n",
      "          -0.3367,   1.5925,   2.8809,   3.5918,   3.7874,   3.5261,   2.8708,\n",
      "           1.8767,   0.6021,  -0.8977,  -2.5691,  -4.3616,  -6.2246,  -8.1128,\n",
      "          -9.9807, -11.7891, -13.4980, -15.0747, -16.4883, -17.7095, -18.7178,\n",
      "         -19.4917, -20.0195, -20.2896, -20.2969, -20.0439, -19.5332, -18.7764,\n",
      "         -17.7881, -16.5928, -15.2134, -13.6855, -12.0469, -10.3408,  -8.6172,\n",
      "          -6.9316,  -5.3452,  -3.9229,  -2.7363,  -1.8633,  -1.3857,  -1.3877,\n",
      "          -1.9609,  -3.2012,  -5.2070,  -8.0781, -11.9238, -16.8457, -22.9600,\n",
      "         -30.3760, -39.2080]], grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "For the module of size: 10 , and type: [3, 2, 3]\n",
      "Loss is: 1566.9266357421875\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 43.954681396484375\n",
      "And was able to produce an Output of: tensor([[  12.4959,   37.5194,   58.2878,   67.5165,   75.9384,   84.3982,\n",
      "           93.1669,  106.2883,  121.0966,  137.8428,  156.3619,  178.8241,\n",
      "          204.6447,  231.7018,  260.1464,  292.1784,  325.6326,  358.7171,\n",
      "          392.1392,  427.2770,  465.3823,  507.3908,  554.4659,  603.5879,\n",
      "          653.7806,  704.2731,  754.6413,  806.7447,  862.5853,  919.6476,\n",
      "          977.6464, 1039.1416, 1103.5543, 1170.0078, 1238.5996, 1309.3312,\n",
      "         1382.1355, 1456.9155, 1533.5688, 1612.0129, 1692.1942, 1774.0973,\n",
      "         1857.7422, 1943.1787, 2030.4806, 2119.7322, 2211.0212, 2304.4285,\n",
      "         2400.0178, 2497.8342, 2597.8970, 2700.1968, 2804.7000, 2911.3481,\n",
      "         3020.0652, 3130.7649, 3243.3584, 3357.7678, 3473.9333, 3591.8228,\n",
      "         3711.4414, 3832.8318, 3956.0718, 4081.2722, 4208.5576, 4338.0547,\n",
      "         4469.8672, 4604.0605, 4740.6362, 4879.5293, 5020.6011, 5163.6592,\n",
      "         5308.4873, 5454.8940, 5602.7695, 5751.3896, 5901.4756, 6053.5396,\n",
      "         6208.0156, 6365.3262, 6525.7480, 6689.3037, 6855.7031, 7024.3838,\n",
      "         7194.6387, 7365.7959, 7537.4062, 7709.3691, 7882.0098, 8056.0728,\n",
      "         8232.6445, 8412.9785, 8598.1865, 8788.7930, 8984.2109, 9182.3262,\n",
      "         9379.3945, 9570.4316, 9750.0625, 9913.5312]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[-3.7504e+01, -1.4481e+01,  2.2878e+00,  5.5165e+00,  5.9384e+00,\n",
      "          4.3982e+00,  1.1669e+00,  2.8832e-01, -9.0338e-01, -2.1572e+00,\n",
      "         -3.6381e+00, -3.1759e+00, -1.3553e+00, -2.9816e-01,  1.4636e-01,\n",
      "          2.1784e+00,  3.6326e+00,  2.7171e+00,  1.3916e-01, -2.7230e+00,\n",
      "         -4.6177e+00, -4.6092e+00, -1.5341e+00,  1.5879e+00,  3.7806e+00,\n",
      "          4.2731e+00,  2.6413e+00,  7.4469e-01,  5.8533e-01, -3.5242e-01,\n",
      "         -2.3536e+00, -2.8584e+00, -2.4457e+00, -1.9922e+00, -1.4004e+00,\n",
      "         -6.6882e-01,  1.3550e-01,  9.1553e-01,  1.5688e+00,  2.0129e+00,\n",
      "          2.1942e+00,  2.0973e+00,  1.7422e+00,  1.1787e+00,  4.8059e-01,\n",
      "         -2.6782e-01, -9.7876e-01, -1.5715e+00, -1.9822e+00, -2.1658e+00,\n",
      "         -2.1030e+00, -1.8032e+00, -1.3000e+00, -6.5186e-01,  6.5186e-02,\n",
      "          7.6489e-01,  1.3584e+00,  1.7678e+00,  1.9333e+00,  1.8228e+00,\n",
      "          1.4414e+00,  8.3179e-01,  7.1777e-02, -7.2778e-01, -1.4424e+00,\n",
      "         -1.9453e+00, -2.1328e+00, -1.9395e+00, -1.3638e+00, -4.7070e-01,\n",
      "          6.0107e-01,  1.6592e+00,  2.4873e+00,  2.8940e+00,  2.7695e+00,\n",
      "          1.3896e+00, -5.2441e-01, -2.4604e+00, -3.9844e+00, -4.6738e+00,\n",
      "         -4.2520e+00, -2.6963e+00, -2.9688e-01,  2.3838e+00,  4.6387e+00,\n",
      "          5.7959e+00,  5.4062e+00,  3.3691e+00,  9.7656e-03, -3.9272e+00,\n",
      "         -7.3555e+00, -9.0215e+00, -7.8135e+00, -3.2070e+00,  4.2109e+00,\n",
      "          1.2326e+01,  1.7395e+01,  1.4432e+01, -1.9375e+00, -3.6469e+01]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 21.69670867919922\n",
      "And was able to produce an Output of: tensor([[  53.5263,   53.6079,   55.1183,   60.2045,   68.8449,   80.4085,\n",
      "           93.3754,  106.9467,  121.6659,  138.6890,  158.8420,  182.0349,\n",
      "          207.2742,  233.3757,  260.0493,  288.4966,  320.5081,  356.2600,\n",
      "          393.6421,  431.2263,  469.3506,  510.4448,  556.3769,  601.5763,\n",
      "          649.0999,  699.6660,  752.4575,  806.9202,  862.8193,  920.2220,\n",
      "          979.4297, 1040.8491, 1104.8083, 1171.3737, 1240.2670, 1310.9856,\n",
      "         1383.1049, 1456.5868, 1531.8342, 1609.3850, 1689.4716, 1771.8680,\n",
      "         1856.1796, 1942.2391, 2030.1801, 2120.1907, 2212.2896, 2306.3308,\n",
      "         2402.1694, 2499.7947, 2599.3462, 2701.0288, 2805.0056, 2911.3210,\n",
      "         3019.8904, 3130.5361, 3243.0554, 3357.2898, 3473.1675, 3590.7156,\n",
      "         3710.0461, 3831.3174, 3954.6951, 4080.3174, 4208.2666, 4338.5586,\n",
      "         4471.1411, 4605.9053, 4742.7075, 4881.3921, 5021.8188, 5163.8892,\n",
      "         5307.5640, 5452.8794, 5599.9497, 5748.9585, 5900.1328, 6053.7026,\n",
      "         6209.8462, 6368.6260, 6529.9424, 6693.5142, 6858.9224, 7025.7114,\n",
      "         7193.5205, 7362.2139, 7531.9473, 7703.1704, 7876.5483, 8052.8408,\n",
      "         8232.7363, 8416.6836, 8604.7188, 8796.3252, 8990.3271, 9184.8799,\n",
      "         9377.5615, 9565.5713, 9746.0166, 9916.2295]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[  3.5263,   1.6079,  -0.8817,  -1.7955,  -1.1551,   0.4085,   1.3754,\n",
      "           0.9467,  -0.3341,  -1.3110,  -1.1580,   0.0349,   1.2742,   1.3757,\n",
      "           0.0493,  -1.5034,  -1.4919,   0.2600,   1.6421,   1.2263,  -0.6494,\n",
      "          -1.5552,   0.3769,  -0.4237,  -0.9001,  -0.3340,   0.4575,   0.9202,\n",
      "           0.8193,   0.2220,  -0.5703,  -1.1509,  -1.1917,  -0.6263,   0.2670,\n",
      "           0.9856,   1.1049,   0.5868,  -0.1658,  -0.6150,  -0.5284,  -0.1320,\n",
      "           0.1796,   0.2391,   0.1801,   0.1907,   0.2896,   0.3308,   0.1694,\n",
      "          -0.2053,  -0.6538,  -0.9712,  -0.9944,  -0.6790,  -0.1096,   0.5361,\n",
      "           1.0554,   1.2898,   1.1675,   0.7156,   0.0461,  -0.6826,  -1.3049,\n",
      "          -1.6826,  -1.7334,  -1.4414,  -0.8589,  -0.0947,   0.7075,   1.3921,\n",
      "           1.8188,   1.8892,   1.5640,   0.8794,  -0.0503,  -1.0415,  -1.8672,\n",
      "          -2.2974,  -2.1538,  -1.3740,  -0.0576,   1.5142,   2.9224,   3.7114,\n",
      "           3.5205,   2.2139,  -0.0527,  -2.8296,  -5.4517,  -7.1592,  -7.2637,\n",
      "          -5.3164,  -1.2812,   4.3252,  10.3271,  14.8799,  15.5615,   9.5713,\n",
      "          -5.9834, -33.7705]], grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "For the module of size: 30 , and type: [2, 2, 3]\n",
      "Loss is: 30.525707244873047\n",
      "For the module of size: 30 , and type: [3, 1, 3]\n",
      "Loss is: 28.028100967407227\n",
      "For the module of size: 10 , and type: [2, 1, 3, 3]\n",
      "Loss is: 101.198486328125\n",
      "For the module of size: 10 , and type: [3, 2, 1, 3]\n",
      "Loss is: 132.8745574951172\n",
      "For the module of size: 20 , and type: [1, 2, 1, 3]\n",
      "Loss is: 64.61758422851562\n",
      "For the module of size: 20 , and type: [1, 2, 3, 3]\n",
      "Loss is: 39.80854415893555\n",
      "For the module of size: 20 , and type: [2, 1, 2, 3]\n",
      "Loss is: 53.25148010253906\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=20, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=20, out_features=20, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=20, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 11.275317192077637\n",
      "And was able to produce an Output of: tensor([[  52.9958,   47.0115,   54.8340,   65.4716,   72.2582,   78.7307,\n",
      "           87.4863,  105.5163,  123.6343,  141.3501,  160.2523,  182.7674,\n",
      "          207.2406,  233.2647,  260.2855,  287.6431,  321.2682,  355.0950,\n",
      "          389.9271,  427.7023,  468.2244,  511.5043,  556.6625,  603.7064,\n",
      "          652.5173,  702.8973,  754.6028,  807.3777,  860.9667,  915.1310,\n",
      "          976.5742, 1042.4271, 1108.3347, 1174.1478, 1239.7463, 1306.7020,\n",
      "         1382.4253, 1457.7051, 1532.5317, 1607.7072, 1692.3109, 1776.4928,\n",
      "         1860.3123, 1943.8435, 2027.1647, 2110.3625, 2205.5605, 2305.3208,\n",
      "         2405.2625, 2505.4941, 2606.1221, 2707.2468, 2808.9697, 2911.3840,\n",
      "         3014.5781, 3118.6394, 3230.2073, 3348.3145, 3467.6919, 3588.4153,\n",
      "         3710.5557, 3834.1797, 3959.3486, 4086.1196, 4214.5464, 4344.6816,\n",
      "         4476.5742, 4610.2642, 4745.7959, 4883.2070, 5022.5317, 5163.8057,\n",
      "         5307.0576, 5452.3110, 5599.5967, 5748.9346, 5900.3418, 6053.8325,\n",
      "         6209.4219, 6367.1250, 6526.9385, 6688.8735, 6852.9307, 7019.0991,\n",
      "         7187.3813, 7357.7622, 7530.2217, 7704.7510, 7881.3232, 8059.9121,\n",
      "         8240.4863, 8423.0107, 8607.4463, 8793.7539, 8981.8799, 9171.7725,\n",
      "         9363.3799, 9556.6396, 9751.4834, 9947.8447]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[  2.9958,  -4.9885,  -1.1660,   3.4716,   2.2582,  -1.2693,  -4.5137,\n",
      "          -0.4837,   1.6343,   1.3501,   0.2523,   0.7674,   1.2406,   1.2647,\n",
      "           0.2855,  -2.3569,  -0.7318,  -0.9050,  -2.0729,  -2.2977,  -1.7756,\n",
      "          -0.4957,   0.6625,   1.7064,   2.5173,   2.8973,   2.6028,   1.3777,\n",
      "          -1.0333,  -4.8690,  -3.4258,   0.4271,   2.3347,   2.1478,  -0.2537,\n",
      "          -3.2980,   0.4253,   1.7051,   0.5317,  -2.2928,   2.3109,   4.4928,\n",
      "           4.3123,   1.8435,  -2.8353,  -9.6375,  -6.4395,  -0.6792,   3.2625,\n",
      "           5.4941,   6.1221,   5.2468,   2.9697,  -0.6160,  -5.4219, -11.3606,\n",
      "         -11.7927,  -7.6855,  -4.3081,  -1.5847,   0.5557,   2.1797,   3.3486,\n",
      "           4.1196,   4.5464,   4.6816,   4.5742,   4.2642,   3.7959,   3.2070,\n",
      "           2.5317,   1.8057,   1.0576,   0.3110,  -0.4033,  -1.0654,  -1.6582,\n",
      "          -2.1675,  -2.5781,  -2.8750,  -3.0615,  -3.1265,  -3.0693,  -2.9009,\n",
      "          -2.6187,  -2.2378,  -1.7783,  -1.2490,  -0.6768,  -0.0879,   0.4863,\n",
      "           1.0107,   1.4463,   1.7539,   1.8799,   1.7725,   1.3799,   0.6396,\n",
      "          -0.5166,  -2.1553]], grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "For the module of size: 20 , and type: [2, 3, 1, 3]\n",
      "Loss is: 49.74529266357422\n",
      "For the module of size: 20 , and type: [2, 3, 2, 3]\n",
      "Loss is: 74.29576873779297\n",
      "For the module of size: 20 , and type: [3, 1, 1, 3]\n",
      "Loss is: 70.4299087524414\n",
      "For the module of size: 20 , and type: [3, 2, 1, 3]\n",
      "Loss is: 38.88680648803711\n",
      "For the module of size: 20 , and type: [3, 2, 3, 3]\n",
      "Loss is: 75.2763671875\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 2.261202335357666\n",
      "And was able to produce an Output of: tensor([[  49.2802,   53.3124,   56.5784,   61.4410,   69.7488,   80.1490,\n",
      "           91.8545,  106.3830,  122.1670,  139.1998,  159.3572,  181.5047,\n",
      "          206.0912,  232.6711,  261.1005,  291.1644,  322.6124,  355.8332,\n",
      "          391.1162,  428.6541,  468.5473,  510.8055,  555.3521,  602.0319,\n",
      "          650.6224,  700.8508,  752.4102,  808.0895,  865.1907,  922.7136,\n",
      "          980.3552, 1041.1484, 1102.0887, 1168.3829, 1235.3776, 1308.3221,\n",
      "         1384.1305, 1458.5189, 1533.7750, 1611.3990, 1689.9235, 1773.2369,\n",
      "         1856.6171, 1943.2882, 2030.8649, 2119.8748, 2210.7788, 2303.9375,\n",
      "         2399.5798, 2497.7881, 2598.5037, 2701.5571, 2806.7083, 2913.7065,\n",
      "         3022.3425, 3132.4932, 3244.1426, 3357.3818, 3472.3811, 3589.3508,\n",
      "         3708.4973, 3829.9775, 3953.8762, 4080.1882, 4208.8311, 4339.6660,\n",
      "         4472.5278, 4607.2583, 4743.7314, 4881.8730, 5021.6685, 5163.1572,\n",
      "         5306.4302, 5451.6157, 5598.8574, 5748.2944, 5900.0430, 6054.1636,\n",
      "         6210.6499, 6369.4146, 6530.3105, 6693.1519, 6857.7520, 7023.9766,\n",
      "         7191.7778, 7361.2075, 7532.4014, 7705.5376, 7880.7959, 8058.3262,\n",
      "         8238.2441, 8420.6133, 8605.4102, 8792.4561, 8981.3887, 9171.7598,\n",
      "         9363.2891, 9556.1152, 9750.9707, 9949.2051]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[-0.7198,  1.3124,  0.5784, -0.5590, -0.2512,  0.1490, -0.1455,  0.3830,\n",
      "          0.1670, -0.8002, -0.6428, -0.4953,  0.0912,  0.6711,  1.1005,  1.1644,\n",
      "          0.6124, -0.1668, -0.8838, -1.3459, -1.4527, -1.1945, -0.6479,  0.0319,\n",
      "          0.6224,  0.8508,  0.4102,  2.0895,  3.1907,  2.7136,  0.3552, -0.8516,\n",
      "         -3.9113, -3.6171, -4.6224, -1.6779,  2.1305,  2.5189,  1.7750,  1.3990,\n",
      "         -0.0765,  1.2369,  0.6171,  1.2882,  0.8649, -0.1252, -1.2212, -2.0625,\n",
      "         -2.4202, -2.2119, -1.4963, -0.4429,  0.7083,  1.7065,  2.3425,  2.4932,\n",
      "          2.1426,  1.3818,  0.3811, -0.6492, -1.5027, -2.0225, -2.1238, -1.8118,\n",
      "         -1.1689, -0.3340,  0.5278,  1.2583,  1.7314,  1.8730,  1.6685,  1.1572,\n",
      "          0.4302, -0.3843, -1.1426, -1.7056, -1.9570, -1.8364, -1.3501, -0.5854,\n",
      "          0.3105,  1.1519,  1.7520,  1.9766,  1.7778,  1.2075,  0.4014, -0.4624,\n",
      "         -1.2041, -1.6738, -1.7559, -1.3867, -0.5898,  0.4561,  1.3887,  1.7598,\n",
      "          1.2891,  0.1152, -1.0293, -0.7949]], grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "For the module of size: 30 , and type: [2, 1, 1, 3]\n",
      "Loss is: 11.800049781799316\n",
      "\n",
      " New Record!\n",
      "The new record holder is the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 1.288314700126648\n",
      "And was able to produce an Output of: tensor([[  48.9893,   53.6004,   56.9629,   61.5299,   68.6918,   78.8060,\n",
      "           91.5669,  106.4537,  123.0415,  141.1330,  160.7595,  182.1232,\n",
      "          205.5008,  231.1336,  259.1413,  289.4902,  322.0078,  356.4280,\n",
      "          392.4444,  429.7530,  470.6807,  512.6401,  555.3493,  601.1039,\n",
      "          649.9562,  701.0038,  753.1593,  806.6119,  861.5333,  918.0503,\n",
      "          978.2449, 1041.5542, 1106.5651, 1173.2141, 1241.4276, 1311.1290,\n",
      "         1382.2321, 1454.6270, 1531.1265, 1609.5221, 1689.5016, 1773.0245,\n",
      "         1856.8207, 1940.6993, 2029.0172, 2121.4224, 2213.8364, 2306.3879,\n",
      "         2399.2473, 2496.8635, 2599.5989, 2703.3469, 2808.2742, 2914.5144,\n",
      "         3022.1704, 3131.3145, 3241.9993, 3354.2625, 3468.7168, 3589.4153,\n",
      "         3708.9780, 3831.4080, 3955.6619, 4081.7866, 4209.8369, 4339.8672,\n",
      "         4471.9346, 4606.0957, 4742.4023, 4880.9058, 5021.6504, 5164.6758,\n",
      "         5307.7671, 5452.5371, 5599.5830, 5748.9131, 5900.5234, 6054.3955,\n",
      "         6210.5000, 6368.7871, 6529.2041, 6691.6826, 6856.1572, 7022.5635,\n",
      "         7190.8511, 7360.9844, 7532.9482, 7706.7578, 7882.4463, 8060.0752,\n",
      "         8239.7168, 8421.4492, 8605.3350, 8791.4219, 8979.7061, 9170.1299,\n",
      "         9362.5527, 9556.7520, 9752.3945, 9949.0449]], grad_fn=<ViewBackward>)\n",
      "The difference between this and the target, element-wise, is: tensor([[-1.0107e+00,  1.6004e+00,  9.6294e-01, -4.7012e-01, -1.3082e+00,\n",
      "         -1.1940e+00, -4.3314e-01,  4.5369e-01,  1.0415e+00,  1.1330e+00,\n",
      "          7.5952e-01,  1.2320e-01, -4.9918e-01, -8.6639e-01, -8.5873e-01,\n",
      "         -5.0980e-01,  7.8430e-03,  4.2804e-01,  4.4440e-01, -2.4701e-01,\n",
      "          6.8073e-01,  6.4008e-01, -6.5070e-01, -8.9606e-01, -4.3762e-02,\n",
      "          1.0038e+00,  1.1593e+00,  6.1188e-01, -4.6667e-01, -1.9497e+00,\n",
      "         -1.7551e+00, -4.4580e-01,  5.6506e-01,  1.2141e+00,  1.4276e+00,\n",
      "          1.1290e+00,  2.3206e-01, -1.3730e+00, -8.7354e-01, -4.7791e-01,\n",
      "         -4.9841e-01,  1.0245e+00,  8.2068e-01, -1.3007e+00, -9.8279e-01,\n",
      "          1.4224e+00,  1.8364e+00,  3.8794e-01, -2.7527e+00, -3.1365e+00,\n",
      "         -4.0112e-01,  1.3469e+00,  2.2742e+00,  2.5144e+00,  2.1704e+00,\n",
      "          1.3145e+00, -7.3242e-04, -1.7375e+00, -3.2832e+00, -5.8472e-01,\n",
      "         -1.0220e+00, -5.9204e-01, -3.3813e-01, -2.1338e-01, -1.6309e-01,\n",
      "         -1.3281e-01, -6.5430e-02,  9.5703e-02,  4.0234e-01,  9.0576e-01,\n",
      "          1.6504e+00,  2.6758e+00,  1.7671e+00,  5.3711e-01, -4.1699e-01,\n",
      "         -1.0869e+00, -1.4766e+00, -1.6045e+00, -1.5000e+00, -1.2129e+00,\n",
      "         -7.9590e-01, -3.1738e-01,  1.5723e-01,  5.6348e-01,  8.5107e-01,\n",
      "          9.8438e-01,  9.4824e-01,  7.5781e-01,  4.4629e-01,  7.5195e-02,\n",
      "         -2.8320e-01, -5.5078e-01, -6.6504e-01, -5.7812e-01, -2.9395e-01,\n",
      "          1.2988e-01,  5.5273e-01,  7.5195e-01,  3.9453e-01, -9.5508e-01]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "\n",
      " \n",
      "\n",
      "For the module of size: 30 , and type: [2, 2, 1, 3]\n",
      "Loss is: 7.1528191566467285\n",
      "For the module of size: 30 , and type: [2, 2, 2, 3]\n",
      "Loss is: 12.539352416992188\n",
      "For the module of size: 30 , and type: [2, 3, 1, 3]\n",
      "Loss is: 8.88739013671875\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.arange(100)\n",
    "Input_Data = Input_Data.reshape(-1, 1)\n",
    "Target_Data = Input_Data**2 + Input_Data + 50\n",
    "\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32)\n",
    "Input_Data = torch.tensor(Input_Data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "Max_Iterations = 10000\n",
    "Loss_Tracker = []\n",
    "\n",
    "for NNIndex, TrialModule in enumerate(TrialNNs):\n",
    "    optimizer = torch.optim.Adam(TrialModule.parameters())\n",
    "    break_flag = 0\n",
    "\n",
    "    for n in range(Max_Iterations):\n",
    "        optimizer.zero_grad()   # zero the gradient buffers\n",
    "        Output_Data = TrialModule(Input_Data)\n",
    "        Loss = Loss_Function(Output_Data, Target_Data)\n",
    "        if n > 1000 and Loss > 50000: #arbitrarily chosen numbers. just to ditch lost causes and save time.\n",
    "            break_flag = 1\n",
    "            break\n",
    "        Loss.backward()\n",
    "        optimizer.step()    # Does the update\n",
    "        \n",
    "    Output_Data = TrialModule(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data).item() #I don't want PyTorch to track loss back through previous modules, so this is convenient place to set Loss as a number\n",
    "    \n",
    "    if break_flag:\n",
    "        Loss_Tracker += [Loss]\n",
    "        continue\n",
    "    \n",
    "    if (len(Loss_Tracker) > 0) and (Loss < min(Loss_Tracker)):\n",
    "        print('\\nNew Record!')\n",
    "        print('The new record holder is the network module of description:', TrialModule)\n",
    "        print('It had a loss of', Loss)\n",
    "        print('And was able to produce an Output of:', Output_Data.view(1, -1))\n",
    "        print('The difference between this and the target, element-wise, is:', (Output_Data-Target_Data).view(1, -1))\n",
    "        print('\\n\\n')\n",
    "    elif len(Loss_Tracker) == 0 or Loss < 10*min(Loss_Tracker):\n",
    "        print('For the module of size:', All_Sizes[NNIndex], ', and type:', All_Perms[NNIndex])\n",
    "        print(\"Loss is:\", Loss)\n",
    "    \n",
    "    Loss_Tracker += [Loss]\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the details of the winning network again ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best network was the network module of description: Sequential(\n",
      "  (0): Linear(in_features=1, out_features=30, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (5): Tanh()\n",
      "  (6): Linear(in_features=30, out_features=30, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Linear(in_features=30, out_features=1, bias=True)\n",
      ")\n",
      "It had a loss of 1.288314700126648\n",
      "And was able to produce an Output of: tensor([[-3695.3784, -3679.1431, -3654.6975, -3577.8147, -3463.6873, -3395.9768,\n",
      "         -3358.9016, -3338.3000, -3333.5745, -3346.2144, -3375.6094, -3417.8489,\n",
      "         -3465.9539, -3511.8667, -3549.7742, -3578.2715, -3599.4890, -3616.5039,\n",
      "         -3631.3237, -3644.2495, -3654.5027, -3661.4978, -3665.5903, -3667.8140,\n",
      "         -3669.2480, -3670.6370, -3672.3179, -3674.2957, -3676.3369, -3678.0603,\n",
      "         -3679.0227, -3678.8066, -3677.1003, -3673.7498, -3668.7791, -3662.3923,\n",
      "         -3654.9568, -3646.9690, -3638.9675, -3631.4316, -3624.7024, -3618.9539,\n",
      "         -3614.2146, -3610.4121, -3607.4214, -3605.0999, -3603.3132, -3601.9453,\n",
      "         -3600.8994, -3600.1006, -3599.4910, -3599.0266, -3598.6743, -3598.4089,\n",
      "         -3598.2117, -3598.0691, -3597.9700, -3597.9065, -3597.8721, -3597.8621,\n",
      "         -3597.8726, -3597.9001, -3597.9441, -3597.9993, -3598.0647, -3598.1396,\n",
      "         -3598.2207, -3598.3069, -3598.3953, -3598.4846, -3598.5715, -3598.6543,\n",
      "         -3598.7297, -3598.7942, -3598.8447, -3598.8772, -3598.8870, -3598.8694,\n",
      "         -3598.8176, -3598.7263, -3598.5876, -3598.3923, -3598.1301, -3597.7883,\n",
      "         -3597.3518, -3596.8025, -3596.1174, -3595.2678, -3594.2180, -3592.9243,\n",
      "         -3591.3286, -3589.3604, -3586.9294, -3583.9246, -3580.2061, -3575.6057,\n",
      "         -3569.9197, -3562.9053, -3554.2781, -3543.7080]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "BestIndex = np.argmin(np.array(Loss_Tracker))\n",
    "Good_Network = TrialNNs[BestIndex]\n",
    "print('The best network was the network module of description:', Good_Network)\n",
    "print('It had a loss of', Loss_Tracker[BestIndex])\n",
    "Best_Output = Good_Network(Input_Data)\n",
    "print('And was able to produce an Output of:', Best_Output.view(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so mixed findings here. Let's start with what is positive. Learnings include:\n",
    "\n",
    "- Most networks are bad. Very few produced final loss values of under 100. This tells us not to worry if the network isn't working as it needs to be designed just right!\n",
    "\n",
    "- It is a very good idea to end with a ReLU\n",
    "\n",
    "- Above a certain point, size isn't the most important thing. I ran the above code a few times and the lowest loss was from a network with only 3 layers often, although 4-layer networks never performed worse. This implies that there is a ceiling at which point increasing the number of hidden layers is of little use. For smaller networks however, both increasing the number of layers as well as the size of them helps.\n",
    "    - In fact, the 4 layer networks in particular performed better after the introduction of the early break point. This implies that they converged faster and some good 3 layer networks may have been abandoned prematurely.\n",
    "\n",
    "\n",
    "- Stepping up the activations from Sigmoid -> Tanh -> ReLU seems to work best. This fact seemed consistant the first few times I ran this, but then switched to promoting Tanh -> Sigmoid -> ReLU once I introduced the early break criteria. This could imply that S->T->R does work better, but takes a longer time to converge. A mostly consistant factor seems to be starting with Tanh, regardless of the mix of T and S between this and R at the end, in order to converge faster.\n",
    "\n",
    "- The loop slows down for the larger networks. This is not only because they are more computationally costly, but also because a greater proportion of them probably passed the early break check, even if the loss at the end was still ~10 000. Looking at their final losses supports this.\n",
    "\n",
    "Now the bad. As can be seen when i try to print it at the end. Something perplexing has occurred. The best network has a terrible output!! However, we can see by printing teh record holder's details earlier, in the loop, that this wasn't the case while we were testing. Somehow, the training of the module has been forgotten ...\n",
    "\n",
    "To check what is going on, let's recreate the network and test it fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 62620.8672,  34518.9258,  16187.9854,  10948.6396,  10144.0068,\n",
      "           9339.3682,   8413.7959,   6735.7412,   5524.5654,   4554.2627,\n",
      "           3583.9524,   2613.6477,   1643.3137,    672.9836,   -297.3347,\n",
      "          -1267.6609,  -1801.7507,  -2589.1414,  -3617.5867,  -4646.0361,\n",
      "          -5674.5068,  -6702.9189,  -7731.3623,  -8759.8467,  -9788.3467,\n",
      "         -10816.7920, -11845.3037, -12873.7393, -14234.5420, -15645.6396,\n",
      "         -17056.7539, -18467.8379, -19879.0176, -21290.0723, -22743.3789,\n",
      "         -24517.0957, -24550.3594, -23671.0879, -22791.8066, -21912.5918,\n",
      "         -21033.2129, -20154.0332, -19274.7676, -18395.4238, -17516.1465,\n",
      "         -16636.9512, -15757.6787, -14878.4287, -13999.0186, -13119.8154,\n",
      "         -12240.5420, -11361.2881, -10482.0107,  -9602.6904,  -8723.4834,\n",
      "          -7844.3154,  -6965.0732,  -6085.6436,  -5206.4287,  -4327.1943,\n",
      "          -3447.9519,  -2568.6277,  -1689.4753,   -809.9675,     69.1692,\n",
      "            948.4231,   1827.7512,   2706.9387,   3586.1418,   4465.3994,\n",
      "           5344.8057,   6223.9307,   7103.3447,   7982.6104,   8861.8682,\n",
      "           9741.1885,  10620.4697,  11499.6729,  12378.9072,  13258.1650,\n",
      "          14137.3994,  15016.6963,  15896.0166,  16775.3223,  17654.5723,\n",
      "          18533.8379,  19413.0254,  20292.3066,  21171.4941,  22050.8691,\n",
      "          22930.1973,  23809.2676,  24688.7051,  25567.8535,  26447.2285,\n",
      "          27326.4629,  28209.3535,  29127.0254,  30037.6973,  31325.4941]],\n",
      "       grad_fn=<ViewBackward>)\n",
      "Loss is: 224575456.0\n"
     ]
    }
   ],
   "source": [
    "But_Why_NN = nn.Sequential(nn.Linear(1, 30), nn.Tanh(), nn.Linear(30, 30), nn.Sigmoid(), nn.Linear(30, 30), nn.Tanh(), nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 1))\n",
    "\n",
    "optimizer = torch.optim.Adam(But_Why_NN.parameters())\n",
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = But_Why_NN(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "\n",
    "Output_Data = TrialModule(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data).item()\n",
    "print(Output_Data.view(1, -1))\n",
    "print('Loss is:', Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well then, I give up, I don't know what happened. Something has been overlooked. Even running the final block of code multiple times (to try and see if the network is just very sensitive to initial random parameters) provides consistantly poor results. The network seems to now perform awfully..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
