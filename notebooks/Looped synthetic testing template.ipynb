{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VE DG and DeepMoD testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../src')\n",
    "import deepymod_torch.VE_datagen as VE_datagen\n",
    "import deepymod_torch.VE_params as VE_params\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.library_function import mech_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set many parameters that define our problem. We define:\n",
    "- The model and model type (which is determined by input_type)\n",
    "- The shape, frequency and magnitude of the sinc input manipulation\n",
    "- The sampling rate from the data that will be generated\n",
    "\n",
    "I also here define any other parameters that need to be defined (for saving or the normal flow) but are not being used or tested for this particular use of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigated_param = 'Varying NumPy random seed for problem that works'\n",
    "\n",
    "input_type = 'Strain'\n",
    "E = 3*[0.0005]\n",
    "eta = [0.00022, 0.00004]\n",
    "\n",
    "func_desc = 'Sinc'\n",
    "omega = 2*np.pi\n",
    "Amp = 7\n",
    "input_expr = lambda t: Amp*np.sin(omega*t)/(omega*t)\n",
    "d_input_expr = lambda t: (Amp/t)*(np.cos(omega*t) - np.sin(omega*t)/(omega*t))\n",
    "input_torch_lambda = lambda t: Amp*torch.sin(omega*t)/(omega*t)\n",
    "\n",
    "number_of_samples = 1000\n",
    "noise_level = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data, we need to choose where and when to evaluate the target data for a given manipulation. Below, we choose those time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = np.linspace(10**-10, 10*np.pi/omega, 5000).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the notebook, each trial uses the same data manipulation and mechanical model, so the response data will be identical in each iteration of the loop, and hence this can be calculated in advance, just one single time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=d_input_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of the nb, as all data is calculated in adance, all scaling can be calculated and applied in advance also, and is not different between different iterations of the loop.\n",
    "\n",
    "scaled_omega is the effective omega of manipulation after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_omega = 1.2\n",
    "time_sf = omega/scaled_omega\n",
    "strain_sf = 1/np.max(abs(strain_array))\n",
    "stress_sf = 1/np.max(abs(stress_array))\n",
    "scaled_time_array = time_array*time_sf\n",
    "if input_type == 'Strain':\n",
    "    scaled_input_torch_lambda = lambda t: strain_sf*input_torch_lambda(t/time_sf)\n",
    "    scaled_target_array = stress_array*stress_sf\n",
    "elif input_type == 'Stress':\n",
    "    scaled_input_torch_lambda = lambda t: stress_sf*input_torch_lambda(t/time_sf)\n",
    "    scaled_target_array = strain_array*strain_sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure DeepMoD. We configure:\n",
    "- The initial L1 regularisation penalty\n",
    "- The learning rate\n",
    "- The number of epochs at each stage of training\n",
    "- The size and shape of the network\n",
    "- The library function for calculating potential terms relevant to VE problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = 0.05\n",
    "thresh_pc = lambda *args: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'lr_coeffs': 0.002, 'thresh_func': thresh_pc}\n",
    "network_config = {'hidden_dim': 30}\n",
    "library_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': input_type, 'input_expr': input_torch_lambda}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for saving that doesn't change each loop...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = '../data/Results'\n",
    "first_subfolder = investigated_param.replace('.', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scaling and mechanical models are known and unchanging, so all ceffs, scaled and true can be calculated in advance and are always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.183640318952187],\n",
       "       [0.531654141376734],\n",
       "       [0.550920956856561],\n",
       "       [1.50000000000000],\n",
       "       [0.531654141376735]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if input_type == 'Strain':\n",
    "    model = 'GMM'\n",
    "elif input_type == 'Stress':\n",
    "    model = 'GKM'\n",
    "\n",
    "unscaled_expected_coeffs = VE_params.coeffs_from_model_params(E, eta, model)\n",
    "\n",
    "scaled_expected_coeffs = VE_params.scaled_coeffs_from_true(unscaled_expected_coeffs, time_sf, strain_sf, stress_sf)\n",
    "target_coeffs_array = np.array(scaled_expected_coeffs).reshape(-1,1)\n",
    "target_coeffs_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identified some data wich is the same each loop due to identical manipulation and mech model, identical scaling and processing, and identical configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg_series_data = np.concatenate((time_array, strain_array, stress_array), axis=1)\n",
    "\n",
    "dg_info_list = [f'E: {E}', f'eta: {eta}', 'Input: '+input_type, 'Desc: '+func_desc, f'omega: {omega}', f'Amp: {Amp}']\n",
    "treatment_info_list = [f'noise_factor: {noise_level}', f'time_sf: {time_sf}', f'strain_sf: {strain_sf}', f'stress_sf: {stress_sf}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the values we wish to test.\n",
    "\n",
    "In this use of the notebook it is the value of the PyTorch random seed. The purpose is to investigate if DeepMoD is sensitive to this, does it discover different models given different starting initialisations. Aferwards will want to check if each model replicates essentially the same curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy seeds\n",
    "param_vals_1 = np.arange(10)\n",
    "param_vals_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta_2_values = np.geomspace(1e-6, 2.2e-4, num=11)\n",
    "# eta_2_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the loop. In each iteration of the loop:\n",
    "\n",
    "- Parameter(s) of interest will be updated.\n",
    "- Data will be synthesised from the model and manipulation description.\n",
    "- The data will be prepared for DeepMoD injection\n",
    "- DeepMoD will try its best\n",
    "- The results will be organised and saved in a named folder.\n",
    "- The progress will be available in Tensorboard files also which will need to be manually dragged across after the loop is done (or during!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tests are now (relevant only when looping 2 or more sets of values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tau_2_values)*len(noise_level_values)*number_of_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 | NA\n",
      "100000 NAN NAN NAN NAN NAN\n",
      "tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True)\n",
      "Time elapsed: 42.0 minutes 32.36418676376343 seconds\n",
      "[tensor([[nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan],\n",
      "        [nan]], requires_grad=True)] [tensor([], size=(0, 1), requires_grad=True)] [tensor([], dtype=torch.int64)]\n",
      "Now running final cycle.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 8: lda should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:363",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bf118161f472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# run DeepMoD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running DeepMoD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mcoeff_info_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepMoD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0msparse_coeff_vector_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_coeff_vector_list_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask_list_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoeff_info_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving results'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeePyMoD_torch_AlexVersion/src/deepymod_torch/DeepMod.py\u001b[0m in \u001b[0;36mDeepMoD\u001b[0;34m(data, target, library_config, network_config, optim_config, print_interval, plot)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Training of the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparsity_mask_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibrary_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim_config_internal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Thresholding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeePyMoD_torch_AlexVersion/src/deepymod_torch/neural_net.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, target, network, coeff_vector_list, sparsity_mask_list, library_config, optim_config, print_interval, plot)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Calculating PI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mreg_cost_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msparse_theta\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcoeff_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mloss_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_cost_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/DeePyMoD_torch_AlexVersion/src/deepymod_torch/neural_net.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Calculating PI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mreg_cost_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msparse_theta\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mcoeff_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtime_deriv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_deriv_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_theta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoeff_vector_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mloss_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_cost_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 8: lda should be at least max(1, 0), but have 0 at ../aten/src/TH/generic/THBlas.cpp:363"
     ]
    }
   ],
   "source": [
    "for param_val in param_vals_1:\n",
    "    \n",
    "    print('Starting loop with parameter value:', param_val)\n",
    "    \n",
    "    # Update investigated parameter\n",
    "    np.random.seed(param_val)\n",
    "    \n",
    "    # reset randomised initialisation\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # DATA GENERATION\n",
    "    # Generate data using updated model\n",
    "#     strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=d_input_expr)\n",
    "    \n",
    "    # Scale data\n",
    "#     strain_sf = 1/np.max(abs(strain_array))\n",
    "#     stress_sf = 1/np.max(abs(stress_array))\n",
    "#     if input_type == 'Strain':\n",
    "#         scaled_input_torch_lambda = lambda t: strain_sf*input_torch_lambda(t/t_sf)\n",
    "#         scaled_target_array = stress_array*stress_sf\n",
    "#     elif input_type == 'Stress':\n",
    "#         scaled_input_torch_lambda = lambda t: stress_sf*input_torch_lambda(t/t_sf)\n",
    "#         scaled_target_array = strain_array*strain_sf\n",
    "    \n",
    "    # Add noise to value at each time point\n",
    "#     noisy_strain_array = strain_array + noise_level * np.std(strain_array) * np.random.standard_normal(strain_array.shape)\n",
    "\n",
    "    # randomly sample\n",
    "    reordered_row_indices = np.random.permutation(time_array.size)\n",
    "    reduced_time_array = scaled_time_array[reordered_row_indices, :][:number_of_samples]\n",
    "    reduced_target_array = scaled_target_array[reordered_row_indices, :][:number_of_samples]\n",
    "\n",
    "    # DEEPMOD PREPARATION\n",
    "    # convert to tensors\n",
    "    time_tensor = torch.tensor(reduced_time_array, dtype=torch.float32, requires_grad=True)\n",
    "    target_tensor = torch.tensor(reduced_target_array, dtype=torch.float32)\n",
    "    \n",
    "    # load redefined torch expression for input based on change in omega into config\n",
    "#     library_config['input_expr'] = scaled_input_torch_lambda\n",
    "\n",
    "    \n",
    "    # DEEPMOD\n",
    "    # record start time for later transfer of tensorboard files to correct folders\n",
    "    # start time is always in GMT, regardless of system clock it seems....\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # run DeepMoD\n",
    "    print('Running DeepMoD')\n",
    "    coeff_info_tuple, _, network = DeepMoD(time_tensor, target_tensor, library_config, network_config, optim_config)\n",
    "    sparse_coeff_vector_list_list, scaled_coeff_vector_list_list, sparsity_mask_list_list = coeff_info_tuple\n",
    "    print('Saving results')\n",
    "\n",
    "\n",
    "    # ORGANISING RESULTS\n",
    "    # Calculate unscaled expected coeffs.\n",
    "#     if input_type == 'Stress':\n",
    "#         unscaled_expected_coeffs = VE_params.coeffs_from_model_params_kelvin(E, eta)\n",
    "#     elif input_type == 'Strain':\n",
    "#         unscaled_expected_coeffs = VE_params.coeffs_from_model_params_maxwell(E, eta)\n",
    "\n",
    "    # Scale true coeffs to ones we expect to be found after scaling.\n",
    "    # Convert list of expected coeffs to array for saving\n",
    "#     scaled_expected_coeffs = VE_params.scaled_coeffs_from_true(unscaled_expected_coeffs, t_sf, strain_sf, stress_sf)\n",
    "#     target_coeffs_array = np.array(scaled_expected_coeffs).reshape(-1,1)\n",
    "    \n",
    "    # recalculate prediction from trained network and convert to array for saving\n",
    "    prediction_array = np.array(network(time_tensor).detach().cpu())\n",
    "    \n",
    "    # convert pre-thresholding coeffs data to arrays for saving\n",
    "    pre_thresh_coeffs_array = np.array(sparse_coeff_vector_list_list[0][0].detach().cpu())\n",
    "    pre_thresh_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[0][0].detach().cpu())\n",
    "\n",
    "    # convert final coeffs data to arrays for saving\n",
    "    final_coeffs_array = np.array(sparse_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    final_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    sparsity_mask_array = np.array(sparsity_mask_list_list[-1][0].cpu()).reshape(-1,1)\n",
    "\n",
    "    # group like data vectors together for saving\n",
    "#     dg_series_data = np.concatenate((time_array, strain_array, stress_array), axis=1)\n",
    "    NN_series_data = np.concatenate((reduced_time_array, reduced_target_array, prediction_array), axis=1)\n",
    "    pre_thresh_coeffs_data = np.concatenate((pre_thresh_coeffs_array, pre_thresh_scaled_coeffs_array), axis=1)\n",
    "    final_coeffs_data = np.concatenate((final_coeffs_array, final_scaled_coeffs_array, sparsity_mask_array), axis=1)\n",
    "    \n",
    "    # remove library from dictionary as saved separately\n",
    "    input_theta = library_config.pop('input_theta')\n",
    "    \n",
    "    # Gather miscellaneous information into lists for saving\n",
    "#     dg_info_list = ['E: '+str(E), 'eta: '+str(eta), 'Input: '+input_type, 'Desc: '+func_desc, 'omega: '+str(omega), 'Amp: '+str(Amp)]\n",
    "#     treatment_info_list = ['noise_factor: '+str(noise_level), 'time_sf: '+str(t_sf), 'strain_sf: '+str(strain_sf), 'stress_sf: '+str(stress_sf)]\n",
    "    config_dict_list = [f'optim: {optim_config}', f'network: {network_config}', f'library: {library_config}']\n",
    "    misc_list = ['date_stamp: '+dt_string]\n",
    "\n",
    "\n",
    "    # SAVING RESULTS\n",
    "    # collect parameters to name folder for saving\n",
    "    second_subfolder = 'param_' + str(param_val).replace('.', '-')\n",
    "#     third_subfolder = 'noise_' + str(noise_level).replace('.', '-')\n",
    "#     fourth_subfolder = 'seed_' + str(seed_value)\n",
    "    foldername = parent_folder + '/' + first_subfolder + '/' + second_subfolder# + '/' + third_subfolder + '/' + fourth_subfolder\n",
    "\n",
    "    # make folder\n",
    "    if not os.path.isdir(foldername):\n",
    "        os.makedirs(foldername)\n",
    "\n",
    "    # save all array data\n",
    "    np.savetxt(foldername+'/DG_series_data.csv', dg_series_data, delimiter=',', header='Time, Strain, Stress')\n",
    "    np.savetxt(foldername+'/NN_series_data.csv', NN_series_data, delimiter=',', header='Time, Target, Prediction')\n",
    "    np.savetxt(foldername+'/expected_coeffs.csv', target_coeffs_array, delimiter=',', header='Expected_coeffs')\n",
    "    np.savetxt(foldername+'/pre_thresh_coeffs_data.csv', pre_thresh_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs')\n",
    "    np.savetxt(foldername+'/final_coeffs_data.csv', final_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs, Sparsity_Mask')\n",
    "    np.savetxt(foldername+'/final_library.csv', input_theta, delimiter=',')\n",
    "    \n",
    "    # save all lists data\n",
    "    with open(foldername+'/DG_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in dg_info_list)\n",
    "        \n",
    "    with open(foldername+'/treatment_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in treatment_info_list)\n",
    "    \n",
    "    with open(foldername+'/config_dict_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in config_dict_list)\n",
    "    \n",
    "    with open(foldername+'/misc_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in misc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
