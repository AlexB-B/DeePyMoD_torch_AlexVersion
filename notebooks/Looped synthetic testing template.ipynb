{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VE DG and DeepMoD testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../src')\n",
    "import deepymod_torch.VE_datagen as VE_datagen\n",
    "import deepymod_torch.VE_params as VE_params\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.library_function import mech_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set many parameters that define our problem. We define:\n",
    "- The model and model type (which is determined by input_type)\n",
    "- The shape, frequency and magnitude of the sinc input manipulation\n",
    "- The sampling rate from the data that will be generated\n",
    "\n",
    "I also here define any other parameters that need to be defined (for saving or the normal flow) but are not being used or tested for this particular use of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigated_param = 'Varying tau for real imitation'\n",
    "\n",
    "input_type = 'Strain'\n",
    "E = [5e-4, 5e-4, 5e-4]\n",
    "eta = [2.2e-4, 1] # For this test, we vary the second value of eta, so I assign it a nonsense value of 1 here.\n",
    "\n",
    "func_desc = 'Sinc'\n",
    "omega = 2*np.pi\n",
    "Amp = 7\n",
    "input_expr = lambda t: Amp*np.sin(omega*t)/(omega*t)\n",
    "d_input_expr = lambda t: (Amp/t)*(np.cos(omega*t) - np.sin(omega*t)/(omega*t))\n",
    "input_torch_lambda = lambda t: Amp*torch.sin(omega*t)/(omega*t)\n",
    "\n",
    "number_of_samples = 1000\n",
    "noise_level = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data, we need to choose where and when to evaluate the target data for a given manipulation. Below, we choose those time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = np.linspace(10**-10, 10*np.pi/omega, 5000).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We the scale factor for the x axis data (time) as this is determined by our manipulation description only, not the synthetic response. We can also apply the scaling to the time_array provided we keep the original array also for calculating the synthetic response.\n",
    "\n",
    "scaled_omega is the effective omega of manipulation after scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_omega = 1.2\n",
    "t_sf = omega/scaled_omega\n",
    "scaled_time_array = time_array*t_sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure DeepMoD. We configure:\n",
    "- The initial L1 regularisation penalty\n",
    "- The learning rate\n",
    "- The number of epochs at each stage of training\n",
    "- The size and shape of the network\n",
    "- The library function for calculating potential terms relevant to VE problems.\n",
    "\n",
    "We do not yet put the final lambda function into library_config. This must be added in the loop as it relies on knowing the time scale factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'lr_coeffs': 0.002, 'max_iterations': 100001, 'mse_only_iterations': 20001, 'final_run_iterations': 10001}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 30, 'layers': 4, 'output_dim': 1}\n",
    "library_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': input_type}#, 'input_expr': input_torch_lambda}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for saving that doesn't change each loop...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = '../data/Results'\n",
    "first_subfolder = investigated_param.replace('.', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the values we wish to test.\n",
    "\n",
    "In this use of the notebook it is the value of one of the viscosities used to define our model. We make a geometric progression of values to test a range of magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00022   , 0.00040568, 0.00074806, 0.00137942, 0.00254363,\n",
       "       0.00469042, 0.00864906, 0.01594876, 0.02940929, 0.05423033,\n",
       "       0.1       ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_2_values = np.geomspace(2.2e-4, 1e-1, num=11)\n",
    "eta_2_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the loop. In each iteration of the loop:\n",
    "\n",
    "- Data will be synthesised from the model and manipulation description.\n",
    "- The data will be prepared for DeepMoD injection\n",
    "- DeepMoD will try its best\n",
    "- The results will be organised and saved in a named folder.\n",
    "- The progress will be available in Tensorboard files also which will need to be manually dragged across after the loop is done (or during!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tests are now (relevant when looping 2 sets of values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tau_2_values)*len(noise_level_values)*number_of_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "10000 4.1E-06 2.0E-06 2.1E-06 0.0E+00\n",
      "tensor([[0.2825],\n",
      "        [0.4274],\n",
      "        [1.0014]], requires_grad=True)\n",
      "Time elapsed: 4.0 minutes 7.94896388053894 seconds\n",
      "Saving results\n"
     ]
    }
   ],
   "source": [
    "for param_val in eta_2_values:\n",
    "    \n",
    "    print('Starting loop with parameter value:', param_val)\n",
    "    \n",
    "    # Update investigated parameter\n",
    "    eta[-1] = param_val\n",
    "    \n",
    "    # reset randomised initialisation\n",
    "    np.random.seed(2)\n",
    "    torch.manual_seed(3)\n",
    "\n",
    "    # DATA GENERATION\n",
    "    # Generate data using updated model\n",
    "    strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=d_input_expr)\n",
    "    \n",
    "    # Scale data (y only)\n",
    "    strain_sf = 1/np.max(abs(strain_array))\n",
    "    stress_sf = 1/np.max(abs(stress_array))\n",
    "    if input_type == 'Strain':\n",
    "        scaled_input_torch_lambda = lambda t: strain_sf*input_torch_lambda(t/t_sf)\n",
    "        scaled_target_array = stress_array*stress_sf\n",
    "    elif input_type == 'Stress':\n",
    "        scaled_input_torch_lambda = lambda t: stress_sf*input_torch_lambda(t/t_sf)\n",
    "        scaled_target_array = strain_array*strain_sf\n",
    "    \n",
    "    # Add noise to value at each time point\n",
    "#     noisy_strain_array = strain_array + noise_level * np.std(strain_array) * np.random.standard_normal(strain_array.shape)\n",
    "\n",
    "    # randomly sample\n",
    "    reordered_row_indices = np.random.permutation(time_array.size)\n",
    "    reduced_time_array = scaled_time_array[reordered_row_indices, :][:number_of_samples]\n",
    "    reduced_target_array = scaled_target_array[reordered_row_indices, :][:number_of_samples]\n",
    "\n",
    "\n",
    "    # DEEPMOD PREPARATION\n",
    "    # convert to tensors\n",
    "    time_tensor = torch.tensor(reduced_time_array, dtype=torch.float32, requires_grad=True)\n",
    "    target_tensor = torch.tensor(reduced_target_array, dtype=torch.float32)\n",
    "    \n",
    "    # load redefined torch expression for input based on change in omega into config\n",
    "    library_config['input_expr'] = scaled_input_torch_lambda\n",
    "\n",
    "    \n",
    "    # DEEPMOD\n",
    "    # record start time for later transfer of tensorboard files to correct folders\n",
    "    # start time is always in GMT, regardless of system clock it seems....\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # run DeepMoD\n",
    "    print('Running DeepMoD')\n",
    "    sparse_coeff_vector_list_list, scaled_coeff_vector_list_list, sparsity_mask_list_list, network = DeepMoD(time_tensor, target_tensor, network_config, library_config, optim_config)\n",
    "    print('Saving results')\n",
    "\n",
    "\n",
    "    # ORGANISING RESULTS\n",
    "    # Calculate unscaled expected coeffs.\n",
    "    if input_type == 'Stress':\n",
    "        unscaled_expected_coeffs = VE_params.coeffs_from_model_params_kelvin(E, eta)\n",
    "    elif input_type == 'Strain':\n",
    "        unscaled_expected_coeffs = VE_params.coeffs_from_model_params_maxwell(E, eta)\n",
    "\n",
    "    # Scale true coeffs to ones we expect to be found after scaling.\n",
    "    # Convert list of expected coeffs to array for saving\n",
    "    scaled_expected_coeffs = VE_params.scaled_coeffs_from_true(unscaled_expected_coeffs, t_sf, strain_sf, stress_sf)\n",
    "    target_coeffs_array = np.array(scaled_expected_coeffs).reshape(-1,1)\n",
    "    \n",
    "    # recalculate prediction from trained network and convert to array for saving\n",
    "    prediction_array = np.array(network(time_tensor).detach().cpu())\n",
    "    \n",
    "    # convert pre-thresholding coeffs data to arrays for saving\n",
    "    pre_thresh_coeffs_array = np.array(sparse_coeff_vector_list_list[0][0].detach().cpu())\n",
    "    pre_thresh_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[0][0].detach().cpu())\n",
    "\n",
    "    # convert final coeffs data to arrays for saving\n",
    "    final_coeffs_array = np.array(sparse_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    final_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    sparsity_mask_array = np.array(sparsity_mask_list_list[-1][0].cpu()).reshape(-1,1)\n",
    "\n",
    "    # group like data vectors together for saving\n",
    "    dg_series_data = np.concatenate((time_array, strain_array, stress_array), axis=1)\n",
    "    NN_series_data = np.concatenate((reduced_time_array, reduced_target_array, prediction_array), axis=1)\n",
    "    pre_thresh_coeffs_data = np.concatenate((pre_thresh_coeffs_array, pre_thresh_scaled_coeffs_array), axis=1)\n",
    "    final_coeffs_data = np.concatenate((final_coeffs_array, final_scaled_coeffs_array, sparsity_mask_array), axis=1)\n",
    "    \n",
    "    # remove library from dictionary as we don't want to save that\n",
    "    input_theta = library_config.pop('input_theta')\n",
    "    \n",
    "    # Gather miscellaneous information into lists for saving\n",
    "    dg_info_list = ['E: '+str(E), 'eta: '+str(eta), 'Input: '+input_type, 'Desc: '+func_desc, 'omega: '+str(omega), 'Amp: '+str(Amp)]\n",
    "    treatment_info_list = ['noise_factor: '+str(noise_level), 'time_sf: '+str(t_sf), 'strain_sf: '+str(strain_sf), 'stress_sf: '+str(stress_sf)]\n",
    "    config_dict_list = ['optim: '+str(optim_config), 'network: '+str(network_config), 'library: '+str(library_config)]\n",
    "    misc_list = ['date_stamp: '+dt_string]\n",
    "\n",
    "\n",
    "    # SAVING RESULTS\n",
    "    # collect parameters to name folder for saving\n",
    "    second_subfolder = 'param_' + str(param_val).replace('.', '-')\n",
    "#     third_subfolder = 'noise_' + str(noise_level).replace('.', '-')\n",
    "#     fourth_subfolder = 'seed_' + str(seed_value)\n",
    "    foldername = parent_folder + '/' + first_subfolder + '/' + second_subfolder# + '/' + third_subfolder + '/' + fourth_subfolder\n",
    "\n",
    "    # make folder\n",
    "    if not os.path.isdir(foldername):\n",
    "        os.makedirs(foldername)\n",
    "\n",
    "    # save all array data\n",
    "    np.savetxt(foldername+'/DG_series_data.csv', dg_series_data, delimiter=',', header='Time, Strain, Stress')\n",
    "    np.savetxt(foldername+'/NN_series_data.csv', NN_series_data, delimiter=',', header='Time, Target, Prediction')\n",
    "    np.savetxt(foldername+'/expected_coeffs.csv', target_coeffs_array, delimiter=',', header='Expected_coeffs')\n",
    "    np.savetxt(foldername+'/pre_thresh_coeffs_data.csv', pre_thresh_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs')\n",
    "    np.savetxt(foldername+'/final_coeffs_data.csv', final_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs, Sparsity_Mask')\n",
    "    \n",
    "    # save all lists data\n",
    "    with open(foldername+'/DG_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in dg_info_list)\n",
    "        \n",
    "    with open(foldername+'/treatment_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in treatment_info_list)\n",
    "    \n",
    "    with open(foldername+'/config_dict_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in config_dict_list)\n",
    "    \n",
    "    with open(foldername+'/misc_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in misc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
