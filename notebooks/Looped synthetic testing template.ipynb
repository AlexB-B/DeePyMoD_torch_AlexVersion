{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying the second characteristic decay time and noise level in a viscoelastic model based on a Kelvin model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "sys.path.append('../src')\n",
    "import deepymod_torch.VE_datagen as VE_datagen\n",
    "import deepymod_torch.VE_params as VE_params\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.library_function import mech_library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set many parameters that define our problem. We define:\n",
    "- The shape, frequency and magnitude of the sinc input manipulation\n",
    "- The model and model type (which in the case that input_type is stress is a Kelvin type)\n",
    "- The sampling rate from the data that will be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# omega = 1 # Today, this is the parameter to vary\n",
    "E = [1, 1, 1]\n",
    "eta = [2.5, 0.5]\n",
    "Amp = 7\n",
    "# input_expr = lambda t: Amp*np.sin(omega*t)/(omega*t)\n",
    "# d_input_expr = lambda t: (Amp/t)*(np.cos(omega*t) - np.sin(omega*t)/(omega*t))\n",
    "# input_torch_lambda = lambda t: Amp*torch.sin(omega*t)/(omega*t)\n",
    "input_type = 'Stress'\n",
    "func_desc = 'Sinc'\n",
    "number_of_samples = 1000\n",
    "investigated_param = 'Varying manipulation omega'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate data, we need to choose where and when to evaluate the target data fr a given manipulation. Below, we choose those time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_array = np.linspace(10**-10, 10*np.pi/omega, 5000).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we configure DeepMoD. We configure:\n",
    "- The initial L1 regularisation penalty\n",
    "- The number of epochs at each stage of training\n",
    "- The size and shape of the network\n",
    "- The library function for calculating potential terms relevant to VE problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'lr_coeffs': 0.002, 'max_iterations': 2, 'mse_only_iterations': 2, 'final_run_iterations': 2}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 30, 'layers': 4, 'output_dim': 1}\n",
    "library_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': input_type}#, 'input_expr': input_torch_lambda}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the decay constant values we wish to test, in this case those between $10^{-2}$ and $10^3$, doubling each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau_2_values = [0.01]\n",
    "# while tau_2_values[-1] < 1000:\n",
    "#     tau_2_values += [tau_2_values[-1]*2]\n",
    "\n",
    "# tau_2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5. , 5.2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# omega_values = np.arange(5, 7.5, 0.2)\n",
    "omega_values = np.arange(5, 5.3, 0.2)\n",
    "omega_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the noise levels that we are going to test. In each case the noise level refers to the percentage of the standard deviation on the whole data set used to scale a noise adjustment that would otherwise be of the magnitude of a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0\n",
    "# noise_level_values = [0, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, each combination of $\\tau_2$ and noise level will be tested 5 times, each time with a different random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_seeds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the loop which will generate the data on a tau_2 by tau_2 individual basis for each value above, as well as testing each of these tau_2 values at all noise levels of interest as specified above.\n",
    "\n",
    "With this data, in each iteration of the loop:\n",
    "\n",
    "- The data will be prepared for DeepMoD injection\n",
    "- DeepMoD will try its best\n",
    "- The results will be organised and saved in a named folder.\n",
    "- The progress will be available in Tensorboard files also which will need to be manually dragged across after the loop is done (or during!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of tests are now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tau_2_values)*len(noise_level_values)*number_of_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this particular test doesn't change the model each test, the expected coefficients are the same each time, and can be calculated outside the loop, so that they are not recalculated every loop.\n",
    "\n",
    "The coeffs DeepMoD should find vary a little however due to small variations in teh scaling performed each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.333333333333333,\n",
       " 0.416666666666667,\n",
       " 1.00000000000000,\n",
       " 2.00000000000000,\n",
       " 0.416666666666667]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if input_type == 'Stress':\n",
    "    unscaled_expected_coeffs = VE_params.coeffs_from_model_params_kelvin(E, eta)\n",
    "elif input_type == 'Strain':\n",
    "    unscaled_expected_coeffs = VE_params.coeffs_from_model_params_maxwell(E, eta)\n",
    "\n",
    "unscaled_expected_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information for saving that doesn't change each loop...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_folder = '../data/Results'\n",
    "first_subfolder = investigated_param.replace('.', '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 5.8E+01 6.9E-02 5.8E+01 0.0E+00\n",
      "tensor([[0.1597],\n",
      "        [1.6958],\n",
      "        [0.1607],\n",
      "        [0.7869],\n",
      "        [1.2044],\n",
      "        [0.2287],\n",
      "        [0.7270]], requires_grad=True)\n",
      "Time elapsed: 0.0 minutes 0.04239916801452637 seconds\n",
      "Saving results\n",
      "sleeping...\n"
     ]
    }
   ],
   "source": [
    "for omega in omega_values:\n",
    "    \n",
    "    print('Generating data for omega value:', omega)\n",
    "    \n",
    "    # reset randomised initialisation\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # DATA GENERATION\n",
    "    # redefine manipulation profile per new omega\n",
    "    input_expr = lambda t: Amp*np.sin(omega*t)/(omega*t)\n",
    "    d_input_expr = lambda t: (Amp/t)*(np.cos(omega*t) - np.sin(omega*t)/(omega*t))\n",
    "    input_torch_lambda = lambda t: Amp*torch.sin(omega*t)/(omega*t)\n",
    "    \n",
    "    # Generate different length time series to correspond to same oscillations with new omega\n",
    "    time_array = np.linspace(10**-10, 10*np.pi/omega, 5000).reshape(-1, 1)\n",
    "\n",
    "    # generate data\n",
    "    strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=d_input_expr)\n",
    "    \n",
    "    # Scale data (y scaling only as testing x scaling)\n",
    "    strain_sf = 1/np.max(abs(strain_array))\n",
    "    stress_sf = 1/np.max(abs(stress_array))\n",
    "    if input_type == 'Strain':\n",
    "        scaled_input_torch_lambda = lambda t: strain_sf*input_torch_lambda(t)\n",
    "        scaled_target_array = stress_array*stress_sf\n",
    "    elif input_type == 'Stress':\n",
    "        scaled_input_torch_lambda = lambda t: stress_sf*input_torch_lambda(t)\n",
    "        scaled_target_array = strain_array*strain_sf\n",
    "    \n",
    "    # add noise to value at each time point\n",
    "#     noisy_strain_array = strain_array + noise_level * np.std(strain_array) * np.random.standard_normal(strain_array.shape)\n",
    "\n",
    "    # randomly sample\n",
    "    reordered_row_indices = np.random.permutation(time_array.size)\n",
    "    reduced_time_array = time_array[reordered_row_indices, :][:number_of_samples]\n",
    "    reduced_target_array = scaled_target_array[reordered_row_indices, :][:number_of_samples]\n",
    "\n",
    "\n",
    "    # DEEPMOD PREPARATION\n",
    "    # convert to tensors\n",
    "    time_tensor = torch.tensor(reduced_time_array, dtype=torch.float32, requires_grad=True)\n",
    "    target_tensor = torch.tensor(reduced_target_array, dtype=torch.float32)\n",
    "    \n",
    "    # load redefined torch expression for input based on change in omega into config\n",
    "    library_config['input_expr'] = scaled_input_torch_lambda\n",
    "\n",
    "    \n",
    "    # DEEPMOD\n",
    "    # record start time for later transfer of tensorboard files to correct folders\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    # run DeepMoD\n",
    "    print('Running DeepMoD')\n",
    "    sparse_coeff_vector_list_list, scaled_coeff_vector_list_list, sparsity_mask_list_list, network = DeepMoD(time_tensor, target_tensor, network_config, library_config, optim_config)\n",
    "    print('Saving results')\n",
    "\n",
    "\n",
    "    # ORGANISING RESULTS\n",
    "    # calculate expected coeffs. Depending on input type, a different model will have been used to intepret the provided model parameters during data generation.\n",
    "    # the choice of model must be taken into account to calculate the coeffs that are correct for the data generated.\n",
    "#     if input_type == 'Stress':\n",
    "#         expected_coeffs = VE_params.coeffs_from_model_params_kelvin(E, eta)\n",
    "#     elif input_type == 'Strain':\n",
    "#         expected_coeffs = VE_params.coeffs_from_model_params_maxwell(E, eta)\n",
    "\n",
    "    # reshape, sample and convert to arrays all series data for homogeneity of form and saving\n",
    "#     stress_array = stress_array.reshape(-1,1)\n",
    "#     reduced_stress_array = stress_array[reordered_row_indices, :][:number_of_samples]\n",
    "    prediction_array = np.array(network(time_tensor).detach().cpu())\n",
    "\n",
    "    # Scale true coeffs to ones we expect to be found after scaling.\n",
    "    # Convert list of expected coeffs to array for saving\n",
    "    scaled_expected_coeffs = VE_params.scaled_coeffs_from_true(unscaled_expected_coeffs, 1, strain_sf, stress_sf)\n",
    "    target_coeffs_array = np.array(scaled_expected_coeffs).reshape(-1,1)\n",
    "\n",
    "    # convert pre-thresholding coeffs data to arrays for saving\n",
    "    pre_thresh_coeffs_array = np.array(sparse_coeff_vector_list_list[0][0].detach().cpu())\n",
    "    pre_thresh_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[0][0].detach().cpu())\n",
    "\n",
    "    # convert final coeffs data to arrays for saving\n",
    "    final_coeffs_array = np.array(sparse_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    final_scaled_coeffs_array = np.array(scaled_coeff_vector_list_list[-1][0].detach().cpu())\n",
    "    sparsity_mask_array = np.array(sparsity_mask_list_list[-1][0].cpu()).reshape(-1,1)\n",
    "\n",
    "    # group like data vectors together for saving\n",
    "    dg_series_data = np.concatenate((time_array, strain_array, stress_array), axis=1)\n",
    "    NN_series_data = np.concatenate((reduced_time_array, reduced_target_array, prediction_array), axis=1)\n",
    "    pre_thresh_coeffs_data = np.concatenate((pre_thresh_coeffs_array, pre_thresh_scaled_coeffs_array), axis=1)\n",
    "    final_coeffs_data = np.concatenate((final_coeffs_array, final_scaled_coeffs_array, sparsity_mask_array), axis=1)\n",
    "    \n",
    "    # remove library from dictionary as we don't want to save that\n",
    "    input_theta = library_config.pop('input_theta')\n",
    "    \n",
    "    # Gather miscellaneous information into lists for saving\n",
    "    dg_info_list = ['E: '+str(E), 'eta: '+str(eta), 'Input: '+input_type, 'Desc: '+func_desc, 'omega: '+str(omega), 'Amp: '+str(Amp)]\n",
    "    treatment_info_list = ['noise_factor: '+str(noise_level), 'time_sf: '+'1', 'strain_sf: '+str(strain_sf), 'stress_sf: '+str(stress_sf)]\n",
    "    config_dict_list = ['optim: '+str(optim_config), 'network: '+str(network_config), 'library: '+str(library_config)]\n",
    "    misc_list = ['date_stamp: '+dt_string]\n",
    "\n",
    "\n",
    "    # SAVING RESULTS\n",
    "    # collect parameters to name folder for saving\n",
    "    second_subfolder = 'param_' + str(omega).replace('.', '-')\n",
    "#     third_subfolder = 'noise_' + str(noise_level).replace('.', '-')\n",
    "#     fourth_subfolder = 'seed_' + str(seed_value)\n",
    "    foldername = parent_folder + '/' + first_subfolder + '/' + second_subfolder# + '/' + third_subfolder + '/' + fourth_subfolder\n",
    "\n",
    "    # make folder\n",
    "    if not os.path.isdir(foldername):\n",
    "        os.makedirs(foldername)\n",
    "\n",
    "    # save all array data\n",
    "    np.savetxt(foldername+'/DG_series_data.csv', dg_series_data, delimiter=',', header='Time, Strain, Stress')\n",
    "    np.savetxt(foldername+'/NN_series_data.csv', NN_series_data, delimiter=',', header='Time, Target, Prediction')\n",
    "    np.savetxt(foldername+'/expected_coeffs.csv', target_coeffs_array, delimiter=',', header='Expected_coeffs')\n",
    "    np.savetxt(foldername+'/pre_thresh_coeffs_data.csv', pre_thresh_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs')\n",
    "    np.savetxt(foldername+'/final_coeffs_data.csv', final_coeffs_data, delimiter=',', header='Trained_Coeffs, Scaled_Trained_Coeffs, Sparsity_Mask')\n",
    "    \n",
    "    # save all lists data\n",
    "    with open(foldername+'/DG_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in dg_info_list)\n",
    "        \n",
    "    with open(foldername+'/treatment_info_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in treatment_info_list)\n",
    "    \n",
    "    with open(foldername+'/config_dict_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in config_dict_list)\n",
    "    \n",
    "    with open(foldername+'/misc_list.txt', 'w') as file:\n",
    "        file.writelines(\"%s\\n\" % line for line in misc_list)\n",
    "        \n",
    "    print('sleeping...')\n",
    "    time.sleep(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
