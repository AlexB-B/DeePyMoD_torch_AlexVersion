{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook allowing creating of VE synthetic data and conversion from coeffs to model params only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import VE_datagen\n",
    "import VE_params\n",
    "\n",
    "np_seed = 2\n",
    "torch_seed = 0\n",
    "np.random.seed(np_seed)\n",
    "torch.manual_seed(torch_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The below cell is very important for preparing the generation, examination, and saving of the data. It is one of only a few cells that requires configuration in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_type = 'Strain'\n",
    "\n",
    "# For Boltzmann DG, specific model required for calculation of response given manipulation type. Strain -> GMM, Stress -> GKM.\n",
    "# For odeint method, choose what you want.\n",
    "mech_model = 'GMM' \n",
    "    \n",
    "E = [5e-4, 5e-4, 5e-4] # All 2 kOhm resistors\n",
    "eta = [220e-6, 33e-6] # 220 uF and 33 uF capacitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to the equivalent description of model specified by 'model' which is what flow is forced to presume given input_type\n",
    "# # The third arg should be the original format of the model described above, the opposite to what will be assumed by input_type\n",
    "# E, eta = VE_params.convert_between_models(E_GMM, eta_GMM, 'GMM')\n",
    "# print(E, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_alt = np.array(E[1:])\n",
    "eta_alt = np.array(eta)\n",
    "tau = eta_alt/E_alt\n",
    "tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the functional form of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_desc = 'Half Sinc'\n",
    "\n",
    "# OLD MANIPULATION\n",
    "# sinc\n",
    "omega = 1\n",
    "Amp = 7\n",
    "input_expr = lambda t: Amp*np.sin(omega*t)/(omega*t) # For DG\n",
    "d_input_expr = lambda t: (Amp/t)*(np.cos(omega*t) - np.sin(omega*t)/(omega*t)) # For DG\n",
    "input_torch_lambda = lambda t: Amp*torch.sin(omega*t)/(omega*t) # For library\n",
    "\n",
    "# YOUR MANIPULATION\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the independant data points over which to synthesise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_array = np.linspace(10**-10, 10*np.pi/omega, 5000).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_array, stress_array = VE_datagen.calculate_strain_stress(input_type, time_array, input_expr, E, eta, D_input_lambda=d_input_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strain_array, stress_array = VE_datagen.calculate_int_diff_equation_initial(time_array, input_expr, E, eta, input_type, mech_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_array.flatten(), strain_array.flatten(), label='strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time_array.flatten(), stress_array.flatten(), label='stress')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small section uses numerical derivs to check that the data generated follows the expected diff equation. First, the true expected coeffs are calaculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_coeffs = VE_params.coeffs_from_model_params(E, eta, mech_model)\n",
    "unscaled_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the GDM residual is calculated at each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = VE_datagen.equation_residuals(time_array, strain_array, stress_array, unscaled_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(abs(errors.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presence of t/time_sf in new lambdas is hard to understand but has a reason. The reason is that while unscaled t, stress and strain all map to each other, they need to all map once scaled also. Scaling the time array does not change the target array as these are both precalculated. however, because the analytical input data is calculated based off this NEW SCALED time series in the library function, it is calculated over the scaled time series for the old function, effectively doubling the number of bumps in the curve, rather than stretching it out. we want to calculate the old input_data, ie that originates from the unscaled time data, so we have to unscale the time data on the fly in the library, hence the factor in the lambda function.\n",
    "\n",
    "This allows PyTorch to map scaled time to scaled input and calculate the appropriate derivatives\n",
    "\n",
    "Note, this is not an issue for the real data as there is no analytical input term, and the input variable is a dumb target, just like the output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'normalising'\n",
    "time_sf = omega/1.2\n",
    "strain_sf = 1/np.max(abs(strain_array))\n",
    "stress_sf = 1/np.max(abs(stress_array))\n",
    "print(time_sf, strain_sf, stress_sf)\n",
    "\n",
    "scaled_time_array = time_array*time_sf\n",
    "scaled_strain_array = strain_array*strain_sf\n",
    "scaled_stress_array = stress_array*stress_sf\n",
    "if input_type == 'Strain':\n",
    "    scaled_input_expr = lambda t: strain_sf*input_expr(t/time_sf)\n",
    "    scaled_input_torch_lambda = lambda t: strain_sf*input_torch_lambda(t/time_sf)\n",
    "    scaled_target_array = scaled_stress_array\n",
    "elif input_type == 'Stress':\n",
    "    scaled_input_expr = lambda t: stress_sf*input_expr(t/time_sf)\n",
    "    scaled_input_torch_lambda = lambda t: stress_sf*input_torch_lambda(t/time_sf)\n",
    "    scaled_target_array = scaled_strain_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the true coeffs to what deepmod should find based on the scaling of each term in the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_coeffs = VE_params.scaled_coeffs_from_true(unscaled_coeffs, time_sf, strain_sf, stress_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = VE_datagen.equation_residuals(scaled_time_array, scaled_strain_array, scaled_stress_array, expected_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(abs(errors.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add noise\n",
    "noise_level = 0 # No noise ever added\n",
    "\n",
    "noisy_target_array = scaled_target_array + noise_level * np.std(scaled_target_array) * np.random.standard_normal(scaled_target_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "number_of_samples = 1000\n",
    "\n",
    "reordered_row_indices = np.random.permutation(scaled_time_array.size)\n",
    "\n",
    "reduced_time_array = scaled_time_array[reordered_row_indices, :][:number_of_samples]\n",
    "reduced_target_array = noisy_target_array[reordered_row_indices, :][:number_of_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as auto\n",
    "    \n",
    "def mech_library(inputs, **library_config):    \n",
    "    \n",
    "    prediction, data = inputs\n",
    "    \n",
    "    # Load already calculated derivatives of manipulation variable\n",
    "    input_theta = library_config['input_theta']\n",
    "    if data.shape[0] == 1: # Swaps real input_theta out for dummy in initialisation pass.\n",
    "        input_theta = torch.ones((1, input_theta.shape[1]))\n",
    "    \n",
    "    # Automatic derivatives of response variable \n",
    "    output_derivs = auto_deriv(data, prediction, library_config['diff_order'])\n",
    "    output_theta = torch.cat((prediction, output_derivs), dim=1)\n",
    "    \n",
    "    # Identify the manipulation/response as Stress/Strain and organise into returned variables\n",
    "    if library_config['input_type'] == 'Strain':\n",
    "        strain = input_theta\n",
    "        stress = output_theta\n",
    "    else: # 'Stress'\n",
    "        strain = output_theta\n",
    "        stress = input_theta\n",
    "        \n",
    "    strain_t = strain[:, 1:2] # Extract the first time derivative of strain\n",
    "    strain = torch.cat((strain[:, 0:1], strain[:, 2:]), dim=1) # remove this before it gets put into theta\n",
    "    strain *= -1 # The coefficient of all strain terms will always be negative. rather than hoping deepmod will find these negative terms, we assume the negative factor here and later on DeepMoD will just find positive coefficients\n",
    "    theta = torch.cat((strain, stress), dim=1) # I have arbitrarily set the convention of making Strain the first columns of data\n",
    "    \n",
    "    return [strain_t], theta\n",
    "\n",
    "\n",
    "def auto_deriv(data, prediction, max_order):\n",
    "    '''\n",
    "    data and prediction must be single columned tensors.\n",
    "    If it is desired to calculate the derivatives of different predictions wrt different data, this function must be called multiple times.\n",
    "    This function does not return a column with the zeroth derivative (the prediction).\n",
    "    '''\n",
    "    \n",
    "    # First derivative builds off prediction.\n",
    "    derivs = auto.grad(prediction, data, grad_outputs=torch.ones_like(prediction), create_graph=True)[0]\n",
    "    for _ in range(max_order-1):\n",
    "        # Higher derivatives chain derivatives from first derivative.\n",
    "        derivs = torch.cat((derivs, auto.grad(derivs[:, -1:], data, grad_outputs=torch.ones_like(prediction), create_graph=True)[0]), dim=1)\n",
    "            \n",
    "    return derivs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepMod prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_tensor = torch.tensor(reduced_time_array, dtype=torch.float32, requires_grad=True)\n",
    "target_tensor = torch.tensor(reduced_target_array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulation derivative library pre-calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_diff_order = 3\n",
    "\n",
    "input_data = scaled_input_torch_lambda(time_tensor)\n",
    "input_derivs = auto_deriv(time_tensor, input_data, library_diff_order)\n",
    "input_theta = torch.cat((input_data.detach(), input_derivs.detach()), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library_config = {'library_func': mech_library,\n",
    "                  'diff_order': library_diff_order,\n",
    "                  'coeff_sign': 'positive',\n",
    "                  'input_type': input_type,\n",
    "                  'input_theta': input_theta}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW INSERT DEEPMOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rediscovering mechanical model parameters if possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take the coefficients that DeepMoD has found and reverse the process in predicting coeffients.\n",
    "\n",
    "First we do the reverse scaling of the coeffs, this time dividing by the multiplication factor previously found, to scale the scaled coefficients to the true ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below line explanation:\n",
    "\n",
    ">final_coeffs_array needs to be the result you get.\n",
    ">\n",
    ">mask and library_diff_order are optional arguements and can be omitted if the set of coeffs you get back are of a viable pattern. Otherwise, to make suure correct scaling applied to correct coeff, need the mask. If in doubt, include the mask.\n",
    ">\n",
    ">For library_diff_order = 3, a viable second order result will have mask [0,1,3,4,5], and a 1st order result would be [0,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unscaled_final_coeffs = VE_params.true_coeffs_from_scaled(final_coeffs_array, time_sf, strain_sf, stress_sf, mask=sparsity_mask_array, library_diff_order=library_diff_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_coeffs = list(unscaled_final_coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next use these coefficients to recover our model parameters.\n",
    "\n",
    "Note, if sparsity mask describes an inviable model, this may still run, but results are nonsense as assumption is made about which values correspond to which coeffs, which is not followed for an invalid mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_mech_params = VE_params.model_params_from_coeffs(true_coeffs, mech_model, True)\n",
    "recovered_mech_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
