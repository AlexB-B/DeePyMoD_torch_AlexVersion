{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viscolelastic Materials Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we Choose the model and other parameters for our data, and save this data for reference later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "import data.Generation.VE_DataGen_Functions as vedg\n",
    "from deepymod_torch.library_function import mech_library\n",
    "from deepymod_torch.DeepMod import DeepMoD\n",
    "from deepymod_torch.neural_net import deepmod_init, train\n",
    "from deepymod_torch.sparsity import scaling, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Type = 'Strain'\n",
    "E = [1,1,1]\n",
    "Eta = [1,10]\n",
    "t = sp.symbols('t', real=True)\n",
    "Input_Function = sp.exp(-t) + sp.exp(-t/10)\n",
    "Input_Description = 'Two_e_Decays_at_Taus'\n",
    "Int_Type = 'Analytical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t*(-exp(21*t/10) - 10*exp(6*t/5))*exp(-11*t/5)/10 + (171*exp(t/10) + 72*exp(t))*exp(-11*t/10)/81 - (162*exp(11*t/10) - 9*exp(t/10) + 90*exp(t))*exp(-11*t/10)/81 + 2.0 + 2.0*exp(-t) + 2.0*exp(-t/10)\n"
     ]
    }
   ],
   "source": [
    "Tuple_of_Expressions = vedg.Stress_Strain_Master_Int(Input_Type, E, Eta, Input_Function, Int_Type, t)\n",
    "Tuple_of_Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = np.linspace(0.001, 50, 5000)\n",
    "\n",
    "title = ('Input: ' + Input_Description + ', ' + Input_Type + \n",
    "         '.\\nModel: E Mods ' + str(E) + ', Viscs ' + str(Eta))\n",
    "\n",
    "Strain_Array, Stress_Array = vedg.Eval_Graph_Strain_Stress(title, time, Input_Function, Tuple_of_Expressions, Input_Type, Int_Type, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vedg.save_stress_strain(time, Strain_Array, Stress_Array, '../data/StressStrain', Input_Type, Input_Description, E, Eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mostly use DeepMoD using default settings as we would for any other problems. the configuratiosn specific to this application come from knowledge we already have, namely:\n",
    "\n",
    "- We are going to use a customised $\\Theta$ that only ever computes the derivatives of stress and strain, never the squares, as we are looking solve a system where these are the only terms present. We are looking at a hybrid problem here where we aren't completely blindly exploring the system. We assume the data is stress/strain data and we assume our models for these materials are accurate so as to produce an accurate format for the equations describing their dynamic behaviour.\n",
    "\n",
    "- For this, we need to use a library function of a new design.\n",
    "\n",
    "- In this library function, we will ask PyTorch to compute derivatives for the output variable up to a couple of orders of differentiation beyond expectations, just to demonstrate teh DeepmoD both, finds the correct coefficients, but also removes terms that should not be present.\n",
    "\n",
    "- the library function will also contain the assumption that all coefficients on the input function are negative. this way, teh coefficients that DeepMoD shoudl find will all be positive, as all viscosties and elastic moduli are postive, so all sums and products of them are also, and this si what makes up the coefficient.\n",
    "\n",
    "- This will allow us to further tinker and ask the random initial values of teh coefficients to all be positive, getting round an issue with DeepMod getting stuck in local minima\n",
    "\n",
    "- The derivatives of teh input function can be calculated without the neural network, as teh functional form of this is know. These results are still built into the loss though, as part of the equation-regression term.\n",
    "\n",
    "- Additionalyl, the lambda paramter will be made small. in essence, we are not looking for a very sparse vector, so L1 regullarisation actually hinders the convergence process.\n",
    "\n",
    "- Additionally, the LHS of the equation is fixed to the time derivative of the input with coeff 1\n",
    "\n",
    "> Is it the time derivative of the input, or always strain!!!!!!\n",
    "\n",
    "Once DeepMoD has its coefficnets, the job is not done as the coefficients need to be traced back into E_Mods and Viscs. i am not yet sure how to do this but\n",
    "\n",
    "- there will be two stages. the first is developing teh expressions for coeff 1 is this combination, coeff 2 is that one etc, and then teh second stage is using these simultaneous expressions to instead right each paramter in terms of teh coeffs.\n",
    "\n",
    "- The first part is not trivial as the parameters that go into forming the coeffs will be different depending on how many parameters are genuinely present in the model. The number of derivatives that deepmod finds can be used to determine which set of equations are going to be used.\n",
    "\n",
    "- Either these equations can be hard coded or\n",
    "\n",
    "- A pattern can be identified so that once the number of parameters is known, a loop can be run to generate the expressions (However, this method is only weakly scalable as, although identifying a pattern would allow easy identification of the full description of the coeffs for low order derivatives, the patterns would need to be worked out by hand for even higher derivatives leaving a) a lot of work by hand to allow the analysis to handle, let's say, 12th order derivatives and b) there will always be a ceiling determined arbitrarily by how high I have worked out the expressions for.) or\n",
    "\n",
    "- The root equation with sum could be looped and written in sympy, and the rearranged and sympy could be asked to find the expressions for coeffs of each term. the function `Differential` rather than `.diff` will be useful here to maintain the unsolved derivatives. Side note for coding: probably symbols can be placed into lists so that they do not have to be given unique names which could be tricky in a loop. -> ie `List_Syms[3] = sym.symbols('Tau'+Loop_Number, real=True)` etc.\n",
    "\n",
    "- The coeff on the LHS of the equation being fixed to 1 fixes all the other coeffs, so there is no ambiguity here.\n",
    "\n",
    "For the second part, using the coeffs to calculate each model parameter\n",
    "\n",
    "- I am hoping i can just use some function called solve or something in sympy to get the expressions for each aparamter in term so the coeffs, once the part 1 above is done. Indeed, this is possible on mathematica as demonstrated by the screenshot Remy sent by email.\n",
    "\n",
    "- Alternatively, they could be hard coded in if worked them out by hand......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra idea: The order of derivative should be teh same in both stress and strain. This is not a wild assumption. Perhaps teh loss can be further modified so as to penilise a situation where higher derivatives are being allowed in the 1st half of coeffs vs the other.\n",
    "\n",
    "Extra Idea: If the third derivative has been identified as being zero, all higher derivatives must also be zero. Remy suggested not harshly setting all to zero immediately, but removing the highest derivative and retraining, and then repeating, to see if lower derivatives are recovered.\n",
    "\n",
    "There is another consideration. There are two ways we can get the optimisation procedure to discover the right equation. The 'One Big Step' approach means modifying the loss function to be more and more aggressive. The other is a more iterative approach where the result can be critiqued, something changed, and the optimisation run again, such as the removal of the highest derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra note: need to think about the GKM vs GMM stuff. Essentially, teh creep and relaxation expressions used to generate the data have come from a GKM, not a GMM!. Presumably the results may differ slightly with a GMM, but at the same time, this leads to a direct final result, and the result should be the same if the two models are genuinely equivalent???\n",
    "\n",
    "In any case, the coefficents worked out will be different depending on whether we analyse in terms of a GKM or GMM. At leats at first glnce, would be an interesting test. certainly, the set of parameters making up each deduced coeff would be different, but perhaps, once the simultaneous equations are solved the resulting paramters are the same. ie, the coeff for the second deriv of stress will be described by different parameters sepending on the model, but a) the value for this coeff is model independant and b) the paremeters are model independant, its just that a different set of these parameters can be combined in a different way to get the same coeff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually getting on with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need to reshape the data into a conveneint form. `vedg.save_stress_strain` does this also, so if importing the data this next step is unnecessary.\n",
    "\n",
    "For the purpose of running deepmod, we do not need the calculated dat for whatever the input is, as we will instead provide teh analytical expression. Understanding this is important as it explains why for around half of the terms in theta, we don't need to use the result of the NN to obtain the values, we can easily obtain them directly from the anayltical expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_Array = time.reshape(-1, 1)\n",
    "Stress_Array = Stress_Array.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/StressStrain/Strain Three_e_Decays E[1, 1, 1] V[1.25, 2.5].csv', delimiter=',')\n",
    "time_Array = data[:,[0]] # submitting the column index as a list preserves the idea that time_Array is a column,\n",
    "                            #otherwise it would default to a 1D array.\n",
    "Stress_Array = data[:,[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This is now the point where we might add **noise** and perform **random sampling** of the data. For now, in this first version of the notebook, we will skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to fully prepare the data for injection into PyTorch, we need to convert the arrays into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_Tensor = torch.tensor(time_Array, dtype=torch.float32, requires_grad=True)\n",
    "Stress_Tensor = torch.tensor(Stress_Array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by setting up DeepMoD as normal, as per Burgers example and then I will interogate for changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`optim_config` is contains only a minor change. I have reduced the lambda variable to deemphasise the need for a sparse coeff_vector. I may do further tweaks later.\n",
    "\n",
    "`network config` contains the important change that there is only 1 input dimension in our problem, ie, only $t$. I have also reduced the size of the NN rather arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-6, 'max_iterations': 20000}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 15, 'layers': 3, 'output_dim': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changes to `lib_config` is the business end. Poly order is a redundant variable as the single 'polynomial' term which would be stress or strain to the power of one, can be accounted for as the zeroth order derivative. There is no constant. An entirely new library 'type' function is created for our problem.\n",
    "\n",
    "Additionally, three new configuration paramters for our library dictionary are created. The first is the additional entry that will enforce the idea that all the initial guesses at the value of the coeffs (ksi vector) must be positive. The latter two allow the library to put the coefficients in the same order regardless of whether strain or stress was described by an analytical expression of our design, the analytical expression also being given as the final item in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.library_function import mech_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': Input_Type, 'input_expr': Input_Function}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these configurations, in theory we should be able to run deepmod to interrogate the values of the coefficients of the derivatives in the general expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except, to break it down a little for this first run, let's actually pull out the code from DeepMoD and run bits step by step to better check for errors.\n",
    "\n",
    "First, we initialise the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.neural_net import deepmod_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network, coeff_vector_list, sparsity_mask_list = deepmod_init(network_config, lib_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=1, out_features=15, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=15, out_features=15, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Linear(in_features=15, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to some changes made in the neural_net.py file, we notice the following has managed to take on board the need for these starting values to be positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.4994],\n",
       "         [1.5211],\n",
       "         [1.5500],\n",
       "         [0.6857],\n",
       "         [0.1835],\n",
       "         [0.3029],\n",
       "         [1.6142]], requires_grad=True)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 2, 3, 4, 5, 6])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our network and starting coeffs, let's try and train the network. This is the part one trainign which still includes the L1 term and contains all terms. As I specified a diff order of 5 earlier, this may be a little long to process...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.neural_net import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 1.4E+00 1.3E+00 6.7E-02 5.8E-06\n",
      "tensor([[0.4974],\n",
      "        [1.5191],\n",
      "        [1.5520],\n",
      "        [0.6877],\n",
      "        [0.1855],\n",
      "        [0.3009],\n",
      "        [1.6162]], requires_grad=True)\n",
      "500 1.1E-02 1.0E-02 1.2E-03 1.6E-05\n",
      "tensor([[0.2972],\n",
      "        [1.9223],\n",
      "        [1.1311],\n",
      "        [0.3688],\n",
      "        [1.1013],\n",
      "        [1.0335],\n",
      "        [0.4551]], requires_grad=True)\n",
      "1000 1.1E-03 8.8E-04 1.5E-04 2.0E-05\n",
      "tensor([[0.2577],\n",
      "        [1.9467],\n",
      "        [1.1045],\n",
      "        [0.3646],\n",
      "        [1.1979],\n",
      "        [0.9729],\n",
      "        [0.2536]], requires_grad=True)\n",
      "1500 3.9E-04 3.0E-04 7.1E-05 2.0E-05\n",
      "tensor([[0.2563],\n",
      "        [1.9489],\n",
      "        [1.1013],\n",
      "        [0.3610],\n",
      "        [1.1937],\n",
      "        [0.9511],\n",
      "        [0.1865]], requires_grad=True)\n",
      "2000 3.4E-03 3.0E-03 3.7E-04 2.0E-05\n",
      "tensor([[0.2560],\n",
      "        [1.9513],\n",
      "        [1.0979],\n",
      "        [0.3561],\n",
      "        [1.1835],\n",
      "        [0.9344],\n",
      "        [0.1446]], requires_grad=True)\n",
      "2500 1.3E-04 8.4E-05 3.1E-05 2.0E-05\n",
      "tensor([[0.2535],\n",
      "        [1.9533],\n",
      "        [1.0946],\n",
      "        [0.3520],\n",
      "        [1.1721],\n",
      "        [0.9158],\n",
      "        [0.1232]], requires_grad=True)\n",
      "3000 2.1E-03 1.8E-03 2.8E-04 1.9E-05\n",
      "tensor([[0.2509],\n",
      "        [1.9543],\n",
      "        [1.0921],\n",
      "        [0.3439],\n",
      "        [1.1577],\n",
      "        [0.9031],\n",
      "        [0.1067]], requires_grad=True)\n",
      "3500 8.7E-05 4.3E-05 2.6E-05 1.9E-05\n",
      "tensor([[0.2442],\n",
      "        [1.9543],\n",
      "        [1.0902],\n",
      "        [0.3361],\n",
      "        [1.1403],\n",
      "        [0.8885],\n",
      "        [0.0986]], requires_grad=True)\n",
      "4000 7.2E-05 3.1E-05 2.2E-05 1.9E-05\n",
      "tensor([[0.2415],\n",
      "        [1.9543],\n",
      "        [1.0880],\n",
      "        [0.3310],\n",
      "        [1.1244],\n",
      "        [0.8722],\n",
      "        [0.0895]], requires_grad=True)\n",
      "4500 7.1E-05 2.9E-05 2.3E-05 1.8E-05\n",
      "tensor([[0.2324],\n",
      "        [1.9522],\n",
      "        [1.0871],\n",
      "        [0.3170],\n",
      "        [1.1019],\n",
      "        [0.8622],\n",
      "        [0.0855]], requires_grad=True)\n",
      "5000 5.5E-05 2.1E-05 1.6E-05 1.8E-05\n",
      "tensor([[0.2295],\n",
      "        [1.9516],\n",
      "        [1.0845],\n",
      "        [0.3116],\n",
      "        [1.0844],\n",
      "        [0.8457],\n",
      "        [0.0797]], requires_grad=True)\n",
      "5500 8.7E-05 4.6E-05 2.3E-05 1.7E-05\n",
      "tensor([[0.2196],\n",
      "        [1.9486],\n",
      "        [1.0831],\n",
      "        [0.2966],\n",
      "        [1.0606],\n",
      "        [0.8365],\n",
      "        [0.0774]], requires_grad=True)\n",
      "6000 4.5E-05 1.6E-05 1.2E-05 1.7E-05\n",
      "tensor([[0.2155],\n",
      "        [1.9475],\n",
      "        [1.0793],\n",
      "        [0.2897],\n",
      "        [1.0411],\n",
      "        [0.8211],\n",
      "        [0.0741]], requires_grad=True)\n",
      "6500 3.9E-05 1.2E-05 9.6E-06 1.7E-05\n",
      "tensor([[0.2134],\n",
      "        [1.9481],\n",
      "        [1.0730],\n",
      "        [0.2855],\n",
      "        [1.0236],\n",
      "        [0.8042],\n",
      "        [0.0690]], requires_grad=True)\n",
      "7000 3.9E-05 1.4E-05 9.0E-06 1.7E-05\n",
      "tensor([[0.2059],\n",
      "        [1.9457],\n",
      "        [1.0686],\n",
      "        [0.2738],\n",
      "        [1.0003],\n",
      "        [0.7919],\n",
      "        [0.0675]], requires_grad=True)\n",
      "7500 3.3E-05 9.1E-06 7.9E-06 1.6E-05\n",
      "tensor([[0.2048],\n",
      "        [1.9465],\n",
      "        [1.0606],\n",
      "        [0.2708],\n",
      "        [0.9818],\n",
      "        [0.7736],\n",
      "        [0.0633]], requires_grad=True)\n",
      "8000 3.4E-05 9.0E-06 8.8E-06 1.6E-05\n",
      "tensor([[0.1986],\n",
      "        [1.9438],\n",
      "        [1.0552],\n",
      "        [0.2608],\n",
      "        [0.9580],\n",
      "        [0.7588],\n",
      "        [0.0621]], requires_grad=True)\n",
      "8500 2.9E-05 7.0E-06 6.9E-06 1.6E-05\n",
      "tensor([[0.1986],\n",
      "        [1.9443],\n",
      "        [1.0462],\n",
      "        [0.2592],\n",
      "        [0.9387],\n",
      "        [0.7383],\n",
      "        [0.0580]], requires_grad=True)\n",
      "9000 3.0E-05 7.1E-06 7.6E-06 1.5E-05\n",
      "tensor([[0.1931],\n",
      "        [1.9404],\n",
      "        [1.0411],\n",
      "        [0.2499],\n",
      "        [0.9137],\n",
      "        [0.7216],\n",
      "        [0.0572]], requires_grad=True)\n",
      "9500 2.7E-05 5.6E-06 6.0E-06 1.5E-05\n",
      "tensor([[0.1937],\n",
      "        [1.9400],\n",
      "        [1.0322],\n",
      "        [0.2491],\n",
      "        [0.8938],\n",
      "        [0.6993],\n",
      "        [0.0532]], requires_grad=True)\n",
      "10000 2.6E-05 5.6E-06 6.0E-06 1.5E-05\n",
      "tensor([[0.1898],\n",
      "        [1.9356],\n",
      "        [1.0271],\n",
      "        [0.2421],\n",
      "        [0.8694],\n",
      "        [0.6802],\n",
      "        [0.0519]], requires_grad=True)\n",
      "10500 2.8E-04 2.4E-04 3.1E-05 1.4E-05\n",
      "tensor([[0.1866],\n",
      "        [1.9311],\n",
      "        [1.0216],\n",
      "        [0.2358],\n",
      "        [0.8456],\n",
      "        [0.6601],\n",
      "        [0.0498]], requires_grad=True)\n",
      "11000 2.3E-05 4.4E-06 4.8E-06 1.4E-05\n",
      "tensor([[0.1864],\n",
      "        [1.9273],\n",
      "        [1.0156],\n",
      "        [0.2339],\n",
      "        [0.8240],\n",
      "        [0.6373],\n",
      "        [0.0471]], requires_grad=True)\n",
      "11500 2.4E-05 5.5E-06 4.5E-06 1.4E-05\n",
      "tensor([[0.1822],\n",
      "        [1.9203],\n",
      "        [1.0125],\n",
      "        [0.2265],\n",
      "        [0.7993],\n",
      "        [0.6177],\n",
      "        [0.0459]], requires_grad=True)\n",
      "12000 2.1E-05 3.5E-06 3.7E-06 1.3E-05\n",
      "tensor([[0.1826],\n",
      "        [1.9159],\n",
      "        [1.0066],\n",
      "        [0.2253],\n",
      "        [0.7788],\n",
      "        [0.5946],\n",
      "        [0.0423]], requires_grad=True)\n",
      "12500 2.0E-05 3.5E-06 3.5E-06 1.3E-05\n",
      "tensor([[0.1786],\n",
      "        [1.9075],\n",
      "        [1.0046],\n",
      "        [0.2180],\n",
      "        [0.7546],\n",
      "        [0.5751],\n",
      "        [0.0410]], requires_grad=True)\n",
      "13000 2.3E-05 6.7E-06 4.0E-06 1.3E-05\n",
      "tensor([[0.1742],\n",
      "        [1.8981],\n",
      "        [1.0034],\n",
      "        [0.2101],\n",
      "        [0.7305],\n",
      "        [0.5565],\n",
      "        [0.0397]], requires_grad=True)\n",
      "13500 1.8E-05 3.1E-06 2.7E-06 1.2E-05\n",
      "tensor([[0.1728],\n",
      "        [1.8906],\n",
      "        [1.0003],\n",
      "        [0.2068],\n",
      "        [0.7094],\n",
      "        [0.5346],\n",
      "        [0.0371]], requires_grad=True)\n",
      "14000 1.8E-05 3.4E-06 2.7E-06 1.2E-05\n",
      "tensor([[0.1680],\n",
      "        [1.8800],\n",
      "        [1.0001],\n",
      "        [0.1985],\n",
      "        [0.6858],\n",
      "        [0.5164],\n",
      "        [0.0360]], requires_grad=True)\n",
      "14500 1.6E-05 2.5E-06 2.0E-06 1.2E-05\n",
      "tensor([[0.1673],\n",
      "        [1.8719],\n",
      "        [0.9973],\n",
      "        [0.1960],\n",
      "        [0.6662],\n",
      "        [0.4954],\n",
      "        [0.0331]], requires_grad=True)\n",
      "15000 1.6E-05 2.7E-06 2.0E-06 1.1E-05\n",
      "tensor([[0.1616],\n",
      "        [1.8598],\n",
      "        [0.9983],\n",
      "        [0.1865],\n",
      "        [0.6426],\n",
      "        [0.4783],\n",
      "        [0.0322]], requires_grad=True)\n",
      "15500 1.5E-05 2.3E-06 1.6E-06 1.1E-05\n",
      "tensor([[0.1602],\n",
      "        [1.8505],\n",
      "        [0.9965],\n",
      "        [0.1831],\n",
      "        [0.6232],\n",
      "        [0.4586],\n",
      "        [0.0297]], requires_grad=True)\n",
      "16000 1.5E-05 2.3E-06 1.4E-06 1.1E-05\n",
      "tensor([[0.1553],\n",
      "        [1.8381],\n",
      "        [0.9974],\n",
      "        [0.1747],\n",
      "        [0.6008],\n",
      "        [0.4414],\n",
      "        [0.0285]], requires_grad=True)\n",
      "16500 3.7E-05 2.5E-05 1.7E-06 1.0E-05\n",
      "tensor([[0.1535],\n",
      "        [1.8275],\n",
      "        [0.9963],\n",
      "        [0.1708],\n",
      "        [0.5817],\n",
      "        [0.4224],\n",
      "        [0.0263]], requires_grad=True)\n",
      "17000 1.3E-05 1.9E-06 1.1E-06 1.0E-05\n",
      "tensor([[0.1486],\n",
      "        [1.8143],\n",
      "        [0.9977],\n",
      "        [0.1625],\n",
      "        [0.5596],\n",
      "        [0.4052],\n",
      "        [0.0251]], requires_grad=True)\n",
      "17500 5.9E-05 4.7E-05 2.0E-06 9.9E-06\n",
      "tensor([[0.1458],\n",
      "        [1.8026],\n",
      "        [0.9975],\n",
      "        [0.1573],\n",
      "        [0.5404],\n",
      "        [0.3877],\n",
      "        [0.0233]], requires_grad=True)\n",
      "18000 1.2E-05 1.8E-06 8.6E-07 9.5E-06\n",
      "tensor([[0.1402],\n",
      "        [1.7887],\n",
      "        [0.9993],\n",
      "        [0.1480],\n",
      "        [0.5182],\n",
      "        [0.3716],\n",
      "        [0.0224]], requires_grad=True)\n",
      "18500 1.6E-04 1.5E-04 3.6E-06 9.2E-06\n",
      "tensor([[0.1360],\n",
      "        [1.7750],\n",
      "        [1.0004],\n",
      "        [0.1414],\n",
      "        [0.4978],\n",
      "        [0.3556],\n",
      "        [0.0211]], requires_grad=True)\n",
      "19000 1.1E-05 1.5E-06 6.7E-07 8.9E-06\n",
      "tensor([[0.1335],\n",
      "        [1.7622],\n",
      "        [1.0004],\n",
      "        [0.1359],\n",
      "        [0.4786],\n",
      "        [0.3368],\n",
      "        [0.0192]], requires_grad=True)\n",
      "19500 1.1E-05 1.5E-06 6.3E-07 8.6E-06\n",
      "tensor([[0.1288],\n",
      "        [1.7468],\n",
      "        [1.0027],\n",
      "        [0.1280],\n",
      "        [0.4571],\n",
      "        [0.3195],\n",
      "        [0.0181]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "time_deriv_list, theta, coeff_vector_list = train(time_Tensor, Stress_Tensor, network, coeff_vector_list, \n",
    "                                                  sparsity_mask_list, lib_config, optim_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in the deepmod procedure is to apply scaling to the coefficients before thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.sparsity import scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_coeff_vector_list = [scaling(coeff_vector, theta, time_deriv) for coeff_vector, time_deriv in zip(coeff_vector_list, time_deriv_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.7359],\n",
       "         [1.3081],\n",
       "         [0.7443],\n",
       "         [1.0574],\n",
       "         [2.3255],\n",
       "         [1.3626],\n",
       "         [0.7127]], grad_fn=<MulBackward0>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_coeff_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then thresholding, which unlike scaling has been modified to take additional arguements so that some common sense rejection on deepmods results can be implemented based on\n",
    "\n",
    "-  if between pairs of like order derivatives of stress and strain, one of the pair is thresholded to zero but the other not\n",
    "\n",
    "- if derivatives of certain order have been thresholded to zero but not higher order derivatives\n",
    "\n",
    "then\n",
    "\n",
    "- deepmods sparse selection is rejected, highest order derivative pair is removed, and deepmod retrains with all other coeffs still allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepymod_torch.sparsity import threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coeff_vector_list, sparsity_mask_list, Overode_list = zip(*[threshold(scaled_coeff_vector, coeff_vector, sparsity_mask, lib_config) for scaled_coeff_vector, coeff_vector, sparsity_mask in zip(scaled_coeff_vector_list, coeff_vector_list, sparsity_mask_list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1240],\n",
       "         [1.7304],\n",
       "         [1.0057],\n",
       "         [0.1199],\n",
       "         [0.4357],\n",
       "         [0.3018],\n",
       "         [0.0169]], requires_grad=True),)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6]),)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Overode_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens here is that if Overode_List contained a true, DeepMoD would not enter teh final phase and would befgin again. In the trial that i am running today, DeepMoD didn't threshold **any** of the terms, so the thresholding was not rejected (we happen to know this is wrong, but no common sense way to see that the model doesn't make sense). As a result, we enter directly final phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training is run again with the L1 loss term set to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 3.2E-06 2.5E-06 6.7E-07 0.0E+00\n",
      "tensor([[0.1220],\n",
      "        [1.7284],\n",
      "        [1.0077],\n",
      "        [0.1219],\n",
      "        [0.4337],\n",
      "        [0.3038],\n",
      "        [0.0189]], requires_grad=True)\n",
      "500 2.4E-06 1.7E-06 6.7E-07 0.0E+00\n",
      "tensor([[0.1212],\n",
      "        [1.7275],\n",
      "        [1.0083],\n",
      "        [0.1157],\n",
      "        [0.4297],\n",
      "        [0.3001],\n",
      "        [0.0175]], requires_grad=True)\n",
      "1000 2.0E-06 1.4E-06 6.1E-07 0.0E+00\n",
      "tensor([[0.1228],\n",
      "        [1.7292],\n",
      "        [1.0065],\n",
      "        [0.1177],\n",
      "        [0.4285],\n",
      "        [0.2964],\n",
      "        [0.0165]], requires_grad=True)\n",
      "1500 1.7E-06 1.2E-06 5.4E-07 0.0E+00\n",
      "tensor([[0.1241],\n",
      "        [1.7298],\n",
      "        [1.0057],\n",
      "        [0.1193],\n",
      "        [0.4273],\n",
      "        [0.2928],\n",
      "        [0.0156]], requires_grad=True)\n",
      "2000 1.6E-06 1.1E-06 4.8E-07 0.0E+00\n",
      "tensor([[0.1247],\n",
      "        [1.7291],\n",
      "        [1.0065],\n",
      "        [0.1199],\n",
      "        [0.4261],\n",
      "        [0.2900],\n",
      "        [0.0148]], requires_grad=True)\n",
      "2500 1.5E-06 1.0E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1218],\n",
      "        [1.7253],\n",
      "        [1.0102],\n",
      "        [0.1156],\n",
      "        [0.4188],\n",
      "        [0.2861],\n",
      "        [0.0149]], requires_grad=True)\n",
      "3000 5.8E-06 5.2E-06 6.5E-07 0.0E+00\n",
      "tensor([[0.1191],\n",
      "        [1.7210],\n",
      "        [1.0145],\n",
      "        [0.1111],\n",
      "        [0.4099],\n",
      "        [0.2809],\n",
      "        [0.0149]], requires_grad=True)\n",
      "3500 1.6E-06 1.1E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1184],\n",
      "        [1.7186],\n",
      "        [1.0168],\n",
      "        [0.1098],\n",
      "        [0.4026],\n",
      "        [0.2730],\n",
      "        [0.0143]], requires_grad=True)\n",
      "4000 1.4E-06 9.4E-07 4.7E-07 0.0E+00\n",
      "tensor([[0.1183],\n",
      "        [1.7159],\n",
      "        [1.0198],\n",
      "        [0.1092],\n",
      "        [0.3966],\n",
      "        [0.2659],\n",
      "        [0.0135]], requires_grad=True)\n",
      "4500 1.5E-06 1.0E-06 4.9E-07 0.0E+00\n",
      "tensor([[0.1159],\n",
      "        [1.7101],\n",
      "        [1.0260],\n",
      "        [0.1051],\n",
      "        [0.3867],\n",
      "        [0.2580],\n",
      "        [0.0133]], requires_grad=True)\n",
      "5000 1.4E-06 9.3E-07 4.7E-07 0.0E+00\n",
      "tensor([[0.1158],\n",
      "        [1.7062],\n",
      "        [1.0307],\n",
      "        [0.1045],\n",
      "        [0.3807],\n",
      "        [0.2505],\n",
      "        [0.0125]], requires_grad=True)\n",
      "5500 1.4E-06 9.2E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1134],\n",
      "        [1.6994],\n",
      "        [1.0383],\n",
      "        [0.1006],\n",
      "        [0.3709],\n",
      "        [0.2421],\n",
      "        [0.0122]], requires_grad=True)\n",
      "6000 1.3E-06 8.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1125],\n",
      "        [1.6937],\n",
      "        [1.0450],\n",
      "        [0.0988],\n",
      "        [0.3633],\n",
      "        [0.2338],\n",
      "        [0.0116]], requires_grad=True)\n",
      "6500 8.0E-05 7.6E-05 3.8E-06 0.0E+00\n",
      "tensor([[0.1106],\n",
      "        [1.6867],\n",
      "        [1.0532],\n",
      "        [0.0957],\n",
      "        [0.3556],\n",
      "        [0.2274],\n",
      "        [0.0114]], requires_grad=True)\n",
      "7000 1.3E-06 7.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.1103],\n",
      "        [1.6812],\n",
      "        [1.0601],\n",
      "        [0.0947],\n",
      "        [0.3495],\n",
      "        [0.2189],\n",
      "        [0.0105]], requires_grad=True)\n",
      "7500 1.3E-06 7.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1085],\n",
      "        [1.6732],\n",
      "        [1.0696],\n",
      "        [0.0916],\n",
      "        [0.3410],\n",
      "        [0.2105],\n",
      "        [0.0101]], requires_grad=True)\n",
      "8000 1.2E-06 7.5E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1084],\n",
      "        [1.6668],\n",
      "        [1.0778],\n",
      "        [0.0911],\n",
      "        [0.3360],\n",
      "        [0.2030],\n",
      "        [0.0094]], requires_grad=True)\n",
      "8500 1.5E-06 9.8E-07 5.2E-07 0.0E+00\n",
      "tensor([[0.1059],\n",
      "        [1.6578],\n",
      "        [1.0885],\n",
      "        [0.0870],\n",
      "        [0.3266],\n",
      "        [0.1946],\n",
      "        [0.0093]], requires_grad=True)\n",
      "9000 1.1E-06 6.3E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.1059],\n",
      "        [1.6515],\n",
      "        [1.0968],\n",
      "        [0.0867],\n",
      "        [0.3220],\n",
      "        [0.1875],\n",
      "        [0.0086]], requires_grad=True)\n",
      "9500 1.2E-06 6.7E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1042],\n",
      "        [1.6429],\n",
      "        [1.1074],\n",
      "        [0.0838],\n",
      "        [0.3143],\n",
      "        [0.1795],\n",
      "        [0.0082]], requires_grad=True)\n",
      "10000 1.4E-05 1.4E-05 5.8E-07 0.0E+00\n",
      "tensor([[0.1028],\n",
      "        [1.6344],\n",
      "        [1.1182],\n",
      "        [0.0814],\n",
      "        [0.3078],\n",
      "        [0.1725],\n",
      "        [0.0080]], requires_grad=True)\n",
      "10500 1.1E-06 6.1E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1024],\n",
      "        [1.6268],\n",
      "        [1.1281],\n",
      "        [0.0804],\n",
      "        [0.3019],\n",
      "        [0.1640],\n",
      "        [0.0073]], requires_grad=True)\n",
      "11000 3.1E-05 2.4E-05 6.2E-06 0.0E+00\n",
      "tensor([[0.1007],\n",
      "        [1.6183],\n",
      "        [1.1389],\n",
      "        [0.0775],\n",
      "        [0.2957],\n",
      "        [0.1589],\n",
      "        [0.0072]], requires_grad=True)\n",
      "11500 1.1E-06 5.9E-07 5.1E-07 0.0E+00\n",
      "tensor([[0.1002],\n",
      "        [1.6112],\n",
      "        [1.1484],\n",
      "        [0.0765],\n",
      "        [0.2905],\n",
      "        [0.1504],\n",
      "        [0.0066]], requires_grad=True)\n",
      "12000 1.2E-06 7.1E-07 5.3E-07 0.0E+00\n",
      "tensor([[0.0984],\n",
      "        [1.6021],\n",
      "        [1.1597],\n",
      "        [0.0735],\n",
      "        [0.2821],\n",
      "        [0.1417],\n",
      "        [0.0064]], requires_grad=True)\n",
      "12500 9.9E-07 4.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0994],\n",
      "        [1.5969],\n",
      "        [1.1672],\n",
      "        [0.0747],\n",
      "        [0.2815],\n",
      "        [0.1373],\n",
      "        [0.0057]], requires_grad=True)\n",
      "13000 1.2E-05 1.1E-05 9.1E-07 0.0E+00\n",
      "tensor([[0.0984],\n",
      "        [1.5899],\n",
      "        [1.1766],\n",
      "        [0.0732],\n",
      "        [0.2770],\n",
      "        [0.1317],\n",
      "        [0.0055]], requires_grad=True)\n",
      "13500 9.9E-07 4.9E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0974],\n",
      "        [1.5831],\n",
      "        [1.1859],\n",
      "        [0.0713],\n",
      "        [0.2722],\n",
      "        [0.1261],\n",
      "        [0.0052]], requires_grad=True)\n",
      "14000 3.9E-06 3.3E-06 6.1E-07 0.0E+00\n",
      "tensor([[0.0953],\n",
      "        [1.5753],\n",
      "        [1.1963],\n",
      "        [0.0681],\n",
      "        [0.2659],\n",
      "        [0.1202],\n",
      "        [0.0052]], requires_grad=True)\n",
      "14500 1.0E-06 5.0E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0954],\n",
      "        [1.5691],\n",
      "        [1.2051],\n",
      "        [0.0679],\n",
      "        [0.2625],\n",
      "        [0.1143],\n",
      "        [0.0047]], requires_grad=True)\n",
      "15000 1.8E-05 1.7E-05 1.1E-06 0.0E+00\n",
      "tensor([[0.0939],\n",
      "        [1.5620],\n",
      "        [1.2149],\n",
      "        [0.0658],\n",
      "        [0.2581],\n",
      "        [0.1095],\n",
      "        [0.0046]], requires_grad=True)\n",
      "15500 9.2E-07 4.3E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0944],\n",
      "        [1.5556],\n",
      "        [1.2241],\n",
      "        [0.0659],\n",
      "        [0.2548],\n",
      "        [0.1031],\n",
      "        [0.0040]], requires_grad=True)\n",
      "16000 9.5E-07 4.5E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0932],\n",
      "        [1.5490],\n",
      "        [1.2334],\n",
      "        [0.0639],\n",
      "        [0.2502],\n",
      "        [0.0979],\n",
      "        [0.0039]], requires_grad=True)\n",
      "16500 4.0E-06 3.1E-06 8.8E-07 0.0E+00\n",
      "tensor([[0.0918],\n",
      "        [1.5423],\n",
      "        [1.2429],\n",
      "        [0.0619],\n",
      "        [0.2459],\n",
      "        [0.0932],\n",
      "        [0.0038]], requires_grad=True)\n",
      "17000 8.8E-07 3.8E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0923],\n",
      "        [1.5361],\n",
      "        [1.2519],\n",
      "        [0.0622],\n",
      "        [0.2430],\n",
      "        [0.0872],\n",
      "        [0.0033]], requires_grad=True)\n",
      "17500 8.9E-07 4.0E-07 5.0E-07 0.0E+00\n",
      "tensor([[0.0913],\n",
      "        [1.5300],\n",
      "        [1.2608],\n",
      "        [0.0605],\n",
      "        [0.2389],\n",
      "        [0.0825],\n",
      "        [0.0031]], requires_grad=True)\n",
      "18000 8.3E-07 3.4E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0913],\n",
      "        [1.5236],\n",
      "        [1.2699],\n",
      "        [0.0602],\n",
      "        [0.2360],\n",
      "        [0.0772],\n",
      "        [0.0028]], requires_grad=True)\n",
      "18500 8.5E-07 3.6E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0902],\n",
      "        [1.5176],\n",
      "        [1.2786],\n",
      "        [0.0585],\n",
      "        [0.2321],\n",
      "        [0.0727],\n",
      "        [0.0027]], requires_grad=True)\n",
      "19000 3.2E-06 1.8E-06 1.4E-06 0.0E+00\n",
      "tensor([[0.0889],\n",
      "        [1.5114],\n",
      "        [1.2876],\n",
      "        [0.0564],\n",
      "        [0.2282],\n",
      "        [0.0688],\n",
      "        [0.0028]], requires_grad=True)\n",
      "19500 7.9E-07 3.1E-07 4.9E-07 0.0E+00\n",
      "tensor([[0.0894],\n",
      "        [1.5056],\n",
      "        [1.2962],\n",
      "        [0.0569],\n",
      "        [0.2259],\n",
      "        [0.0632],\n",
      "        [0.0022]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "optim_config_internal = optim_config.copy()\n",
    "optim_config_internal['lambda'] = 0\n",
    "\n",
    "time_deriv_list, theta, coeff_vector_list = train(time_Tensor, Stress_Tensor, network, coeff_vector_list, sparsity_mask_list, lib_config, optim_config_internal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this in theory is the whole process done. Only 2 particular lists of tensors are important for us, in this case both lists containing a single tensor, and so simply two tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0884],\n",
       "         [1.4997],\n",
       "         [1.3048],\n",
       "         [0.0553],\n",
       "         [0.2222],\n",
       "         [0.0589],\n",
       "         [0.0021]], requires_grad=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_vector_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 4, 5, 6]),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All in one shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into accounts all of these edits, the DeepMoD algorithm can be run in one shot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will restate the config dictionaries and reimport the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "from deepymod_torch.library_function import mech_library\n",
    "from deepymod_torch.DeepMod import DeepMoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Type = 'Strain'\n",
    "Input_Function = lambda time_data: torch.sin(time_data)/time_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/StressStrain/Strain Sinc E[1, 1, 1] V[1.25, 2.5].csv', delimiter=',')\n",
    "time_Array, Stress_Array = data[:,0:1], data[:,2:]\n",
    "time_Tensor = torch.tensor(time_Array, dtype=torch.float32, requires_grad=True)\n",
    "Stress_Tensor = torch.tensor(Stress_Array, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_config = {'lambda': 10**-5, 'max_iterations': 20000}\n",
    "network_config = {'input_dim': 1, 'hidden_dim': 50, 'layers': 4, 'output_dim': 1}\n",
    "lib_config = {'type': mech_library, 'diff_order': 3, 'coeff_sign': 'positive', 'input_type': Input_Type, 'input_expr': Input_Function}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 2.5E+01 2.9E-01 2.5E+01 4.1E-04\n",
      "tensor([[1.0591],\n",
      "        [0.3921],\n",
      "        [1.3743],\n",
      "        [2.0478],\n",
      "        [0.6237],\n",
      "        [0.7047],\n",
      "        [0.3268]], requires_grad=True)\n",
      "500 8.1E+00 6.8E-02 8.0E+00 1.2E-03\n",
      "tensor([[1.0216],\n",
      "        [0.3833],\n",
      "        [1.2907],\n",
      "        [2.1062],\n",
      "        [0.6194],\n",
      "        [0.8057],\n",
      "        [0.2459]], requires_grad=True)\n",
      "1000 4.4E-01 7.8E-02 3.6E-01 1.1E-03\n",
      "tensor([[1.0192],\n",
      "        [0.3834],\n",
      "        [1.2785],\n",
      "        [2.1292],\n",
      "        [0.5957],\n",
      "        [0.8376],\n",
      "        [0.2470]], requires_grad=True)\n",
      "1500 2.3E-01 7.5E-02 1.6E-01 1.1E-03\n",
      "tensor([[1.0482],\n",
      "        [0.3621],\n",
      "        [1.2750],\n",
      "        [2.1206],\n",
      "        [0.5978],\n",
      "        [0.8469],\n",
      "        [0.2464]], requires_grad=True)\n",
      "2000 3.1E-01 7.4E-02 2.4E-01 1.1E-03\n",
      "tensor([[1.0956],\n",
      "        [0.3232],\n",
      "        [1.2720],\n",
      "        [2.0980],\n",
      "        [0.6067],\n",
      "        [0.8531],\n",
      "        [0.2452]], requires_grad=True)\n",
      "2500 1.9E-01 7.3E-02 1.1E-01 1.1E-03\n",
      "tensor([[1.1450],\n",
      "        [0.2815],\n",
      "        [1.2691],\n",
      "        [2.0684],\n",
      "        [0.6187],\n",
      "        [0.8559],\n",
      "        [0.2446]], requires_grad=True)\n",
      "3000 1.4E-01 7.0E-02 6.6E-02 1.1E-03\n",
      "tensor([[1.1923],\n",
      "        [0.2457],\n",
      "        [1.2660],\n",
      "        [2.0363],\n",
      "        [0.6313],\n",
      "        [0.8575],\n",
      "        [0.2440]], requires_grad=True)\n",
      "3500 1.1E-01 6.6E-02 4.3E-02 1.1E-03\n",
      "tensor([[1.2372],\n",
      "        [0.2174],\n",
      "        [1.2625],\n",
      "        [2.0029],\n",
      "        [0.6438],\n",
      "        [0.8591],\n",
      "        [0.2430]], requires_grad=True)\n",
      "4000 1.0E-01 6.2E-02 4.0E-02 1.1E-03\n",
      "tensor([[1.2793],\n",
      "        [0.1968],\n",
      "        [1.2586],\n",
      "        [1.9686],\n",
      "        [0.6557],\n",
      "        [0.8613],\n",
      "        [0.2417]], requires_grad=True)\n",
      "4500 1.1E-01 5.9E-02 4.7E-02 1.1E-03\n",
      "tensor([[1.3179],\n",
      "        [0.1841],\n",
      "        [1.2541],\n",
      "        [1.9337],\n",
      "        [0.6668],\n",
      "        [0.8644],\n",
      "        [0.2402]], requires_grad=True)\n",
      "5000 2.0E-01 5.5E-02 1.4E-01 1.0E-03\n",
      "tensor([[1.3516],\n",
      "        [0.1807],\n",
      "        [1.2492],\n",
      "        [1.8985],\n",
      "        [0.6771],\n",
      "        [0.8680],\n",
      "        [0.2385]], requires_grad=True)\n",
      "5500 1.2E-01 5.2E-02 6.7E-02 1.1E-03\n",
      "tensor([[1.3790],\n",
      "        [0.1866],\n",
      "        [1.2438],\n",
      "        [1.8634],\n",
      "        [0.6864],\n",
      "        [0.8721],\n",
      "        [0.2368]], requires_grad=True)\n",
      "6000 1.4E-01 5.0E-02 8.6E-02 1.1E-03\n",
      "tensor([[1.3995],\n",
      "        [0.2000],\n",
      "        [1.2380],\n",
      "        [1.8286],\n",
      "        [0.6947],\n",
      "        [0.8764],\n",
      "        [0.2350]], requires_grad=True)\n",
      "6500 3.0E-01 4.7E-02 2.5E-01 1.0E-03\n",
      "tensor([[1.4126],\n",
      "        [0.2175],\n",
      "        [1.2319],\n",
      "        [1.7943],\n",
      "        [0.7020],\n",
      "        [0.8808],\n",
      "        [0.2332]], requires_grad=True)\n",
      "7000 7.9E-02 4.5E-02 3.3E-02 1.0E-03\n",
      "tensor([[1.4185],\n",
      "        [0.2359],\n",
      "        [1.2257],\n",
      "        [1.7605],\n",
      "        [0.7084],\n",
      "        [0.8851],\n",
      "        [0.2314]], requires_grad=True)\n",
      "7500 2.5E-01 4.3E-02 2.1E-01 1.0E-03\n",
      "tensor([[1.4174],\n",
      "        [0.2534],\n",
      "        [1.2194],\n",
      "        [1.7273],\n",
      "        [0.7138],\n",
      "        [0.8893],\n",
      "        [0.2297]], requires_grad=True)\n",
      "8000 2.1E-01 4.1E-02 1.7E-01 1.0E-03\n",
      "tensor([[1.4099],\n",
      "        [0.2682],\n",
      "        [1.2130],\n",
      "        [1.6948],\n",
      "        [0.7183],\n",
      "        [0.8933],\n",
      "        [0.2280]], requires_grad=True)\n",
      "8500 6.4E-02 3.9E-02 2.4E-02 1.0E-03\n",
      "tensor([[1.3966],\n",
      "        [0.2797],\n",
      "        [1.2066],\n",
      "        [1.6629],\n",
      "        [0.7220],\n",
      "        [0.8970],\n",
      "        [0.2264]], requires_grad=True)\n",
      "9000 1.7E-01 3.8E-02 1.3E-01 9.9E-04\n",
      "tensor([[1.3782],\n",
      "        [0.2877],\n",
      "        [1.2001],\n",
      "        [1.6319],\n",
      "        [0.7250],\n",
      "        [0.9006],\n",
      "        [0.2247]], requires_grad=True)\n",
      "9500 7.1E-02 3.6E-02 3.4E-02 1.0E-03\n",
      "tensor([[1.3556],\n",
      "        [0.2921],\n",
      "        [1.1938],\n",
      "        [1.6017],\n",
      "        [0.7274],\n",
      "        [0.9038],\n",
      "        [0.2232]], requires_grad=True)\n",
      "10000 2.0E-01 3.5E-02 1.6E-01 1.0E-03\n",
      "tensor([[1.3294],\n",
      "        [0.2933],\n",
      "        [1.1874],\n",
      "        [1.5723],\n",
      "        [0.7291],\n",
      "        [0.9068],\n",
      "        [0.2216]], requires_grad=True)\n",
      "10500 6.2E-02 3.4E-02 2.7E-02 9.9E-04\n",
      "tensor([[1.3002],\n",
      "        [0.2918],\n",
      "        [1.1810],\n",
      "        [1.5437],\n",
      "        [0.7304],\n",
      "        [0.9097],\n",
      "        [0.2201]], requires_grad=True)\n",
      "11000 6.5E-02 3.3E-02 3.1E-02 9.7E-04\n",
      "tensor([[1.2685],\n",
      "        [0.2880],\n",
      "        [1.1747],\n",
      "        [1.5159],\n",
      "        [0.7312],\n",
      "        [0.9123],\n",
      "        [0.2186]], requires_grad=True)\n",
      "11500 8.9E-02 3.2E-02 5.6E-02 9.6E-04\n",
      "tensor([[1.2351],\n",
      "        [0.2824],\n",
      "        [1.1684],\n",
      "        [1.4889],\n",
      "        [0.7318],\n",
      "        [0.9147],\n",
      "        [0.2172]], requires_grad=True)\n",
      "12000 7.7E-02 3.1E-02 4.5E-02 9.6E-04\n",
      "tensor([[1.2004],\n",
      "        [0.2756],\n",
      "        [1.1622],\n",
      "        [1.4627],\n",
      "        [0.7321],\n",
      "        [0.9169],\n",
      "        [0.2157]], requires_grad=True)\n",
      "12500 5.0E-02 3.0E-02 1.9E-02 9.6E-04\n",
      "tensor([[1.1648],\n",
      "        [0.2679],\n",
      "        [1.1560],\n",
      "        [1.4372],\n",
      "        [0.7324],\n",
      "        [0.9189],\n",
      "        [0.2143]], requires_grad=True)\n",
      "13000 1.6E-01 3.0E-02 1.3E-01 9.8E-04\n",
      "tensor([[1.1286],\n",
      "        [0.2598],\n",
      "        [1.1498],\n",
      "        [1.4124],\n",
      "        [0.7325],\n",
      "        [0.9207],\n",
      "        [0.2128]], requires_grad=True)\n",
      "13500 4.8E-02 2.9E-02 1.8E-02 9.5E-04\n",
      "tensor([[1.0923],\n",
      "        [0.2513],\n",
      "        [1.1436],\n",
      "        [1.3883],\n",
      "        [0.7328],\n",
      "        [0.9223],\n",
      "        [0.2114]], requires_grad=True)\n",
      "14000 8.0E-02 2.8E-02 5.0E-02 9.3E-04\n",
      "tensor([[1.0558],\n",
      "        [0.2429],\n",
      "        [1.1375],\n",
      "        [1.3649],\n",
      "        [0.7330],\n",
      "        [0.9238],\n",
      "        [0.2100]], requires_grad=True)\n",
      "14500 1.7E-01 2.8E-02 1.4E-01 9.6E-04\n",
      "tensor([[1.0195],\n",
      "        [0.2344],\n",
      "        [1.1315],\n",
      "        [1.3421],\n",
      "        [0.7334],\n",
      "        [0.9251],\n",
      "        [0.2085]], requires_grad=True)\n",
      "15000 1.7E-01 2.7E-02 1.4E-01 9.1E-04\n",
      "tensor([[0.9835],\n",
      "        [0.2259],\n",
      "        [1.1254],\n",
      "        [1.3199],\n",
      "        [0.7339],\n",
      "        [0.9262],\n",
      "        [0.2071]], requires_grad=True)\n",
      "15500 1.1E-01 2.7E-02 8.2E-02 9.4E-04\n",
      "tensor([[0.9478],\n",
      "        [0.2177],\n",
      "        [1.1194],\n",
      "        [1.2984],\n",
      "        [0.7346],\n",
      "        [0.9271],\n",
      "        [0.2056]], requires_grad=True)\n",
      "16000 8.7E-02 2.6E-02 6.0E-02 9.0E-04\n",
      "tensor([[0.9126],\n",
      "        [0.2097],\n",
      "        [1.1134],\n",
      "        [1.2774],\n",
      "        [0.7354],\n",
      "        [0.9279],\n",
      "        [0.2042]], requires_grad=True)\n",
      "16500 8.3E-02 2.6E-02 5.6E-02 9.2E-04\n",
      "tensor([[0.8779],\n",
      "        [0.2019],\n",
      "        [1.1074],\n",
      "        [1.2570],\n",
      "        [0.7363],\n",
      "        [0.9285],\n",
      "        [0.2027]], requires_grad=True)\n",
      "17000 1.1E-01 2.5E-02 8.1E-02 8.9E-04\n",
      "tensor([[0.8437],\n",
      "        [0.1944],\n",
      "        [1.1015],\n",
      "        [1.2371],\n",
      "        [0.7373],\n",
      "        [0.9289],\n",
      "        [0.2013]], requires_grad=True)\n",
      "17500 1.2E-01 2.5E-02 9.2E-02 9.2E-04\n",
      "tensor([[0.8102],\n",
      "        [0.1872],\n",
      "        [1.0956],\n",
      "        [1.2176],\n",
      "        [0.7385],\n",
      "        [0.9292],\n",
      "        [0.1999]], requires_grad=True)\n",
      "18000 1.7E-01 2.4E-02 1.5E-01 8.7E-04\n",
      "tensor([[0.7773],\n",
      "        [0.1802],\n",
      "        [1.0897],\n",
      "        [1.1986],\n",
      "        [0.7397],\n",
      "        [0.9293],\n",
      "        [0.1985]], requires_grad=True)\n",
      "18500 1.6E-01 2.4E-02 1.3E-01 9.1E-04\n",
      "tensor([[0.7451],\n",
      "        [0.1737],\n",
      "        [1.0838],\n",
      "        [1.1800],\n",
      "        [0.7411],\n",
      "        [0.9293],\n",
      "        [0.1971]], requires_grad=True)\n",
      "19000 1.8E-01 2.4E-02 1.5E-01 8.6E-04\n",
      "tensor([[0.7134],\n",
      "        [0.1676],\n",
      "        [1.0780],\n",
      "        [1.1620],\n",
      "        [0.7424],\n",
      "        [0.9291],\n",
      "        [0.1957]], requires_grad=True)\n",
      "19500 1.5E-01 2.3E-02 1.3E-01 9.0E-04\n",
      "tensor([[0.6823],\n",
      "        [0.1620],\n",
      "        [1.0722],\n",
      "        [1.1444],\n",
      "        [0.7439],\n",
      "        [0.9288],\n",
      "        [0.1943]], requires_grad=True)\n",
      "Overode, reduced library size\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 5.0E+00 2.3E-02 4.9E+00 1.7E-04\n",
      "tensor([[ 2.0000e-04],\n",
      "        [-2.0000e-04],\n",
      "        [-2.0000e-04],\n",
      "        [ 2.0000e-04],\n",
      "        [ 9.2817e-01]], requires_grad=True)\n",
      "500 8.2E-02 2.9E-02 5.3E-02 1.4E-05\n",
      "tensor([[ 0.0496],\n",
      "        [ 0.0353],\n",
      "        [-0.0311],\n",
      "        [ 0.0412],\n",
      "        [ 0.9157]], requires_grad=True)\n",
      "1000 7.1E-02 3.1E-02 4.0E-02 1.6E-05\n",
      "tensor([[ 0.0887],\n",
      "        [ 0.1274],\n",
      "        [-0.0425],\n",
      "        [ 0.0812],\n",
      "        [ 0.9067]], requires_grad=True)\n",
      "1500 6.4E-02 3.0E-02 3.5E-02 1.9E-05\n",
      "tensor([[ 0.1263],\n",
      "        [ 0.2265],\n",
      "        [-0.0430],\n",
      "        [ 0.1271],\n",
      "        [ 0.8948]], requires_grad=True)\n",
      "2000 5.7E-02 2.8E-02 3.0E-02 2.2E-05\n",
      "tensor([[ 0.1668],\n",
      "        [ 0.3234],\n",
      "        [-0.0342],\n",
      "        [ 0.1799],\n",
      "        [ 0.8792]], requires_grad=True)\n",
      "2500 5.0E-02 2.5E-02 2.5E-02 2.5E-05\n",
      "tensor([[ 0.2091],\n",
      "        [ 0.4171],\n",
      "        [-0.0125],\n",
      "        [ 0.2375],\n",
      "        [ 0.8594]], requires_grad=True)\n",
      "3000 4.3E-02 2.3E-02 2.0E-02 2.9E-05\n",
      "tensor([[0.2514],\n",
      "        [0.5070],\n",
      "        [0.0256],\n",
      "        [0.2976],\n",
      "        [0.8353]], requires_grad=True)\n",
      "3500 3.7E-02 2.1E-02 1.5E-02 3.4E-05\n",
      "tensor([[0.2921],\n",
      "        [0.5916],\n",
      "        [0.0805],\n",
      "        [0.3578],\n",
      "        [0.8072]], requires_grad=True)\n",
      "4000 3.2E-02 2.1E-02 1.1E-02 4.0E-05\n",
      "tensor([[0.3299],\n",
      "        [0.6690],\n",
      "        [0.1466],\n",
      "        [0.4156],\n",
      "        [0.7759]], requires_grad=True)\n",
      "4500 2.6E-02 1.8E-02 7.8E-03 4.5E-05\n",
      "tensor([[0.3634],\n",
      "        [0.7368],\n",
      "        [0.2152],\n",
      "        [0.4681],\n",
      "        [0.7432]], requires_grad=True)\n",
      "5000 2.3E-02 1.7E-02 5.6E-03 5.0E-05\n",
      "tensor([[0.3911],\n",
      "        [0.7927],\n",
      "        [0.2788],\n",
      "        [0.5127],\n",
      "        [0.7110]], requires_grad=True)\n",
      "5500 2.1E-02 1.7E-02 4.4E-03 5.4E-05\n",
      "tensor([[0.4132],\n",
      "        [0.8347],\n",
      "        [0.3325],\n",
      "        [0.5475],\n",
      "        [0.6814]], requires_grad=True)\n",
      "6000 2.0E-02 1.6E-02 3.9E-03 5.7E-05\n",
      "tensor([[0.4301],\n",
      "        [0.8609],\n",
      "        [0.3739],\n",
      "        [0.5718],\n",
      "        [0.6555]], requires_grad=True)\n",
      "6500 2.0E-02 1.6E-02 3.7E-03 5.9E-05\n",
      "tensor([[0.4426],\n",
      "        [0.8706],\n",
      "        [0.4027],\n",
      "        [0.5855],\n",
      "        [0.6335]], requires_grad=True)\n",
      "7000 1.9E-02 1.6E-02 3.6E-03 6.0E-05\n",
      "tensor([[0.4516],\n",
      "        [0.8647],\n",
      "        [0.4202],\n",
      "        [0.5898],\n",
      "        [0.6144]], requires_grad=True)\n",
      "7500 1.9E-02 1.5E-02 3.6E-03 6.0E-05\n",
      "tensor([[0.4575],\n",
      "        [0.8461],\n",
      "        [0.4287],\n",
      "        [0.5866],\n",
      "        [0.5962]], requires_grad=True)\n",
      "8000 1.9E-02 1.5E-02 3.5E-03 5.9E-05\n",
      "tensor([[0.4603],\n",
      "        [0.8188],\n",
      "        [0.4306],\n",
      "        [0.5783],\n",
      "        [0.5766]], requires_grad=True)\n",
      "8500 1.9E-02 1.5E-02 3.4E-03 5.8E-05\n",
      "tensor([[0.4600],\n",
      "        [0.7865],\n",
      "        [0.4271],\n",
      "        [0.5665],\n",
      "        [0.5533]], requires_grad=True)\n",
      "9000 1.8E-02 1.5E-02 3.3E-03 5.7E-05\n",
      "tensor([[0.4554],\n",
      "        [0.7516],\n",
      "        [0.4182],\n",
      "        [0.5518],\n",
      "        [0.5248]], requires_grad=True)\n",
      "9500 2.9E-02 2.5E-02 4.5E-03 5.4E-05\n",
      "tensor([[0.4469],\n",
      "        [0.7148],\n",
      "        [0.4036],\n",
      "        [0.5343],\n",
      "        [0.4901]], requires_grad=True)\n",
      "10000 2.7E-02 2.3E-02 4.0E-03 5.2E-05\n",
      "tensor([[0.4339],\n",
      "        [0.6771],\n",
      "        [0.3832],\n",
      "        [0.5133],\n",
      "        [0.4478]], requires_grad=True)\n",
      "10500 1.8E-02 1.5E-02 2.9E-03 4.9E-05\n",
      "tensor([[0.4175],\n",
      "        [0.6387],\n",
      "        [0.3562],\n",
      "        [0.4880],\n",
      "        [0.3973]], requires_grad=True)\n",
      "11000 2.0E-02 1.7E-02 3.1E-03 4.5E-05\n",
      "tensor([[0.4000],\n",
      "        [0.5991],\n",
      "        [0.3214],\n",
      "        [0.4579],\n",
      "        [0.3385]], requires_grad=True)\n",
      "11500 1.7E-02 1.4E-02 2.5E-03 4.0E-05\n",
      "tensor([[0.3800],\n",
      "        [0.5608],\n",
      "        [0.2833],\n",
      "        [0.4219],\n",
      "        [0.2714]], requires_grad=True)\n",
      "12000 1.7E-02 1.4E-02 2.3E-03 3.6E-05\n",
      "tensor([[0.3645],\n",
      "        [0.5194],\n",
      "        [0.2437],\n",
      "        [0.3844],\n",
      "        [0.1997]], requires_grad=True)\n",
      "12500 1.6E-02 1.4E-02 2.1E-03 3.1E-05\n",
      "tensor([[0.3473],\n",
      "        [0.4760],\n",
      "        [0.2061],\n",
      "        [0.3471],\n",
      "        [0.1287]], requires_grad=True)\n",
      "13000 1.6E-02 1.4E-02 2.0E-03 2.7E-05\n",
      "tensor([[0.3234],\n",
      "        [0.4310],\n",
      "        [0.1745],\n",
      "        [0.3168],\n",
      "        [0.0669]], requires_grad=True)\n",
      "13500 1.6E-02 1.4E-02 1.9E-03 2.5E-05\n",
      "tensor([[0.2987],\n",
      "        [0.3833],\n",
      "        [0.1590],\n",
      "        [0.3039],\n",
      "        [0.0289]], requires_grad=True)\n",
      "14000 1.6E-02 1.4E-02 1.9E-03 2.4E-05\n",
      "tensor([[0.2755],\n",
      "        [0.3520],\n",
      "        [0.1540],\n",
      "        [0.3058],\n",
      "        [0.0179]], requires_grad=True)\n",
      "14500 1.6E-02 1.4E-02 1.9E-03 2.4E-05\n",
      "tensor([[0.2572],\n",
      "        [0.3345],\n",
      "        [0.1504],\n",
      "        [0.3095],\n",
      "        [0.0133]], requires_grad=True)\n",
      "15000 1.6E-02 1.4E-02 1.9E-03 2.4E-05\n",
      "tensor([[0.2427],\n",
      "        [0.3261],\n",
      "        [0.1454],\n",
      "        [0.3117],\n",
      "        [0.0107]], requires_grad=True)\n",
      "15500 1.6E-02 1.4E-02 1.9E-03 2.3E-05\n",
      "tensor([[0.2350],\n",
      "        [0.3183],\n",
      "        [0.1450],\n",
      "        [0.3135],\n",
      "        [0.0073]], requires_grad=True)\n",
      "16000 1.6E-02 1.4E-02 1.9E-03 2.2E-05\n",
      "tensor([[ 0.2278],\n",
      "        [ 0.3050],\n",
      "        [ 0.1399],\n",
      "        [ 0.3105],\n",
      "        [-0.0054]], requires_grad=True)\n",
      "16500 1.5E-02 1.4E-02 1.8E-03 2.0E-05\n",
      "tensor([[ 0.2029],\n",
      "        [ 0.2642],\n",
      "        [ 0.1100],\n",
      "        [ 0.2880],\n",
      "        [-0.0565]], requires_grad=True)\n",
      "17000 1.8E-02 1.6E-02 1.9E-03 1.9E-05\n",
      "tensor([[ 0.1864],\n",
      "        [ 0.2497],\n",
      "        [ 0.0995],\n",
      "        [ 0.2860],\n",
      "        [-0.0685]], requires_grad=True)\n",
      "17500 1.5E-02 1.3E-02 1.8E-03 1.9E-05\n",
      "tensor([[ 0.1737],\n",
      "        [ 0.2389],\n",
      "        [ 0.0962],\n",
      "        [ 0.2868],\n",
      "        [-0.0757]], requires_grad=True)\n",
      "18000 1.5E-02 1.3E-02 1.8E-03 1.9E-05\n",
      "tensor([[ 0.1637],\n",
      "        [ 0.2241],\n",
      "        [ 0.0935],\n",
      "        [ 0.2874],\n",
      "        [-0.0813]], requires_grad=True)\n",
      "18500 1.5E-02 1.3E-02 1.7E-03 1.8E-05\n",
      "tensor([[ 0.1524],\n",
      "        [ 0.2107],\n",
      "        [ 0.0887],\n",
      "        [ 0.2868],\n",
      "        [-0.0898]], requires_grad=True)\n",
      "19000 1.5E-02 1.3E-02 1.7E-03 1.8E-05\n",
      "tensor([[ 0.1405],\n",
      "        [ 0.1976],\n",
      "        [ 0.0819],\n",
      "        [ 0.2848],\n",
      "        [-0.1010]], requires_grad=True)\n",
      "19500 1.5E-02 1.3E-02 1.7E-03 1.8E-05\n",
      "tensor([[ 0.1298],\n",
      "        [ 0.1863],\n",
      "        [ 0.0763],\n",
      "        [ 0.2839],\n",
      "        [-0.1103]], requires_grad=True)\n",
      "Defaulted to minimum library size\n",
      "[tensor([[0.6519],\n",
      "        [0.1571],\n",
      "        [1.0664],\n",
      "        [1.1274],\n",
      "        [0.7455],\n",
      "        [0.9284],\n",
      "        [0.1929]], requires_grad=True)] [tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.2840]], requires_grad=True)] [tensor([0, 3, 4])]\n",
      "Now running final cycle.\n",
      "Epoch | Total loss | MSE | PI | L1 \n",
      "0 1.8E-02 1.3E-02 5.1E-03 0.0E+00\n",
      "tensor([[-2.0000e-04],\n",
      "        [ 2.0000e-04],\n",
      "        [ 2.8380e-01]], requires_grad=True)\n",
      "500 1.5E-02 1.3E-02 2.2E-03 0.0E+00\n",
      "tensor([[-0.0654],\n",
      "        [ 0.0742],\n",
      "        [ 0.3674]], requires_grad=True)\n",
      "1000 1.5E-02 1.3E-02 1.9E-03 0.0E+00\n",
      "tensor([[-0.0890],\n",
      "        [ 0.1127],\n",
      "        [ 0.4115]], requires_grad=True)\n",
      "1500 1.5E-02 1.3E-02 1.9E-03 0.0E+00\n",
      "tensor([[-0.0840],\n",
      "        [ 0.1285],\n",
      "        [ 0.4223]], requires_grad=True)\n",
      "2000 1.5E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[-0.0672],\n",
      "        [ 0.1357],\n",
      "        [ 0.4201]], requires_grad=True)\n",
      "2500 1.5E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[-0.0473],\n",
      "        [ 0.1411],\n",
      "        [ 0.4148]], requires_grad=True)\n",
      "3000 2.2E-02 2.0E-02 1.9E-03 0.0E+00\n",
      "tensor([[-0.0275],\n",
      "        [ 0.1460],\n",
      "        [ 0.4092]], requires_grad=True)\n",
      "3500 1.5E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[-0.0100],\n",
      "        [ 0.1500],\n",
      "        [ 0.4039]], requires_grad=True)\n",
      "4000 1.5E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0044],\n",
      "        [0.1538],\n",
      "        [0.4001]], requires_grad=True)\n",
      "4500 1.5E-02 1.3E-02 1.9E-03 0.0E+00\n",
      "tensor([[0.0153],\n",
      "        [0.1553],\n",
      "        [0.3957]], requires_grad=True)\n",
      "5000 1.4E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0224],\n",
      "        [0.1575],\n",
      "        [0.3944]], requires_grad=True)\n",
      "5500 1.4E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0271],\n",
      "        [0.1592],\n",
      "        [0.3933]], requires_grad=True)\n",
      "6000 1.4E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0307],\n",
      "        [0.1600],\n",
      "        [0.3922]], requires_grad=True)\n",
      "6500 1.5E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0332],\n",
      "        [0.1577],\n",
      "        [0.3870]], requires_grad=True)\n",
      "7000 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0353],\n",
      "        [0.1608],\n",
      "        [0.3904]], requires_grad=True)\n",
      "7500 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0380],\n",
      "        [0.1615],\n",
      "        [0.3897]], requires_grad=True)\n",
      "8000 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0396],\n",
      "        [0.1617],\n",
      "        [0.3889]], requires_grad=True)\n",
      "8500 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0421],\n",
      "        [0.1623],\n",
      "        [0.3882]], requires_grad=True)\n",
      "9000 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0437],\n",
      "        [0.1626],\n",
      "        [0.3874]], requires_grad=True)\n",
      "9500 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0471],\n",
      "        [0.1634],\n",
      "        [0.3864]], requires_grad=True)\n",
      "10000 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0477],\n",
      "        [0.1634],\n",
      "        [0.3861]], requires_grad=True)\n",
      "10500 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0494],\n",
      "        [0.1636],\n",
      "        [0.3855]], requires_grad=True)\n",
      "11000 1.5E-02 1.3E-02 2.0E-03 0.0E+00\n",
      "tensor([[0.0500],\n",
      "        [0.1607],\n",
      "        [0.3811]], requires_grad=True)\n",
      "11500 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0528],\n",
      "        [0.1644],\n",
      "        [0.3843]], requires_grad=True)\n",
      "12000 1.4E-02 1.2E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0550],\n",
      "        [0.1651],\n",
      "        [0.3837]], requires_grad=True)\n",
      "12500 1.3E-02 1.1E-02 1.7E-03 0.0E+00\n",
      "tensor([[0.0519],\n",
      "        [0.1664],\n",
      "        [0.3854]], requires_grad=True)\n",
      "13000 1.3E-02 1.1E-02 1.7E-03 0.0E+00\n",
      "tensor([[0.0539],\n",
      "        [0.1653],\n",
      "        [0.3837]], requires_grad=True)\n",
      "13500 1.3E-02 1.1E-02 1.7E-03 0.0E+00\n",
      "tensor([[0.0554],\n",
      "        [0.1654],\n",
      "        [0.3831]], requires_grad=True)\n",
      "14000 1.2E-02 1.1E-02 1.6E-03 0.0E+00\n",
      "tensor([[0.0569],\n",
      "        [0.1655],\n",
      "        [0.3825]], requires_grad=True)\n",
      "14500 1.3E-02 1.1E-02 1.7E-03 0.0E+00\n",
      "tensor([[0.0580],\n",
      "        [0.1656],\n",
      "        [0.3821]], requires_grad=True)\n",
      "15000 1.2E-02 1.1E-02 1.6E-03 0.0E+00\n",
      "tensor([[0.0584],\n",
      "        [0.1659],\n",
      "        [0.3820]], requires_grad=True)\n",
      "15500 1.2E-02 1.1E-02 1.6E-03 0.0E+00\n",
      "tensor([[0.0585],\n",
      "        [0.1653],\n",
      "        [0.3816]], requires_grad=True)\n",
      "16000 1.4E-02 1.3E-02 1.8E-03 0.0E+00\n",
      "tensor([[0.0567],\n",
      "        [0.1639],\n",
      "        [0.3809]], requires_grad=True)\n",
      "16500 1.2E-02 1.0E-02 1.6E-03 0.0E+00\n",
      "tensor([[0.0552],\n",
      "        [0.1636],\n",
      "        [0.3819]], requires_grad=True)\n",
      "17000 1.2E-02 1.0E-02 1.5E-03 0.0E+00\n",
      "tensor([[0.0518],\n",
      "        [0.1637],\n",
      "        [0.3833]], requires_grad=True)\n",
      "17500 1.2E-02 1.0E-02 1.5E-03 0.0E+00\n",
      "tensor([[0.0483],\n",
      "        [0.1637],\n",
      "        [0.3851]], requires_grad=True)\n",
      "18000 1.2E-02 1.0E-02 1.5E-03 0.0E+00\n",
      "tensor([[0.0453],\n",
      "        [0.1629],\n",
      "        [0.3862]], requires_grad=True)\n",
      "18500 1.2E-02 1.0E-02 1.5E-03 0.0E+00\n",
      "tensor([[0.0436],\n",
      "        [0.1607],\n",
      "        [0.3863]], requires_grad=True)\n",
      "19000 1.1E-02 9.9E-03 1.5E-03 0.0E+00\n",
      "tensor([[0.0417],\n",
      "        [0.1614],\n",
      "        [0.3873]], requires_grad=True)\n",
      "19500 1.1E-02 9.7E-03 1.4E-03 0.0E+00\n",
      "tensor([[0.0409],\n",
      "        [0.1624],\n",
      "        [0.3880]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "sparse_coeff_vector_list, sparsity_mask_list, network = DeepMoD(time_Tensor, Stress_Tensor, network_config, lib_config, optim_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 3, 4])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity_mask_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.0420],\n",
       "         [0.1632],\n",
       "         [0.3879]], requires_grad=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coeff_vector_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why might it not be working?\n",
    "\n",
    "- Currently thresholding is not working. Reasons for rejecting the given idea are valid, but the problem is DeepMoD never finds the correct, or even an acceptable equation. To be less aggressive, maybe a rejected threshold could just run deepmod as a continuation of the previous training (20000 more epochs) but with no change to the sparsity mask list. This just gives the NN more time to get it right.\n",
    "\n",
    "- maybe input is not yet complex enough to make it clear what the solution is?\n",
    "\n",
    "- maybe input is not over broad enough time scales to 'see' all behaviour\n",
    "\n",
    "- maybe the network is not dense or large enough?\n",
    "\n",
    "- is there a limit on how similar the decay parameters can be before deepmod doesn't distinguish between them? For example, what is the effect on the maths of having 2 identical branches? I would predict it would be the same as having one branch with double the viscosity and elastic modulus, ie \\[1,1\\] \\[2,2\\] -> \\[2\\] \\[4\\]\n",
    "\n",
    "- experiment with learning rates (different for network params and coeffs)\n",
    "\n",
    "- experiment with lambda values\n",
    "\n",
    "- consider modifying weighting of loss components\n",
    "\n",
    "- need to ascertain whether or not the issue is in part related to the NN's prediction not being an accurate recreation of the target data. If this is failing we need to look at things like shape and activation functions within the network. If this part is not fine, we need to look at everything, including the loss and optimisation parts. However, it is fine, this tells us we need only look at the loss and optimisation as it relates to the coeff_vector. Conclusion: seeing prediction and target plotted in real-time would be really useful.\n",
    "\n",
    "- could experiment outside of Adam optimiser....\n",
    "\n",
    "> Essentially 5 main categories of tweaking\n",
    "- Shape\n",
    "- Activation*\n",
    "- Loss*\n",
    "- Optimisation\n",
    "- Input Data\n",
    "\\\n",
    "> Where the starred items I feel I less likely to where I can tinker easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepymod_torch.VE_params as VE_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(2.57781134849280, 0.285800607935181, 6.80760112448933)], [E_0, E_1, eta_1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model_Params = VE_params.model_params_from_coeffs(sparse_coeff_vector_list[0])\n",
    "Model_Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
