{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all necessary modules from Numpy and PyTorch (Code copied from neural_net.py).\n",
    "\n",
    "The last import is just so I can slow it down a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create a set of input data. I believe this must be as a Torch tensor, but first I will create it as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = [n for n in range(10)]\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To turn this into a Torch tensor, we first need to turn it into a NumPy array. I do not think it is possible to turn a list directly into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = np.array(Input_Data)\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then turn this array into a tensor. Contrary to the guidelines in the 60 minute blitz tutorial say, you can turn a numpy array into a PyTorch tensor simply using the torch.tensor operation. This, crucially, allows you specify teh data type as float. For reasons best known to the developers of PyTorch, the datatype of a float is not assumed, and then this creates errors later on.\n",
    "\n",
    "The requires_grad=True is not necessary at least for feedfoward use of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input_Data = torch.tensor(Input_Data, dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to make sure the tensor is not given in terms of columns. Each row is taken later as a seperate instance for training, and all higher dimensions are data associated with that same instance. If we supply our simple tensor as a row vector, it will look like one single instance with 10 input parameters. This, we transpose it. Transposing is not made easy because when the array or tensor is just a vector, python modules like to strip it of its awareness of higher dimensions for reasons best known to the developers of NumPy and PyTorch. The -1 means default to whatever dimension (#rows) necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]], grad_fn=<ViewBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "Input_Data = Input_Data.view(-1, 1)\n",
    "print(Input_Data, type(Input_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the target data, which in this case is the result of performing y = 2x + 54 where y is the Target Data and x is the Input_Data. The same method will be used but in one step this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[54.],\n",
      "        [56.],\n",
      "        [58.],\n",
      "        [60.],\n",
      "        [62.],\n",
      "        [64.],\n",
      "        [66.],\n",
      "        [68.],\n",
      "        [70.],\n",
      "        [72.]], grad_fn=<ViewBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "def Poly(x):\n",
    "    y = 2*x + 54\n",
    "    return y\n",
    "\n",
    "Target_Data = [Poly(n) for n in range(10)]\n",
    "Target_Data = np.array(Target_Data)\n",
    "Target_Data = torch.tensor(Target_Data, dtype=torch.float32, requires_grad=True)\n",
    "Target_Data = Target_Data.view(-1,1)\n",
    "print(Target_Data, type(Target_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have Data and the corresponding target, which change subject slightly and set up a simple neural network. The design for this will be one that has a single input, 2 hidden layers, each with 3 nodes, and no activation functions. Edit:(the loop fails after at maximum teh first 10 epochs unless some kind of activation function is used, presumably because the results blow up. All results become Nan). There will be one output. Deciding on the shape and functional description of each node should be enough to make a forward pass on the network. To do this we will need to use PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will construct the netowrk the same way as in \"neural_net\" which seems to differ significantly from what I read in the tutorial. In the tutorial, fundamentally, a class was created with certain properties that were then linked together by a method of that class. In DeepMoD, what seems to be occurring is that a list is created of neural network layers, and then some function is used to transform that into a network of some class that i cannot guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of layers, we start with the first hidden layer. It has 1 as the first arguement to specify that each node should expect 1 input (each single element of out input data). It then has a 3 to specify the number of nodes in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network = [nn.Linear(1, 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the piecewise activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the second hidden layer, which we chose to be identical to the first. This time the 1st arguement is 3 as there were 3 nodes in teh previous layer. In between these two elements is where we would have applied the \"activation function\" if we were including one. append() is a method of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Linear(3, 3))\n",
    "Network.append(nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then append the output. It is single valued, so the second arguement becomes 1. There is no activation function here as we just want to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Network.append(nn.Linear(3, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply ths funny function that changes it into a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "Torch_Network = nn.Sequential(*Network)\n",
    "print(type(Torch_Network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a built network. We can feed a tensor to this network and get the feedforward output. Currently, the weights and biases have all been set randomly so the result will itself be fairly random. However, for a given input, it will be consistant, as no backprop occurs, and the network is unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the feedforward output, we can create a tensor of single value that will therefore output a single value. We need to call the number 1 as \"1.\" so that it takes it as a torch.float class object, which otherwise is, for soem reason, not assumed. We will output what teh network currently tells us for 1 and 20, twixe each, to show consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7607], grad_fn=<AddBackward0>)\n",
      "tensor([0.7607], grad_fn=<AddBackward0>)\n",
      "tensor([0.7586], grad_fn=<AddBackward0>)\n",
      "tensor([0.7586], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(Torch_Network(torch.tensor([1.])))\n",
    "print(Torch_Network(torch.tensor([1.])))\n",
    "print(Torch_Network(torch.tensor([20.])))\n",
    "print(Torch_Network(torch.tensor([20.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7589],\n",
      "        [0.7607],\n",
      "        [0.7619],\n",
      "        [0.7624],\n",
      "        [0.7625],\n",
      "        [0.7621],\n",
      "        [0.7617],\n",
      "        [0.7612],\n",
      "        [0.7607],\n",
      "        [0.7603]], grad_fn=<AddmmBackward>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "Output_Data = Torch_Network(Input_Data)\n",
    "print(Output_Data, type(Output_Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is to start to train the neural network. For that, 2 additional things need to be decided upon; the loss function, and the optimisation function. For the former, we will simply use MSE loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_Function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the optimisation function, we will simply use stochastic gradient descent with a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(Torch_Network.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrowing the simple components of the loop from the Neural Networks part of the PyTorch Tutorial, we combine everything into a loop, so that we train the neural network. The function \"optimizer.zero_grad()\" apparently needs to be run.\n",
    "After that, we calaculate the networks prediction on the data. We are feeding it a Tensor of size 10, so it will sequentially (I think) use the network to evaluate each element in turn, and output a tensor of equal size to give the results.\n",
    "The loss is then calculated, I believe this is a tensor as well\n",
    "Then we just sort of *do* the backprop to work out the gradients in loss with respect to each weight and bias\n",
    "Then we trigger the SGD to adjust each bias for each input data element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to end this\n"
     ]
    }
   ],
   "source": [
    "Max_Iterations = 10000\n",
    "\n",
    "for n in range(Max_Iterations):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    Output_Data = Torch_Network(Input_Data)\n",
    "    Loss = Loss_Function(Output_Data, Target_Data)\n",
    "    Loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    '''\n",
    "    if n % 10 == 0:\n",
    "        print('Reached Epoch', n)\n",
    "        print('Loss is', Loss)\n",
    "        print('Current Prediction is', Output_Data)\n",
    "        #time.sleep(5) # This is just for my convenience so i can see what is happening before it continues\n",
    "    '''\n",
    "\n",
    "print('Ready to end this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as we never used the 1000th iteration of our network to calculate results, we do this last bit one last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0627, grad_fn=<MeanBackward0>)\n",
      "tensor([[54.1621],\n",
      "        [55.9690],\n",
      "        [58.1034],\n",
      "        [60.2701],\n",
      "        [62.2707],\n",
      "        [64.1645],\n",
      "        [66.1712],\n",
      "        [68.3785],\n",
      "        [70.4899],\n",
      "        [72.0553]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "Output_Data = Torch_Network(Input_Data)\n",
    "Loss = Loss_Function(Output_Data, Target_Data)\n",
    "print(Loss)\n",
    "print(Output_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
